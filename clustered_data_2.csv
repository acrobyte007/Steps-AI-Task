Topic Name,Links,Information and Text,Cluster
cuda toolkit documentation 12.5 update 1,#cuda-toolkit-documentation-v12-4,"cuda toolkit documentation 12.5 update 1 develop, optimize and deploy gpu-accelerated apps the nvidia cuda toolkit provides a development environment for creating high performance gpu-accelerated
applications. with the cuda toolkit, you can develop, optimize, and deploy your applications on gpu-accelerated
embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and hpc supercomputers.
the toolkit includes gpu-accelerated libraries, debugging and optimization tools, a c/c++ compiler, and a runtime
library to deploy your application. using built-in capabilities for distributing computations across multi-gpu configurations, scientists and researchers
can develop applications that scale from single gpu workstations to cloud installations with thousands of gpus.",0
installation guides,#installation-guides,installation guides,9
programming guides,#programming-guides,programming guides,9
cuda api references,#cuda-api-references,cuda api references,0
ptx compiler api references,#ptx-compiler-api-references,ptx compiler api references,7
miscellaneous,#miscellaneous,miscellaneous,9
tools,#tools,tools,9
white papers,#white-papers,white papers,9
application notes,#application-notes,application notes,9
compiler sdk,#compiler-sdk,compiler sdk,6
search results,https://docs.nvidia.com/cuda/debugger-api/index.html,search results,9
debugger api,https://docs.nvidia.com/cuda/debugger-api/index.html,debugger api,9
table of contents,https://docs.nvidia.com/cuda/debugger-api/index.html,table of contents,9
1.turing tuning guide,#turing-tuning-guide,1.turing tuning guide,9
1.1.nvidia turing compute architecture,#nvidia-turing-compute-architecture,"1.1.nvidia turing compute architecture turing is nvidias latest architecture for cuda compute applications. turing retains and extends the same cuda programming model provided by previous nvidia architectures such as pascal and volta, and applications that follow the best practices for those architectures should typically see speedups on the turing architecture without any code changes. this guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging turing architectural features.1 for further details on the programming features discussed in this guide, please refer to thecuda c++ programming guide.",0
1.2.cuda best practices,#cuda-best-practices,1.2.cuda best practices the performance guidelines and best practices described in thecuda c++ programming guideand thecuda c++ best practices guideapply to all cuda-capable gpu architectures. programmers must primarily focus on following those recommendations to achieve the best performance. the high-priority recommendations from those guides are as follows:,0
1.3.application compatibility,#application-compatibility,"1.3.application compatibility before addressing specific performance tuning issues covered in this guide, refer to theturing compatibility guide for cuda applicationsto ensure that your application is compiled in a way that is compatible with turing.",0
1.4.turing tuning,#turing-tuning,1.4.turing tuning,9
1.4.1.streaming multiprocessor,#streaming-multiprocessor,"1.4.1.streaming multiprocessor the turing streaming multiprocessor (sm) is based on the same major architecture (7.x) as volta, and provides similar improvements over pascal.",5
1.4.2.tensor core operations,#tensor-core-operations,"1.4.2.tensor core operations volta introduced tensor cores to accelerate matrix multiply operations on mixed precision floating point data. turing adds acceleration for integer matrix multiply operations. the tensor cores are exposed as warp-level matrix operations in the cuda 10 c++ api. the api provides specialized matrix load, matrix multiply and accumulate, and matrix store operations, where each warp processes a small matrix fragment, allowing to efficiently use tensor cores from a cuda-c++ program. in practice, tensor cores are used to perform much larger 2d or higher dimensional matrix operations, built up from these smaller matrix fragments. each tensor core performs the matrix multiply-accumulate: d = a x b + c. the tensor cores support half precision matrix multiplication, where the matrix multiply inputs a and b are fp16 matrices, while the accumulation matrices c and d may be either fp16 or fp32 matrices. when accumulating in fp32, the fp16 multiply results in a full precision product that is then accumulated using fp32 addition. cuda 10 supports several fragment sizes, 16x16x16, 32x8x16, and 8x32x16 to use the tensor cores on volta or turing with fp16 inputs. any binary compiled for volta will run on turing, but volta binaries using tensor cores will only be able to reach half of turings tensor core peak performance. recompiling the binary specifically for turing would allow it to reach the peak performance. see the turing compatibility guide for more information. turings tensor core supports integer matrix multiply operations, which can operate on 8-bit, 4-bit and 1-bit integer inputs, with 32-bit integer accumulation. when operating on 8-bit inputs, cuda exposes fragment sizes of 16x16x16, 32x8x16, and 8x32x16. for sub-byte operations the fragment sizes available are 8x8x32 for 4-bit inputs, or 8x8x128 for 1-bit inputs. see thecuda c++ programming guidefor more information.",5
1.4.3.memory throughput,#memory-throughput,1.4.3.memory throughput,5
2.revision history,#revision-history,2.revision history version 1.0 version 1.1,9
3.notices,#notices,3.notices,9
3.1.notice,#notice,"3.1.notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation (nvidia) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material (defined below), code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer (terms of sale). nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion and/or use of nvidia products in such equipment or applications and therefore such inclusion and/or use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions and/or requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the nvidia product in any manner that is contrary to this document or (ii) customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding third-party products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents (together and separately, materials) are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product.",4
3.2.opencl,#opencl,3.2.opencl opencl is a trademark of apple inc. used under license to the khronos group inc.,4
3.3.trademarks,#trademarks,3.3.trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.,4
1.turing compatibility,#turing-compatibility,1.turing compatibility,6
1.1.about this document,#about-this-document,"1.1.about this document this application note, turing compatibility guide for cuda applications, is intended to help developers ensure that their nvidiacudaapplications will run on gpus based on the nvidiaturing architecture. this document provides guidance to developers who are already familiar with programming in cuda c++ and want to make sure that their software applications are compatible with turing.",0
1.2.application compatibility on turing,#application-compatibility-on-turing,"1.2.application compatibility on turing the nvidia cuda c++ compiler,nvcc, can be used to generate both architecture-specificcubinfiles and forward-compatibleptxversions of each kernel. each cubin file targets a specific compute-capability version and is forward-compatibleonly with gpu architectures of the same major version number. for example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (kepler) devices but arenotsupported on compute-capability 5.x (maxwell) or 6.x (pascal) devices. for this reason, to ensure forward compatibility with gpu architectures introduced after the application has been released, it is recommended that all applications include ptx versions of their kernels. applications that already include ptx versions of their kernels should work as-is on turing-based gpus. applications that only support specific gpu architectures via cubin files, however, will need to be updated to provide turing-compatible ptx or cubins.",0
1.3.compatibility between volta and turing,#compatibility-between-volta-and-turing,"1.3.compatibility between volta and turing the turing architecture is based on voltas instruction set architectureisa7.0, extending it with new instructions. as a consequence, any binary that runs on volta will be able to run on turing (forward compatibility), but a turing binary will not be able to run on volta. please note that volta kernels using more than 64kb of shared memory (via the explicit opt-in, seecuda c++ programming guide) will not be able to launch on turing, as they would exceed turings shared memory capacity. most applications compiled for volta should run efficiently on turing, except if the application uses heavily the tensor cores, or if recompiling would allow use of new turing-specific instructions. voltas tensor core instructions can only reach half of the peak performance on turing. recompiling explicitly for turing is thus recommended.",5
1.4.verifying turing compatibility for existing applications,#verifying-turing-compatibility-for-existing-applications,1.4.verifying turing compatibility for existing applications the first step is to check that turing-compatible device code (at least ptx) is compiled into the application. the following sections show how to accomplish this for applications built with different cuda toolkit versions.,0
1.4.1.applications using cuda toolkit 8.0 or earlier,#applications-using-cuda-toolkit-8-0-or-earlier,"1.4.1.applications using cuda toolkit 8.0 or earlier cuda applications built using cuda toolkit versions 2.1 through 8.0 are compatible with turing as long as they are built to include ptx versions of their kernels. to test that ptx jit is working for your application, you can do the following: when starting a cuda application for the first time with the above environment flag, the cuda driver will jit-compile the ptx for each cuda kernel that is used into native cubin code. if you set the environment variable above and then launch your program and it works properly, then you have successfully verified turing compatibility.",0
1.4.2.applications using cuda toolkit 9.x,#applications-using-cuda-toolkit-9-x,1.4.2.applications using cuda toolkit 9.x cuda applications built using cuda toolkit 9.x are compatible with turing as long as they are built to include kernels in either volta-native cubin format (seecompatibility between volta and turing) or ptx format (seeapplications using cuda toolkit 8.0 or earlier) or both.,0
1.4.3.applications using cuda toolkit 10.0,#applications-using-cuda-toolkit-10-0,"1.4.3.applications using cuda toolkit 10.0 cuda applications built using cuda toolkit 10.0 are compatible with turing as long as they are built to include kernels in volta-native or turing-native cubin format (seecompatibility between volta and turing), or ptx format (seeapplications using cuda toolkit 8.0 or earlier), or both.",0
1.5.building applications with turing support,#building-applications-with-turing-support,"1.5.building applications with turing support when a cuda application launches a kernel, the cuda runtime determines the compute capability of each gpu in the system and uses this information to automatically find the best matching cubin or ptx version of the kernel that is available. if a cubin file supporting the architecture of the target gpu is available, it is used; otherwise, the cuda runtime will load the ptx and jit-compile that ptx to the gpus native cubin format before launching it. if neither is available, then the kernel launch will fail. the method used to build your application with either native cubin or at least ptx support for turing depend on the version of the cuda toolkit used. the main advantages of providing native cubins are as follows:",0
1.5.1.applications using cuda toolkit 8.0 or earlier,#id1,"1.5.1.applications using cuda toolkit 8.0 or earlier the compilers included in cuda toolkit 8.0 or earlier generate cubin files native to earlier nvidia architectures such as maxwell and pascal, but theycannotgenerate cubin files native to volta or turing architecture. to allow support for volta, turing and future architectures when using version 8.0 or earlier of the cuda toolkit, the compiler must generate a ptx version of each kernel. below are compiler settings that could be used to buildmykernel.cuto run on maxwell or pascal devices natively and on turing devices via ptx jit. windows mac/linux alternatively, you may be familiar with the simplifiednvcccommand-line option-arch=sm_xx, which is a shorthand equivalent to the following more explicit-gencode=command-line options used above.-arch=sm_xxexpands to the following: however, while the-arch=sm_xxcommand-line option does result in inclusion of a ptx back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple-arch=options on the samenvcccommand line, which is why the examples above use-gencode=explicitly.",0
1.5.2.applications using cuda toolkit 9.x,#id2,"1.5.2.applications using cuda toolkit 9.x with versions 9.x of the cuda toolkit,nvcccan generate cubin files native to the volta architecture (compute capability 7.0). when using cuda toolkit 9.x, to ensure thatnvccwill generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate-gencode=parameters on thenvcccommand line as shown in the examples below. windows mac/linux also, note that cuda 9.0 removes support for compute capability 2.x (fermi) devices. any compute_2x and sm_2x flags need to be removed from your compiler commands.",0
1.5.3.applications using cuda toolkit 10.0,#id3,"1.5.3.applications using cuda toolkit 10.0 with version 10.0 of the cuda toolkit,nvcccan generate cubin files native to the turing architecture (compute capability 7.5). when using cuda toolkit 10.0, to ensure thatnvccwill generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate-gencode=parameters on thenvcccommand line as shown in the examples below. windows mac/linux",0
1.5.4.independent thread scheduling compatibility,#independent-thread-scheduling-compatibility,"1.5.4.independent thread scheduling compatibility the volta and turing architectures feature independent thread scheduling among threads in a warp. if the developer made assumptions about warp-synchronicity,1this feature can alter the set of threads participating in the executed code compared to previous architectures. please see compute capability 7.0 in thecuda c++ programming guidefor details and corrective actions. to aid migration volta and turing developers can opt-in to the pascal scheduling model with the following combination of compiler options.",5
1.license agreement for nvidia software development kits,#license-agreement-for-nvidia-software-development-kits,"1.license agreement for nvidia software development kits important noticeread before downloading, installing, copying or using the licensed software: this license agreement, including exhibits attached (agreement) is a legal agreement between you and nvidia corporation (nvidia) and governs your use of a nvidia software development kit (sdk). each sdk has its own set of software and materials, but here is a description of the types of items that may be included in a sdk: source code, header files, apis, data sets and assets (examples include images, textures, models, scenes, videos, native api input/output files), binary software, sample code, libraries, utility programs, programming code and documentation. this agreement can be accepted only by an adult of legal age of majority in the country in which the sdk is used. if you are entering into this agreement on behalf of a company or other legal entity, you represent that you have the legal authority to bind the entity to this agreement, in which case you will mean the entity you represent. if you dont have the required age or authority to accept this agreement, or if you dont accept all the terms and conditions of this agreement, do not download, install or use the sdk. you agree to use the sdk only for purposes that are permitted by (a) this agreement, and (b) any applicable law, regulation or generally accepted practices or guidelines in the relevant jurisdictions.",4
1.1.license,#license,1.1.license,9
1.1.1.license grant,#license-grant,"1.1.1.license grant subject to the terms of this agreement, nvidia hereby grants you a non-exclusive, non-transferable license, without the right to sublicense (except as expressly provided in this agreement) to:",4
1.1.2.distribution requirements,#distribution-requirements,1.1.2.distribution requirements these are the distribution requirements for you to exercise the distribution grant:,9
1.1.3.authorized users,#authorized-users,"1.1.3.authorized users you may allow employees and contractors of your entity or of your subsidiary(ies) to access and use the sdk from your secure network to perform work on your behalf. if you are an academic institution you may allow users enrolled or employed by the academic institution to access and use the sdk from your secure network. you are responsible for the compliance with the terms of this agreement by your authorized users. if you become aware that your authorized users didnt follow the terms of this agreement, you agree to take reasonable steps to resolve the non-compliance and prevent new occurrences.",9
1.1.4.pre-release sdk,#pre-release-sdk,"1.1.4.pre-release sdk the sdk versions identified as alpha, beta, preview or otherwise as pre-release, may not be fully functional, may contain errors or design flaws, and may have reduced or different security, privacy, accessibility, availability, and reliability standards relative to commercial versions of nvidia software and materials. use of a pre-release sdk may result in unexpected results, loss of data, project delays or other unpredictable damage or loss. you may use a pre-release sdk at your own risk, understanding that pre-release sdks are not intended for use in production or business-critical systems. nvidia may choose not to make available a commercial version of any pre-release sdk. nvidia may also choose to abandon development and terminate the availability of a pre-release sdk at any time without liability.",4
1.1.5.updates,#updates,"1.1.5.updates nvidia may, at its option, make available patches, workarounds or other updates to this sdk. unless the updates are provided with their separate governing terms, they are deemed part of the sdk licensed to you as provided in this agreement. you agree that the form and content of the sdk that nvidia provides may change without prior notice to you. while nvidia generally maintains compatibility between versions, nvidia may in some cases make changes that introduce incompatibilities in future versions of the sdk.",4
1.1.6.components under other licenses,#components-under-other-licenses,"1.1.6.components under other licenses the sdk may come bundled with, or otherwise include or be distributed with, nvidia or third-party components with separate legal notices or terms as may be described in proprietary notices accompanying the sdk. if and to the extent there is a conflict between the terms in this agreement and the license terms associated with the component, the license terms associated with the components control only to the extent necessary to resolve the conflict. subject to the other terms of this agreement, you may use the sdk to develop and test applications released under open source initiative (osi) approved open source software licenses.",4
1.1.7.reservation of rights,#reservation-of-rights,"1.1.7.reservation of rights nvidia reserves all rights, title, and interest in and to the sdk, not expressly granted to you under this agreement.",4
1.introduction,#introduction,"1.introduction this guide covers the basic instructions needed to install cuda and verify that a cuda application can run on each supported platform. these instructions are intended to be used on a clean installation of a supported platform. for questions which are not answered in this document, please refer to thewindows installation guideandlinux installation guide. the cuda installation packages can be found on thecuda downloads page.",0
2.windows,#windows,"2.windows when installing cuda on windows, you can choose between the network installer and the local installer. the network installer allows you to download only the files you need. the local installer is a stand-alone installer with a large initial download. for more details, refer to thewindows installation guide.",0
2.1.network installer,#network-installer,2.1.network installer perform the following steps to install cuda and verify the installation.,0
2.2.local installer,#local-installer,2.2.local installer perform the following steps to install cuda and verify the installation.,0
2.3.pip wheels - windows,#pip-wheels-windows,"2.3.pip wheels - windows nvidia provides python wheels for installing cuda through pip, primarily for using cuda with python. these packages are intended for runtime use and do not currently include developer tools (these can be installed separately). please note that with this installation method, cuda installation environment is managed via pip and additional care must be taken to set up your host environment to use cuda outside the pip environment. prerequisites to install wheels, you must first install thenvidia-pyindexpackage, which is required in order to set up your pip installation to fetch additional python modules from the nvidia ngc pypi repo. if your pip and setuptools python modules are not up-to-date, then use the following command to upgrade these python modules. if these python modules are out-of-date then the commands which follow later in this section may fail. you should now be able to install thenvidia-pyindexmodule. if your project is using arequirements.txtfile, then you can add the following line to yourrequirements.txtfile as an alternative to installing thenvidia-pyindexpackage: procedure install the cuda runtime package: optionally, install additional packages as listed below using the following command: metapackages the following metapackages will install the latest version of the named component on windows for the indicated cuda version. cu12 should be read as cuda12. these metapackages install the following packages:",0
2.4.conda,#conda,"2.4.conda the conda packages are available athttps://anaconda.org/nvidia. installation to perform a basic install of all cuda toolkit components using conda, run the following command: uninstallation to uninstall the cuda toolkit using conda, run the following command:",0
3.linux,#linux,"3.linux cuda on linux can be installed using an rpm, debian, runfile, or conda package, depending on the platform being installed on.",0
3.1.linux x86_64,#linux-x86-64,"3.1.linux x86_64 for development on the x86_64 architecture. in some cases, x86_64 systems may act as host platforms targeting other architectures. see thelinux installation guidefor more details.",9
3.1.1.redhat / centos,#redhat-centos,"3.1.1.redhat / centos when installing cuda on redhat or centos, you can choose between the runfile installer and the rpm installer. the runfile installer is only available as a local installer. the rpm installer is available as both a local installer and a network installer. the network installer allows you to download only the files you need. the local installer is a stand-alone installer with a large initial download. in the case of the rpm installers, the instructions for the local and network variants are the same. for more details, refer to thelinux installation guide.",0
3.1.2.fedora,#fedora,"3.1.2.fedora when installing cuda on fedora, you can choose between the runfile installer and the rpm installer. the runfile installer is only available as a local installer. the rpm installer is available as both a local installer and a network installer. the network installer allows you to download only the files you need. the local installer is a stand-alone installer with a large initial download. in the case of the rpm installers, the instructions for the local and network variants are the same. for more details, refer to thelinux installation guide.",0
3.1.3.suse linux enterprise server,#suse-linux-enterprise-server,"3.1.3.suse linux enterprise server when installing cuda on suse linux enterprise server, you can choose between the runfile installer and the rpm installer. the runfile installer is only available as a local installer. the rpm installer is available as both a local installer and a network installer. the network installer allows you to download only the files you need. the local installer is a stand-alone installer with a large initial download. in the case of the rpm installers, the instructions for the local and network variants are the same. for more details, refer to thelinux installation guide.",0
3.1.4.opensuse,#opensuse,"3.1.4.opensuse when installing cuda on opensuse, you can choose between the runfile installer and the rpm installer. the runfile installer is only available as a local installer. the rpm installer is available as both a local installer and a network installer. the network installer allows you to download only the files you need. the local installer is a stand-alone installer with a large initial download. in the case of the rpm installers, the instructions for the local and network variants are the same. for more details, refer to thelinux installation guide.",0
3.1.5.amazon linux 2023,#amazon-linux-2023,3.1.5.amazon linux 2023,9
3.1.6.pip wheels - linux,#pip-wheels-linux,"3.1.6.pip wheels - linux nvidia provides python wheels for installing cuda through pip, primarily for using cuda with python. these packages are intended for runtime use and do not currently include developer tools (these can be installed separately). please note that with this installation method, cuda installation environment is managed via pip and additional care must be taken to set up your host environment to use cuda outside the pip environment. prerequisites to install wheels, you must first install thenvidia-pyindexpackage, which is required in order to set up your pip installation to fetch additional python modules from the nvidia ngc pypi repo. if your pip and setuptools python modules are not up-to-date, then use the following command to upgrade these python modules. if these python modules are out-of-date then the commands which follow later in this section may fail. you should now be able to install thenvidia-pyindexmodule. if your project is using arequirements.txtfile, then you can add the following line to yourrequirements.txtfile as an alternative to installing thenvidia-pyindexpackage: procedure install the cuda runtime package: optionally, install additional packages as listed below using the following command: metapackages the following metapackages will install the latest version of the named component on linux for the indicated cuda version. cu12 should be read as cuda12. these metapackages install the following packages:",0
3.1.7.conda,#x86-64-conda,"3.1.7.conda the conda packages are available athttps://anaconda.org/nvidia. installation to perform a basic install of all cuda toolkit components using conda, run the following command: uninstallation to uninstall the cuda toolkit using conda, run the following command:",0
3.1.8.wsl,#wsl,3.1.8.wsl these instructions must be used if you are installing in a wsl environment. do not use the ubuntu instructions in this case.,9
3.1.9.ubuntu,#ubuntu,"3.1.9.ubuntu when installing cuda on ubuntu, you can choose between the runfile installer and the debian installer. the runfile installer is only available as a local installer. the debian installer is available as both a local installer and a network installer. the network installer allows you to download only the files you need. the local installer is a stand-alone installer with a large initial download. in the case of the debian installers, the instructions for the local and network variants are the same. for more details, refer to thelinux installation guide.",0
3.1.10.debian,#debian,"3.1.10.debian when installing cuda on debian 10, you can choose between the runfile installer and the debian installer. the runfile installer is only available as a local installer. the debian installer is available as both a local installer and a network installer. the network installer allows you to download only the files you need. the local installer is a stand-alone installer with a large initial download. for more details, refer to thelinux installation guide.",0
4.notices,#notices,4.notices,9
4.1.notice,#notice,"4.1.notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation (nvidia) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material (defined below), code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer (terms of sale). nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion and/or use of nvidia products in such equipment or applications and therefore such inclusion and/or use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions and/or requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the nvidia product in any manner that is contrary to this document or (ii) customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding third-party products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents (together and separately, materials) are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product.",4
4.2.opencl,#opencl,4.2.opencl opencl is a trademark of apple inc. used under license to the khronos group inc.,4
4.3.trademarks,#trademarks,4.3.trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.,4
1.2.limitations,#limitations,1.2.limitations the following license limitations apply to your use of the sdk:,9
1.3.ownership,#ownership,1.3.ownership,9
1.4.no warranties,#no-warranties,"1.4.no warranties the sdk is provided by nvidia as is and with all faults. to the maximum extent permitted by law, nvidia and its affiliates expressly disclaim all warranties of any kind or nature, whether express, implied or statutory, including, but not limited to, any warranties of merchantability, fitness for a particular purpose, title, non-infringement, or the absence of any defects therein, whether latent or patent. no warranty is made on the basis of trade usage, course of dealing or course of trade.",4
1.5.limitation of liability,#limitation-of-liability,"1.5.limitation of liability to the maximum extent permitted by law, nvidia and its affiliates shall not be liable for any (i) special, incidental, punitive or consequential damages, or (ii) damages for (a) any lost profits, loss of use, loss of data or loss of goodwill, or (b) the costs of procuring substitute products, arising out of or in connection with this agreement or the use or performance of the sdk, whether such liability arises from any claim based upon breach of contract, breach of warranty, tort (including negligence), product liability or any other cause of action or theory of liability. in no event will nvidias and its affiliates total cumulative liability under or arising out of this agreement exceed us$10.00. the nature of the liability or the number of claims or suits shall not enlarge or extend this limit. these exclusions and limitations of liability shall apply regardless if nvidia or its affiliates have been advised of the possibility of such damages, and regardless of whether a remedy fails its essential purpose. these exclusions and limitations of liability form an essential basis of the bargain between the parties, and, absent any of these exclusions or limitations of liability, the provisions of this agreement, including, without limitation, the economic terms, would be substantially different.",4
1.6.termination,#termination,1.6.termination,9
1.7.general,#general,"1.7.general if you wish to assign this agreement or your rights and obligations, including by merger, consolidation, dissolution or operation of law, contact nvidia to ask for permission. any attempted assignment not approved by nvidia in writing shall be void and of no effect. nvidia may assign, delegate or transfer this agreement and its rights and obligations, and if to a non-affiliate you will be notified. you agree to cooperate with nvidia and provide reasonably requested information to verify your compliance with this agreement. this agreement will be governed in all respects by the laws of the united states and of the state of delaware, without regard to the conflicts of laws principles. the united nations convention on contracts for the international sale of goods is specifically disclaimed. you agree to all terms of this agreement in the english language. the state or federal courts residing in santa clara county, california shall have exclusive jurisdiction over any dispute or claim arising out of this agreement. notwithstanding this, you agree that nvidia shall still be allowed to apply for injunctive remedies or an equivalent type of urgent legal relief in any jurisdiction. if any court of competent jurisdiction determines that any provision of this agreement is illegal, invalid or unenforceable, such provision will be construed as limited to the extent necessary to be consistent with and fully enforceable under the law and the remaining provisions will remain in full force and effect. unless otherwise specified, remedies are cumulative. each party acknowledges and agrees that the other is an independent contractor in the performance of this agreement. the sdk has been developed entirely at private expense and is commercial items consisting of commercial computer software and commercial computer software documentation provided with restricted rights. use, duplication or disclosure by the u.s. government or a u.s. government subcontractor is subject to the restrictions in this agreement pursuant to dfars 227.7202-3(a) or as set forth in subparagraphs (c)(1) and (2) of the commercial computer software - restricted rights clause at far 52.227-19, as applicable. contractor/manufacturer is nvidia, 2788 san tomas expressway, santa clara, ca 95051. the sdk is subject to united states export laws and regulations. you agree that you will not ship, transfer or export the sdk into any country, or use the sdk in any manner, prohibited by the united states bureau of industry and security or economic sanctions regulations administered by the u.s. department of treasurys office of foreign assets control (ofac), or any applicable export laws, restrictions or regulations. these laws include restrictions on destinations, end users and end use. by accepting this agreement, you confirm that you are not located in a country currently embargoed by the u.s. or otherwise prohibited from receiving the sdk under u.s. law. any notice delivered by nvidia to you under this agreement will be delivered via mail, email or fax. you agree that any notices that nvidia sends you electronically will satisfy any legal communication requirements. please direct your legal notices or other correspondence to nvidia corporation, 2788 san tomas expressway, santa clara, california 95051, united states of america, attention: legal department. this agreement and any exhibits incorporated into this agreement constitute the entire agreement of the parties with respect to the subject matter of this agreement and supersede all prior negotiations or documentation exchanged between the parties relating to this sdk license. any additional and/or conflicting terms on documents issued by you are null, void, and invalid. any amendment or waiver under this agreement shall be in writing and signed by representatives of both parties.",4
2.cuda toolkit supplement to software license agreement for nvidia software development kits,#cuda-toolkit-supplement-to-software-license-agreement-for-nvidia-software-development-kits,"2.cuda toolkit supplement to software license agreement for nvidia software development kits the terms in this supplement govern your use of the nvidia cuda toolkit sdk under the terms of your license agreement (agreement) as modified by this supplement. capitalized terms used but not defined below have the meaning assigned to them in the agreement. this supplement is an exhibit to the agreement and is incorporated as an integral part of the agreement. in the event of conflict between the terms in this supplement and the terms in the agreement, the terms in this supplement govern.",4
2.1.license scope,#license-scope,2.1.license scope the sdk is licensed for you to develop applications only for use in systems with nvidia gpus.,4
2.2.distribution,#distribution,2.2.distribution the portions of the sdk that are distributable under the agreement are listed inattachment a.,9
2.3.operating systems,#operating-systems,"2.3.operating systems those portions of the sdk designed exclusively for use on the linux or freebsd operating systems, or other operating systems derived from the source code to these operating systems, may be copied and redistributed for use in accordance with this agreement, provided that the object code files are not modified in any way (except for unzipping of compressed files).",9
2.4.audio and video encoders and decoders,#audio-and-video-encoders-and-decoders,"2.4.audio and video encoders and decoders you acknowledge and agree that it is your sole responsibility to obtain any additional third-party licenses required to make, have made, use, have used, sell, import, and offer for sale your products or services that include or incorporate any third-party software and content relating to audio and/or video encoders and decoders from, including but not limited to, microsoft, thomson, fraunhofer iis, sisvel s.p.a., mpeg-la, and coding technologies. nvidia does not grant to you under this agreement any necessary patent or other rights with respect to any audio and/or video encoders and decoders.",4
2.5.licensing,#licensing,"2.5.licensing if the distribution terms in this agreement are not suitable for your organization, or for any questions regarding this agreement, please contact nvidia atnvidia-compute-license-questions@nvidia.com.",4
2.6.attachment a,#attachment-a,"2.6.attachment a the following cuda toolkit files may be distributed with applications developed by you, including certain variations of these files that have version number or architecture specific information embedded in the file name - as an example only, for release version 9.0 of the 64-bit windows software, the file cudart64_90.dll is redistributable. in addition to the rights above, for parties that are developing software intended solely for use on jetson development kits or jetson modules, and running linux for tegra software, the following shall apply:",0
2.7.attachment b,#attachment-b,2.7.attachment b additional licensing obligations the following third party components included in the software are licensed to licensee pursuant to the following terms and conditions:,9
4.12.cudbgeventcallbackdata struct reference,https://docs.nvidia.com/cuda/debugger-api/structCUDBGEventCallbackData.html#structCUDBGEventCallbackData,4.12.cudbgeventcallbackdata struct reference,8
[events],group__EVENT.html,[events],9
public variables,https://docs.nvidia.com/cuda/debugger-api/structCUDBGEventCallbackData.html#structCUDBGEventCallbackData,public variables,9
variables,https://docs.nvidia.com/cuda/debugger-api/structCUDBGEventCallbackData.html#structCUDBGEventCallbackData,variables,9
1.release notes,https://docs.nvidia.com/cuda/debugger-api/release-notes.html#release-notes-6-5,1.release notes,9
1.1.11.8 release,https://docs.nvidia.com/cuda/debugger-api/release-notes.html#release-notes-6-5,1.1.11.8 release,9
1.2.12.3 release,https://docs.nvidia.com/cuda/debugger-api/release-notes.html#release-notes-6-5,1.2.12.3 release,9
1.3.7.0 release,https://docs.nvidia.com/cuda/debugger-api/release-notes.html#release-notes-6-5,1.3.7.0 release,9
1.4.6.5 release,https://docs.nvidia.com/cuda/debugger-api/release-notes.html#release-notes-6-5,1.4.6.5 release,9
1.using inline ptx assembly in cuda,#using-inline-ptx-assembly-in-cuda,"1.using inline ptx assembly in cuda the nvidiacudaprogramming environment provides a parallel thread execution (ptx) instruction set architecture (isa) for using the gpu as a data-parallel computing device. for more information on the ptx isa, refer to the latest version of theptx isa reference document. this application note describes how to inline ptx assembly language statements into cuda code.",0
1.1.assembler (asm) statements,#assembler-asm-statements,"1.1.assembler (asm) statements assembler statements,asm(), provide a way to insert arbitrary ptx code into your cuda program. a simple example is: this inserts a ptxmembar.glinto your generated ptx code at the point of theasm()statement.",7
1.1.1.parameters,#parameters,"1.1.1.parameters anasm()statement becomes more complicated, and more useful, when we pass values in and out of the asm. the basic syntax is as follows: where you can have multiple input or output operands separated by commas. the template string contains ptx instructions with references to the operands. multiple ptx instructions can be given by separating them with semicolons. a simple example is as follows: each%nin the template string is an index into the following list of operands, in text order. so%0refers to the first operand,%1to the second operand, and so on. since the output operands are always listed ahead of the input operands, they are assigned the smallest indices. this example is conceptually equivalent to the following: note that the numbered references in the string can be in arbitrary order. the following is equivalent to the above example: you can also repeat a reference, e.g.: is conceptually if there is no input operand, you can drop the final colon, e.g.: if there is no output operand, the colon separators are adjacent, e.g.: if you want the%in a ptx instruction, then you should escape it with double%%, e.g.: the above was simplified to explain the ordering of the string%references. in reality, the operand values are passed via whatever mechanism the constraint specifies. the full list of constraints will be explained later, but the r constraint refers to a 32bit integer register. so the earlier exampleasm()statement: produces the following code sequence in the output generated by the compiler: this is where the distinction between input and output operands becomes important. the input operands are loaded into registers before theasm()statement, then the result register is stored to the output operand. the = modifier in =r specifies that the register is written to. there is also available a + modifier that specifies the register is both read and written, e.g.: multiple instructions can be combined into a singleasm()statement; basically, anything legal can be put into the asm string. multiple instructions can be split across multiple lines by making use of c/c++s implicit string concatenation. both c++ style line end comments // and classical c-style comments /**/ can be interspersed with these strings. to generate readable output in the ptx intermediate file it is best practice to terminate each instruction string except the last one with nt. for example, a cube routine could be written as: if an output operand is conditionally updated by the asm instructions, then the + modifier should be used. there is an implicit use of the output operand in such a case. for example,",7
1.1.2.constraints,#constraints,"1.1.2.constraints there is a separate constraint letter for each ptx register type: example: generates: the constraint""n""may be used for immediate integer operands with a known value. example: generates: the constraint""c""can be used for operand of type array of const char, where the array contents are known at compile time.
it is intended to allow customization of ptx instruction modes based on compile time computation (see examples). here is the specification
for the""c""constraint: theconstant-expressionis evaluated during compilation and shall generate the address of a variablev, where: during translation, the compiler will replace a reference to the operand within theassembler templatewith the contents ofvs initializer, except for the last trailing zero.
no constraint modifiers are allowed for this constraint. this constraint can only be used in device code. (terms initalicsare c++ standard terms and/or terms from the gnu inline asm specification). heres an example of the use ofcconstraint to generate different ptx instruction modes based on compile time computation: other examples (compile in c++17 or later dialect): there is no constraint letter for 8-bit wide ptx registers. ptx instructions types accepting 8-bit wide typespermit operands to be wider than the instruction-type size. example: generates: the behavior of using a constraint string that is not one of those specified above is undefined.",7
1.2.pitfalls,#pitfalls,"1.2.pitfalls althoughasm()statements are very flexible and powerful, you may encounter some pitfallsthese are listed in this section.",9
1.2.1.namespace conflicts,#namespace-conflicts,"1.2.1.namespace conflicts if the cube function (described before) is called and inlined multiple times in the code, it generates an error about duplicate definitions of the temp register t1. to avoid this error you need to: note that you can similarly use braces for local labels inside theasm()statement.",6
1.2.2.memory space conflicts,#memory-space-conflicts,"1.2.2.memory space conflicts sinceasm()statements have no way of knowing what memory space a register is in, the user must make sure that the appropriate ptx instruction is used. forsm_20and greater, any pointer argument to anasm()statement is passed as a generic address.",7
1.2.3.incorrect optimization,#incorrect-optimization,"1.2.3.incorrect optimization the compiler assumes that anasm()statement has no side effects except to change the output operands. to ensure that the asm is not deleted or moved during generation of ptx, you should use the volatile keyword, e.g.: normally any memory that is written to will be specified as an out operand, but if there is a hidden side effect on user memory (for example, indirect access of a memory location via an operand), or if you want to stop any memory optimizations around theasm()statement performed during generation of ptx, you can add a memory clobbers specification after a 3rd colon, e.g.:",7
1.2.4.incorrect ptx,#incorrect-ptx,"1.2.4.incorrect ptx the compiler front end does not parse theasm()statement template string and does not know what it means or even whether it is valid ptx input. so if there are any errors in the string it will not show up untilptxas. for example, if you pass a value with an r constraint but use it in anadd.f64you will get a parse error from ptxas. similarly, operand modifiers are not supported. for example, in the n modifier in %n1 is not supported and will be passed toptxas, where it can cause undefined behavior. refer to the document nvcc.pdf for further compiler related details.",7
1.3.error checking,#error-checking,1.3.error checking the following are some of the error checks that the compiler will do on inlineptxasm:,6
2.notices,#notices,2.notices,9
2.1.notice,#notice,"2.1.notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation (nvidia) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material (defined below), code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer (terms of sale). nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion and/or use of nvidia products in such equipment or applications and therefore such inclusion and/or use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions and/or requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the nvidia product in any manner that is contrary to this document or (ii) customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding third-party products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents (together and separately, materials) are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product.",4
2.2.opencl,#opencl,2.2.opencl opencl is a trademark of apple inc. used under license to the khronos group inc.,4
2.3.trademarks,#trademarks,2.3.trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.,4
3.1.general,https://docs.nvidia.com/cuda/debugger-api/group__GENERAL.html#group__GENERAL,3.1.general,9
enumerations,https://docs.nvidia.com/cuda/debugger-api/group__GENERAL.html#group__GENERAL,enumerations,9
4.4.cudbgevent::cases_st::contextcreate_st struct reference,https://docs.nvidia.com/cuda/debugger-api/structCUDBGEvent_1_1cases__st_1_1contextCreate__st.html#structCUDBGEvent_1_1cases__st_1_1contextCreate__st,4.4.cudbgevent::cases_st::contextcreate_st struct reference,8
3.10.events,https://docs.nvidia.com/cuda/debugger-api/group__EVENT.html#group__EVENT,3.10.events,9
classes,https://docs.nvidia.com/cuda/debugger-api/group__EVENT.html#group__EVENT,classes,9
typedefs,https://docs.nvidia.com/cuda/debugger-api/group__EVENT.html#group__EVENT,typedefs,6
3.7.grid properties,https://docs.nvidia.com/cuda/debugger-api/group__GRID.html#group__GRID,3.7.grid properties,9
3.8.device properties,https://docs.nvidia.com/cuda/debugger-api/group__DEV.html#group__DEV,3.8.device properties,9
1.overview,#overview,"1.overview gpudirect rdma is a technology introduced in kepler-class gpus and cuda 5.0 that enables a direct path for data exchange between the gpu and a third-party peer device using standard features of pci express. examples of third-party devices are: network interfaces, video acquisition devices, storage adapters. gpudirect rdma is available on both tesla and quadro gpus. a number of limitations can apply, the most important being that the two devices must share the same upstream pci express root complex. some of the limitations depend on the platform used and could be lifted in current/future products. a few straightforward changes must be made to device drivers to enable this functionality with a wide range of hardware devices. this document introduces the technology and describes the steps necessary to enable an gpudirect rdma connection to nvidia gpus on linux.",0
1.1.how gpudirect rdma works,#how-gpudirect-rdma-works,"1.1.how gpudirect rdma works when setting up gpudirect rdma communication between two peers, all physical addresses are the same from the pci express devices point of view. within this physical address space are linear windows called pci bars. each device has six bar registers at most, so it can have up to six active 32bit bar regions. 64bit bars consume two bar registers. the pci express device issues reads and writes to a peer devices bar addresses in the same way that they are issued to system memory. traditionally, resources like bar windows are mapped to user or kernel address space using the cpus mmu as memory mapped i/o (mmio) addresses. however, because current operating systems dont have sufficient mechanisms for exchanging mmio regions between drivers, the nvidia kernel driver exports functions to perform the necessary address translations and mappings. to add gpudirect rdma support to a device driver, a small amount of address mapping code within the kernel driver must be modified. this code typically resides near existing calls toget_user_pages(). the apis and control flow involved with gpudirect rdma are very similar to those used with standard dma transfers. seesupported systemsandpci bar sizesfor more hardware details.",5
1.2.standard dma transfer,#standard-dma-transfer,"1.2.standard dma transfer first, we outline a standard dma transfer initiated from userspace. in this scenario, the following components are present: the general sequence is as follows:",6
1.3.gpudirect rdma transfers,#gpudirect-rdma-transfers,"1.3.gpudirect rdma transfers for the communication to support gpudirect rdma transfers some changes to the sequence above have to be introduced. first of all, two new components are present: as described inbasics of uva cuda memory management, programs using the cuda library have their address space split between gpu and cpu virtual addresses, and the communication library has to implement two separate paths for them. the userspace cuda library provides a function that lets the communication library distinguish between cpu and gpu addresses. moreover, for gpu addresses it returns additional metadata that is required to uniquely identify the gpu memory represented by the address. seeuserspace apifor details. the difference between the paths for cpu and gpu addresses is in how the memory is pinned and unpinned. for cpu memory this is handled by built-in linux kernel functions (get_user_pages()andput_page()). however, in the gpu memory case the pinning and unpinning has to be handled by functions provided by the nvidia kernel driver. seepinning gpu memoryandunpinning gpu memoryfor details. some hardware caveats are explained insupported systemsandpci bar sizes.",5
1.4.changes in cuda 6.0,#changes-in-cuda-6-0,1.4.changes in cuda 6.0 in this section we briefly list the changes that are available in cuda 6.0: caveats as of cuda 6.0:,0
1.5.changes in cuda 7.0,#changes-in-cuda-7-0,1.5.changes in cuda 7.0 in this section we briefly list the changes that are available in cuda 7.0:,0
1.6.changes in cuda 8.0,#changes-in-cuda-8-0,1.6.changes in cuda 8.0 in this section we briefly list the changes that are available in cuda 8.0:,0
1.7.changes in cuda 10.1,#changes-in-cuda-10-1,1.7.changes in cuda 10.1 gpudirect rdma is supported on jetson agx xavier platform. seeporting to tegrasection for details.,0
1.8.changes in cuda 11.2,#changes-in-cuda-11-2,1.8.changes in cuda 11.2 gpudirect rdma is supported on drive agx xavier linux based platform. seeporting to tegrasection for details.,0
1.9.changes in cuda 11.4,#changes-in-cuda-11-4,"1.9.changes in cuda 11.4 added a new a kernel module,nvidia-peermem, which provides nvidia infiniband-based hcas (host channel adapters) direct peer-to-peer read and write access to the nvidia gpus video memory. seeusing nvidia-peermemfor details. gpudirect rdma is supported on jetson orin platform. seeporting to tegrasection for details. known issue: currently, there is no service to automatically loadnvidia-peermem. users need to load the module manually.",0
1.10.changes in cuda 12.2,#changes-in-cuda-12-2,"1.10.changes in cuda 12.2 in drivers released from the r515 up to the r535 branches, except for newer r525 and r535 releases mentioned below, there is a race bug which may show up as a kernel null-pointer dereference. this happens when the gpu invokes the (hereby i/o) kernel driver invalidation callback, the one which was registered during the call to nvidia_p2p_get_pages, concurrently with the i/o driver calling nvidia_p2p_put_pages.
the race bug does not affect the persistent mapping case, as in that case an invalidation callback is not supported nor needed. the bug fix required the following api change: note that i/o drivers, which do not need persistent mappings, do not require source code changes. the api changes described above are deployed in the r535 branch, specifically in release 535.14 and later, and have also been back-ported to the r525 branch, for teslard3 (525.105.17) and later.",0
2.design considerations,#design-considerations,"2.design considerations when designing a system to utilize gpudirect rdma, there a number of considerations which should be taken into account.",9
2.1.lazy unpinning optimization,#lazy-unpinning-optimization,"2.1.lazy unpinning optimization pinning gpu device memory in bar is an expensive operation, taking up to milliseconds. therefore the application should be designed in a way to minimize that overhead. the most straightforward implementation using gpudirect rdma would pin memory before each transfer and unpin it right after the transfer is complete. unfortunately, this would perform poorly in general, as pinning and unpinning memory are expensive operations. the rest of the steps required to perform an rdma transfer, however, can be performed quickly without entering the kernel (the dma list can be cached and replayed using mmio registers/command lists). hence, lazily unpinning memory is key to a high performance rdma implementation. what it implies, is keeping the memory pinned even after the transfer has finished. this takes advantage of the fact that it is likely that the same memory region will be used for future dma transfers thus lazy unpinning saves pin/unpin operations. an example implementation of lazy unpinning would keep a set of pinned memory regions and only unpin some of them (for example the least recently used one) if the total size of the regions reached some threshold, or if pinning a new region failed because of bar space exhaustion (seepci bar sizes).",5
2.2.registration cache,#registration-cache,"2.2.registration cache communication middleware often employs an optimization called a registration cache, or pin-down cache, to minimize pinning overhead. typically it already exists for host memory, implementing lazy unpinning, lru de-registration, etc. for networking middleware, such caches are usually implemented in user-space, as they are used in combination with hardware capable of user-mode message injection. cuda uva memory address layout enables gpu memory pinning to work with these caches by taking into account just a few design considerations. in the cuda environment, this is even more important as the amount of memory which can be pinned may be significantly more constrained than for host memory. as the gpu bar space is typically mapped using 64kb pages, it is more resource efficient to maintain a cache of regions rounded to the 64kb boundary. even more so, as two memory areas which are in the same 64kb boundary would allocate and return the same bar mapping. registration caches usually rely on the ability to intercept deallocation events happening in the user application, so that they can unpin the memory and free important hw resources, e.g. on the network card. to implement a similar mechanism for gpu memory, an implementation has two options: there is a sample application,7_cudalibraries/cuhook, showing how to intercept calls to cuda apis at run-time, which can be used to detect gpu memory de/allocations. while intercepting cuda apis is beyond the scope of this document, an approach to performing tag checks is available starting with cuda 6.0. it involves the usage of thecu_pointer_attribute_buffer_idattribute incupointergetattribute()(orcupointergetattributes()if more attributes are needed) to detect memory buffer deallocations or reallocations. the api will return a different id value in case of reallocation or an error if the buffer address is no longer valid. seeuserspace apifor api usage.",5
2.3.unpin callback,#unpin-callback,"2.3.unpin callback when a third party device driver pins the gpu pages withnvidia_p2p_get_pages()it must also provide a callback function that the nvidia driver will call if it needs to revoke access to the mapping.this callback occurs synchronously, giving the third party driver the opportunity to clean up and remove any references to the pages in question (i.e., wait for outstanding dmas to complete).the user callback function may block for a few milliseconds, although it is recommended that the callback complete as quickly as possible. care has to be taken not to introduce deadlocks as waiting within the callback for the gpu to do anything is not safe. the callback must callnvidia_p2p_free_page_table()(notnvidia_p2p_put_pages()) to free the memory pointed to bypage_table. the corresponding mapped memory areas will only be unmapped by the nvidia driver after returning from the callback. note that the callback will be invoked in two scenarios: in the latter case there can be tear-down ordering issues between closing the file descriptor of the third party kernel driver and that of the nvidia kernel driver. in the case the file descriptor for the nvidia kernel driver is closed first, thenvidia_p2p_put_pages()callback will be invoked. a proper software design is important as the nvidia kernel driver will protect itself from reentrancy issues with locks before invoking the callback. the third party kernel driver will almost certainly take similar actions, so dead-locking or live-locking scenarios may arise if careful consideration is not taken.",5
2.4.supported systems,#supported-systems,"2.4.supported systems general remarks even though the only theoretical requirement for gpudirect rdma to work between a third-party device and an nvidia gpu is that they share the same root complex, there exist bugs (mostly in chipsets) causing it to perform badly, or not work at all in certain setups. we can distinguish between three situations, depending on what is on the path between the gpu and the third-party device: the first situation, where there are only pcie switches on the path, is optimal and yields the best performance. the second one, where a single cpu/ioh is involved, works, but yields worse performance ( especially peer-to-peer read bandwidth has been shown to be severely limited on some processor architectures ). finally, the third situation, where the path traverses a qpi/ht link, may be extremely performance-limited or even not work reliably. platform support for ibm power8 platform, gpudirect rdma and p2p are not supported, but are not explicitly disabled. they may not work at run-time. gpudirect rdma is supported on jetson agx xavier platform starting from cuda 10.1 and on drive agx xavier linux based platforms from cuda 11.2. see sectionporting to tegrafor details. on arm64, the necessary peer-to-peer functionality depends on both the hardware and the software of the particular platform. so while gpudirect rdma is not explicitly disabled on non-jetson and non-drive platforms, there are no guarantees that it will be fully functional. iommus gpudirect rdma currently relies upon all physical addresses being the same from the different pci devices point of view. this makes it incompatible with iommus performing any form of translation other than 1:1, hence they must be disabled or configured for pass-through translation for gpudirect rdma to work.",5
2.5.pci bar sizes,#pci-bar-sizes,"2.5.pci bar sizes pci devices can ask the os/bios to map a region of physical address space to them. these regions are commonly called bars. nvidia gpus currently expose multiple bars, and some of them can back arbitrary device memory, making gpudirect rdma possible.
the maximum bar size available for gpudirect rdma differs from gpu to gpu. for example, currently the smallest available bar size on kepler class gpus is 256 mb. of that, 32mb are currently reserved for internal use. these sizes may change. on some tesla-class gpus a large bar feature is enabled, e.g. bar1 size is set to 16gb or larger. large bars can pose a problem for the bios, especially on older motherbords, related to compatibility support for 32bit operating systems. on those motherboards the bootstrap can stop during the early post phase, or the gpu may be misconfigured and so unusable. if this appears to be occuring it might be necessary to enable some special bios feature to deal with the large bar issue. please consult your system vendor for more details regarding large bar support.",5
2.6.tokens usage,#tokens-usage,"2.6.tokens usage as can be seen inuserspace apiandkernel api, one method for pinning and unpinning memory requires two tokens in addition to the gpu virtual address. these tokens,p2ptokenandvaspacetoken, are necessary to uniquely identify a gpu va space. a process identifier alone does not identify a gpu va space. the tokens are consistent within a single cuda context (i.e., all memory obtained throughcudamalloc()within the same cuda context will have the samep2ptokenandvaspacetoken). however, a given gpu virtual address need not map to the same context/gpu for its entire lifetime. as a concrete example: that is, the same address, when passed tocupointergetattribute, may return different tokens at different times during the programs execution. therefore, the third party communication library must callcupointergetattribute()for every pointer it operates on. security implications the two tokens act as an authentication mechanism for the nvidia kernel driver. if you know the tokens, you can map the address space corresponding to them, and the nvidia kernel driver doesnt perform any additional checks. the 64bitp2ptokenis randomized to prevent it from being guessed by an adversary. when no tokens are used, the nvidia driver limits thekernel apito the process which owns the memory allocation.",5
2.7.synchronization and memory ordering,#synchronization-and-memory-ordering,"2.7.synchronization and memory ordering gpudirect rdma introduces a new independent gpu data flow path exposed to third party devices and it is important to understand how these devices interact with the gpus relaxed memory model. registration for cuda api consistency registration is necessary to ensure the cuda api memory operations visible to a bar mapping happen before the api call returns control to the calling cpu thread. this provides a consistent view of memory to a device using gpudirect rdma mappings when invoked after a cuda api in the thread. this is a strictly more conservative mode of operation for the cuda api and disables optimizations, thus it may negatively impact performance. this behavior is enabled on a per-allocation granularity either by callingcupointersetattribute()with thecu_pointer_attribute_sync_memopsattribute, or p2p tokens are retrieved for a buffer when using the legacy path. seeuserspace apifor more details. an example situation would be read-after-write dependency between acumemcpydtod()and subsequent gpudirect rdma read operation on the destination of the copy. as an optimization the device-to-device memory copy typically returns asynchronously to the calling thread after queuing the copy to the gpu scheduler. however, in this circumstance that will lead to inconsistent data read via the bar mapping, so this optimization is disabled an the copy completed before the cuda api returns. cuda apis for memory ordering only cpu initiated cuda apis provide ordering of gpudirect memory operations as observed by the gpu. that is, despite a third party device having issued all pcie transactions, a running gpu kernel or copy operation may observe stale data or data that arrives out-of-order until a subsequent cpu initiated cuda work submission or synchronization api. to ensure that memory updates are visible to cuda kernels or copies, an implementation should ensure that all writes to the gpu bar happen before control is returned to the cpu thread which will invoke the dependent cuda api. an example situation for a network communication scenario is when a network rdma write operation is completed by the third party network device and the data is written to the gpu bar mapping. though reading back the written data either through gpu bar or a cuda memory copy operation, will return the newly written data, a concurrently running gpu kernel to that network write might observe stale data, the data partially written, or the data written out-of-order. in short, a gpu kernel is wholly inconsistent with concurrent rdma for gpudirect operations and accessing the memory overwritten by the third party device in such a situation would be considered a data race. to resolve this inconsistency and remove the data race the dma write operation must complete with respect to the cpu thread which will launch the dependent gpu kernel.",5
3.how to perform specific tasks,#how-to-perform-specific-tasks,3.how to perform specific tasks,9
3.1.displaying gpu bar space,#displaying-gpu-bar-space,"3.1.displaying gpu bar space starting in cuda 6.0 the nvidia smi utility provides the capability to dump bar1 memory usage. it can be used to understand the application usage of bar space, the primary resource consumed by gpudirect rdma mappings. gpu memory is pinned in fixed size chunks, so the amount of space reflected here might be unexpected. in addition, a certain amount of bar space is reserved by the driver for internal use, so not all available memory may be usable via gpudirect rdma. note that the same ability is offered programmatically through thenvmldevicegetbar1memoryinfo()nvml api.",0
3.2.pinning gpu memory,#pinning-gpu-memory,3.2.pinning gpu memory,5
3.3.unpinning gpu memory,#unpinning-gpu-memory,"3.3.unpinning gpu memory in the kernel driver, invokenvidia_p2p_put_pages(). seekernel apifor details onnvidia_p2p_put_pages(). starting cuda 6.0 zeros should be used as the token parameters. note thatnvidia_p2p_put_pages()must be called from within the same process context as the one from which the correspondingnvidia_p2p_get_pages()has been issued.",0
3.4.handling the free callback,#handling-the-free-callback,3.4.handling the free callback,9
3.5.buffer id tag check for a registration cache,#buffer-id-tag-check-for-a-registration-cache,"3.5.buffer id tag check for a registration cache remember that a solution built around buffer id tag checking is not recommended for latency sensitive implementations. instead, instrumentation of cuda allocation and deallocation apis to provide callbacks to the registration cache is recommended, removing tag checking overhead from the critical path.",5
3.6.linking a kernel module against nvidia.ko,#linking-a-kernel-module-against-nvidia-ko,3.6.linking a kernel module against nvidia.ko,4
3.7.using nvidia-peermem,#using-nvidia-peermem,"3.7.using nvidia-peermem the nvidia gpu driver package provides a kernel module,nvidia-peermem, which provides nvidia infiniband based hcas (host channel adapters) direct peer-to-peer read and write access to the nvidia gpus video memory. it allows gpudirect rdma-based applications to use gpu computing power with the rdma interconnect without needing to copy data to host memory. this capability is supported with nvidia connectx-3 vpi or newer adapters. it works with both infiniband and roce (rdma over converged ethernet) technologies. nvidia ofed (open fabrics enterprise distribution), or mlnx_ofed, introduces an api between the infiniband core and peer memory clients such as nvidia gpus. thenvidia-peermemmodule registers the nvidia gpu with the infiniband subsystem by using peer-to-peer apis provided by the nvidia gpu driver. the kernel must have the required support for rdma peer memory either through additional patches to the kernel or via mlnx_ofed as a prerequisite for loading and usingnvidia-peermem. it is possible that thenv_peer_memmodule from the github project may be installed and loaded on the system. installation ofnvidia-peermemwill not affect the functionality of the existingnv_peer_memmodule. but, to load and usenvidia-peermem, users must disable thenv_peer_memservice. additionally, it is encouraged to uninstall thenv_peer_mempackage to avoid any conflict withnvidia-peermemsince only one module can be loaded at any time. to stop thenv_peer_memservice: check ifnv_peer_mem.kois still loaded after stopping the service: ifnv_peer_mem.kois still loaded, unload it using: uninstall thenv_peer_mempackage: for deb-based os: for rpm-based os: after ensuring kernel support and installing the gpu driver,nvidia-peermemcan be loaded with the following command with root privileges in a terminal window:",5
4.references,#references,4.references,9
4.1.basics of uva cuda memory management,#basics-of-uva-cuda-memory-management,"4.1.basics of uva cuda memory management unified virtual addressing (uva) is a memory address management system enabled by default in cuda 4.0 and later releases on fermi and kepler gpus running 64-bit processes. the design of uva memory management provides a basis for the operation of gpudirect rdma. on uva-supported configurations, when the cuda runtime initializes, the virtual address (va) range of the application is partitioned into two areas: the cuda-managed va range and the os-managed va range. all cuda-managed pointers are within this va range, and the range will always fall within the first 40 bits of the processs va space. subsequently, within the cuda va space, addresses can be subdivided into three types: this partitioning allows the cuda runtime to determine the physical location of a memory object by its pointer value within the reserved cuda va space. addresses are subdivided into these categories at page granularity; all memory within a page is of the same type. note that gpu pages may not be the same size as cpu pages. the cpu pages are usually 4kb and the gpu pages on kepler-class gpus are 64kb. gpudirect rdma operates exclusively on gpu pages (created bycudamalloc()) that are within this cuda va space.",5
4.2.userspace api,#userspace-api,"4.2.userspace api data structures function cupointersetattribute() in gpudirect rdma scope, the interesting usage is whencu_pointer_attribute_sync_memopsis passed as theattribute: parameters returns it is used to explicitly enable a strictly synchronizing behavior on the whole memory allocation pointed to bypointer, and by doing so disabling all data transfer optimizations which might create problems with concurrent rdma and cuda memory copy operations. this api has cuda synchronizing behavior, so it should be considered expensive and possibly invoked only once per buffer. function cupointergetattribute() this function has two different attributes related to gpudirect rdma:cu_pointer_attribute_p2p_tokensandcu_pointer_attribute_buffer_id. whencu_pointer_attribute_p2p_tokensis passed as theattribute,datais a pointer tocuda_pointer_attribute_p2p_tokens: in this case, the function returns two tokens for use with thekernel api. parameters returns this function may be called at any time, including before cuda initialization, and it has cuda synchronizing behavior, as incu_pointer_attribute_sync_memops, so it should be considered expensive and should be invoked only once per buffer. note that values set intokenscan be different for the samepointervalue during a lifetime of a user-space program. seetokens usagefor a concrete example. note that for security reasons the value set inp2ptokenwill be randomized, to prevent it from being guessed by an adversary. in cuda 6.0, a new attribute has been introduced that is useful to detect memory reallocations. whencu_pointer_attribute_buffer_idis passed as theattribute,datais expected to point to a 64bit unsigned integer variable, likeuint64_t. parameters returns some general remarks follow: function ``cupointergetattributes()`` this function can be used to inspect multiple attributes at once. the one most probably related to gpudirect rdma arecu_pointer_attribute_buffer_id,cu_pointer_attribute_memory_typeandcu_pointer_attribute_is_managed.",5
4.3.kernel api,#kernel-api,"4.3.kernel api the following declarations can be found in thenv-p2p.hheader that is distributed in the nvidia driver package. please refer to the inline documentation contained in that header file for a detailed description of the parameters and the return values of the functions described below. preprocessor macros nvidia_p2p_page_table_version_compatible()andnvidia_p2p_dma_mapping_version_compatible()preprocessor macros are meant to be called by third-party device drivers to check for runtime binary compatibility. structure nvidia_p2p_page in thenvidia_p2p_pagestructure only thephysical_addressfield is relevant to gpudirect rdma. structure nvidia_p2p_page_table theversionfield of the page table should be checked by usingnvidia_p2p_page_table_version_compatible()before accessing the other fields. thepage_sizefield is encoded according to thenvidia_p2p_page_size_typeenum. structure nvidia_p2p_dma_mapping the version field of the dma mapping should be passed tonvidia_p2p_dma_mapping_version_compatible()before accessing the other fields. function nvidia_p2p_get_pages() this function makes the pages underlying a range of gpu virtual memory accessible to a third-party device. function nvidia_p2p_put_pages() this function releases a set of pages previously made accessible to a third-party device. warning: it is not meant to be called from within thenvidia_p2p_get_pages()callback. function nvidia_p2p_free_page_table() this function frees a third-party p2p page table and is meant to be invoked during the execution of thenvidia_p2p_get_pages()callback. function nvidia_p2p_dma_map_pages() this function makes the physical pages retrieved usingnvidia_p2p_get_pages()accessible to a third-party device. it is required on platforms where the i/o addresses of pcie resources, used for pcie peer-to-peer transactions, are different from the physical addresses used by the cpu to access those same resources. on some platforms, this function relies on a correct implementation of thedma_map_resource()linux kernel function. function nvidia_p2p_dma_unmap_pages() this function unmaps the physical pages previously mapped to the third-party device by nvidia_p2p_dma_map_pages(). it is not meant to be called from within thenvidia_p2p_get_pages()invalidation callback. function nvidia_p2p_free_dma_mapping() this function is meant to be called from within thenvidia_p2p_get_pages()invalidation callback. note that the deallocation of the i/o mappings may be deferred, for example after returning from the invalidation callback.",6
4.4.porting to tegra,#porting-to-tegra,"4.4.porting to tegra gpudirect rdma is supported on jetson agx xavier platform from cuda 10.1, on drive agx xavier linux based platforms from cuda 11.2 and on jetson orin platform from cuda 11.4. from this point onwards, this document will collectively refer jetson and drive as tegra. owing to hardware and software specific divergence of tegra vis-a-vis linux-desktop, already developed applications needs to be slightly modified in order to port them to tegra. the following sub-sections (4.4.1-4.4.3) briefs over the necessary changes.",0
4.4.1.changing the allocator,#changing-the-allocator,"4.4.1.changing the allocator gpudirect rdma on desktop allows applications to operate exclusively on gpu pages allocated usingcudamalloc(). on tegra, applications will have to change the memory allocator fromcudamalloc()tocudahostalloc(). applications can either:",5
4.4.2.modification to kernel api,#modification-to-kernel-api,4.4.2.modification to kernel api the declarations under tegra api column of the following table can be found in the nv-p2p.h header that is distributed in the nvidia driver package. refer to the inline documentation contained in that header file for a detailed description of the parameters and the return values. the table below represents the kernel api changes on tegra vis-a-vis desktop.,6
4.4.3.other highlights,#other-highlights,4.4.3.other highlights,9
5.notices,#notices,5.notices,9
5.1.notice,#notice,"5.1.notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation (nvidia) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material (defined below), code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer (terms of sale). nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion and/or use of nvidia products in such equipment or applications and therefore such inclusion and/or use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions and/or requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the nvidia product in any manner that is contrary to this document or (ii) customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding third-party products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents (together and separately, materials) are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product.",4
5.2.opencl,#opencl,5.2.opencl opencl is a trademark of apple inc. used under license to the khronos group inc.,4
5.3.trademarks,#trademarks,5.3.trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.,4
contents,#contents,contents tuning guide.,9
3.5.device state inspection,https://docs.nvidia.com/cuda/debugger-api/group__READ.html#group__READ,3.5.device state inspection,9
2.introduction,https://docs.nvidia.com/cuda/debugger-api/r_main.html,2.introduction,9
2.1.debugger api,https://docs.nvidia.com/cuda/debugger-api/r_main.html,2.1.debugger api,9
2.2.elf and dwarf,https://docs.nvidia.com/cuda/debugger-api/r_main.html,2.2.elf and dwarf,9
2.3.abi support,https://docs.nvidia.com/cuda/debugger-api/r_main.html,2.3.abi support,9
2.4.exception reporting,https://docs.nvidia.com/cuda/debugger-api/r_main.html,2.4.exception reporting,9
2.5.attaching and detaching,https://docs.nvidia.com/cuda/debugger-api/r_main.html,2.5.attaching and detaching,9
4.7.cudbgevent::cases_st::contextpush_st struct reference,https://docs.nvidia.com/cuda/debugger-api/structCUDBGEvent_1_1cases__st_1_1contextPush__st.html#structCUDBGEvent_1_1cases__st_1_1contextPush__st,4.7.cudbgevent::cases_st::contextpush_st struct reference,8
1.introduction,#introduction,"1.introduction nvrtc is a runtime compilation library for cuda c++. it accepts cuda c++ source code in
character string form and creates handles that can be used to obtain the ptx. the ptx
string generated by nvrtc can be loaded bycumoduleloaddataandcumoduleloaddataex, and
linked with other modules by using the nvjitlink library or usingculinkadddataof the
cuda driver api. this facility can often provide optimizations and performance not
possible in a purely offline static compilation. in the absence of nvrtc (or any runtime compilation support in cuda), users needed to
spawn a separate process to execute nvcc at runtime if they wished to implement runtime
compilation in their applications or libraries, and, unfortunately, this approach has
the following drawbacks: nvrtc addresses these issues by providing a library interface that eliminates overhead associated with spawning separate processes, disk i/o,and so on, while keeping application deployment simple.",0
2.getting started,#getting-started,2.getting started,9
2.1.system requirements,#system-requirements,2.1.system requirements nvrtc requires the following system configuration:,9
2.2.installation,#installation,2.2.installation nvrtc is part of the cuda toolkit release and the components are organized as follows in the cuda toolkit installation directory:,0
3.user interface,#user-interface,3.user interface this chapter presents the api of nvrtc. basic usage of the api is explained inbasic usage.,6
3.1.error handling,#error-handling,3.1.error handling nvrtc defines the following enumeration type and function for api call error handling. enumerations functions,6
3.1.1.enumerations,#enumerations,3.1.1.enumerations,9
3.1.2.functions,#functions,3.1.2.functions,9
3.2.general information query,#general-information-query,3.2.general information query nvrtc defines the following function for general information query. functions,6
3.2.1.functions,#id1,3.2.1.functions,9
3.3.compilation,#compilation,3.3.compilation nvrtc defines the following type and functions for actual compilation. functions typedefs,6
3.3.1.functions,#id2,3.3.1.functions,9
3.3.2.typedefs,#typedefs,3.3.2.typedefs,6
3.4.supported compile options,#supported-compile-options,"3.4.supported compile options nvrtc supports the compile options below. option names with two preceding dashs (--) are long option names and option names with one preceding dash (-) are short option names. short option names can be used instead of long option names. when a compile option takes an argument, an assignment operator (=) is used to separate the compile option argument from the compile option name, e.g.,""--gpu-architecture=compute_60"". alternatively, the compile option name and the argument can be specified in separate strings without an assignment operator, .e.g,""--gpu-architecture""""compute_60"". single-character short option names, such as-d,-u, and-i, do not require an assignment operator, and the compile option name and the argument can be present in the same string with or without spaces between them. for instance,""-d=<def>"",""-d<def>"", and""-d<def>""are all supported. the valid compiler options are:",6
3.5.host helper,#host-helper,3.5.host helper nvrtc defines the following functions for easier interaction with host code. functions,6
3.5.1.functions,#id3,3.5.1.functions,9
4.language,#language,"4.language unlike the offline nvcc compiler, nvrtc is meant for compiling only device cuda c++ code. it does not accept host code or host compiler extensions in the input code, unless otherwise noted.",6
4.1.execution space,#execution-space,"4.1.execution space nvrtc uses__host__as the default execution space, and it generates an error if it encounters any host code in the input. that is, if the input contains entities with explicit__host__annotations or no execution space annotation, nvrtc will emit an error.__host____device__functions are treated as device functions. nvrtc provides a compile option,--device-as-default-execution-space(refer tosupported compile options), that enables an alternative compilation mode, in which entities with no execution space annotations are treated as__device__entities.",6
4.2.separate compilation,#separate-compilation,"4.2.separate compilation nvrtc itself does not provide any linker. users can, however, use the nvjitlink library orculinkadddatain the cuda driver api to link the generated relocatable ptx code with other relocatable code. to generate relocatable ptx code, the compile option--relocatable-device-code=trueor--device-cis required.",0
4.3.dynamic parallelism,#dynamic-parallelism,4.3.dynamic parallelism nvrtc supports dynamic parallelism under the following conditions: example:dynamic parallelismprovides a simple example.,5
4.4.integer size,#integer-size,"4.4.integer size different operating systems define integer type sizes differently.
linux x86_64 implements lp64, and windows x86_64 implements llp64. nvrtc implements lp64 on linux and llp64 on windows. nvrtc supports 128-bit integer types through the__int128type. this can be enabled with the--device-int128flag. 128-bit integer support is not available on windows.",6
4.5.include syntax,#include-syntax,"4.5.include syntax whennvrtccompileprogram()is called, the current working directory is added to the header search path used for locating files included with the quoted syntax (for example,include""foo.h""), before the code is compiled.",6
4.6.predefined macros,#predefined-macros,4.6.predefined macros,6
4.7.predefined types,#predefined-types,4.7.predefined types,6
4.8.builtin functions,#builtin-functions,"4.8.builtin functions builtin functions provided by the cuda runtime headers when compiling offline withnvccare available, unless otherwise noted.",0
4.9.default c++ dialect,#default-c-dialect,4.9.default c++ dialect the default c++ dialect is c++17. other dialects can be selected using the-stdflag.,6
5.basic usage,#basic-usage,"5.basic usage this section of the document uses a simple example,single-precision x plus y(saxpy), shown infigure 1to explain what is involved in runtime compilation with nvrtc. for brevity and readability, error checks on the api return values are not shown. the complete code listing is available inexample: saxpy. figure 1. cuda source string for saxpy first, an instance ofnvrtcprogramneeds to be created.  figure 2 shows creation ofnvrtcprogramfor saxpy. as saxpy does not require any header, 0 is passed asnumheaders, and null asheadersandincludenames. figure 2. nvrtcprogram creation for saxpy if saxpy had any include directives, the contents of the files that are
included can be passed as elements of headers, and their names as elements
of includenames. for example,include<foo.h>andinclude<bar.h>would
require 2 asnumheaders,{""<contentsoffoo.h>"",""<contentsofbar.h>""}as headers, and{""foo.h"",""bar.h""}asincludenames(<contentsoffoo.h>and<contentsofbar.h>must be replaced by the actual contents offoo.handbar.h). alternatively, the compile option-ican be used if the header
is guaranteed to exist in the file system at runtime. once the instance ofnvrtcprogramfor compilation is created, it can be
compiled bynvrtccompileprogramas shown in figure 3. two compile options
are used in this example,--gpu-architecture=compute_80and--fmad=false,
to generate code for the compute_80 architecture and to disable the
contraction of floating-point multiplies and adds/subtracts into
floating-point multiply-add operations. other combinations of compile
options can be used as needed and supported compile options lists valid
compile options. figure 3. compilation of saxpy for compute_80 with fmad enabled after the compilation completes, users can obtain the program compilation
log and the generated ptx as figure 4 shows. nvrtc does not generate valid
ptx when the compilation fails, and it may generate program compilation log
even when the compilation succeeds if needed. annvrtcprogramcan be compiled bynvrtccompileprogrammultiple times with
different compile options, and users can only retrieve the ptx and the log
generated by the last compilation. when the instance ofnvrtcprogramis no longer needed, it can be destroyed bynvrtcdestroyprogramas shown in figure 5. figure 5. destruction of nvrtcprogram the generated ptx can be further manipulated by the cuda driver api for execution or linking. figure 6 shows an example code sequence for execution of the generated ptx. figure 6. execution of saxpy using the ptx generated by nvrtc",6
6.accessing lowered names,#accessing-lowered-names,"6.accessing lowered names nvrtc will mangle__global__function names and names of__device__and__constant__variables as specified by the ia64 abi. if the generated
ptx is being loaded using the cuda driver api, the kernel function or__device__/__constant__variable must be looked up by name, but this
is hard to do when the name has been mangled. to address this problem,
nvrtc provides api functions that map source level__global__function
or__device__/__constant__variable names to the mangled names present
in the generated ptx. the two api functionsnvrtcaddnameexpressionandnvrtcgetlowerednamework together to provide this functionality. first, a name expression
string denoting the address for the__global__function or__device__/__constant__variable is provided tonvrtcaddnameexpression.
then, the program is compiled withnvrtccompileprogram. during compilation,
nvrtc will parse the name expression string as a c++ constant expression at
the end of the user program. the constant expression must provide the address
of the__global__function or__device__/__constant__variable. finally,
the functionnvrtcgetlowerednameis called with the original name expression
and it returns a pointer to the lowered name. the lowered name can be used
to refer to the kernel or variable in the cuda driver api. nvrtc guarantees that any__global__function or__device__/__constant__variable referenced in a call tonvrtcaddnameexpressionwill be present in
the generated ptx (if the definition is available in the input source code).",6
6.1.example,#example,6.1.example example: using lowered name`_lists a complete runnable example. some relevant snippets:,6
6.2.notes,#notes,6.2.notes,9
7.interfacing with template host code,#interfacing-with-template-host-code,"7.interfacing with template host code in some scenarios, it is useful to instantiate__global__function templates in device
code based on template arguments in host code. the nvrtc helper function nvrtcgettypename
can be used to extract the source level name of a type in host code, and this string can be
used to instantiate a__global__function template and get the mangled name of the
instantiation using thenvrtcaddnameexpressionandnvrtcgetlowerednamefunctions. nvrtcgettypenameis defined inline in the nvrtc header file, and is available when the
macronvrtc_get_type_nameis defined with a non-zero value. it uses theabi::__cxa_demangleandundecoratesymbolnamehost code functions when using gcc/clang and cl.exe compilers,
respectively. users may need to specify additional header paths and libraries to find the
host functions used (abi::__cxa_demangle/undecoratesymbolname). refer to the build instructions
for the example below for reference (nvrtcgettypename build instructions).",6
7.1.template host code example,#template-host-code-example,7.1.template host code example example: using nvrtcgettypenamelists a complete runnable example. some relevant snippets:,6
8.versioning scheme,#versioning-scheme,8.versioning scheme,9
8.1.nvrtc shared library versioning,#nvrtc-shared-library-versioning,"8.1.nvrtc shared library versioning in the following, major and minor denote the major and minor versions of the cuda toolkit. for example, for cuda 11.2, major is 11 and minor is 2. consider a cuda toolkit with major version > 11. the nvrtc shared library in this cuda  toolkit will have the same soname (linux) or dll name (windows) as an nvrtc shared library  in a previous minor version of the same cuda toolkit. similarly, the nvrtc shared library in cuda 11.3 and later 11.x releases will have the same soname (linux) or dll name (windows) as the nvrtc shared library in cuda 11.2. as a consequence of the versioning scheme described above, an nvrtc client that links
against a particular nvrtc shared library will continue to work with a future nvrtc shared
library with a matching soname (linux) or dll name (windows). this allows the nvrtc client
to take advantage of bug fixes and enhancements available in the more recent nvrtc shared
library1. however, the more recent nvrtc shared library may generate ptx with a version that
is not accepted by the cuda driver api functions of an older cuda driver, as explained in thebest practices guide. some approaches to resolving this issue: alternately, an nvrtc client can either link against the static nvrtc library or redistribute
a specific version of the nvrtc shared library and use dlopen (linux) or loadlibrary (windows)
functions to use that library at run time. either approach allows the nvrtc client to maintain
control over the version of nvrtc being used during deployment, to ensure predictable functionality and performance.",0
8.2.nvrtc-builtins library,#nvrtc-builtins-library,"8.2.nvrtc-builtins library the nvrtc-builtins library contains helper code that is part of the nvrtc package.
it is only used by the nvrtc library internally. each nvrtc library is only compatible
with the nvrtc-builtins library from the same cuda toolkit.",0
9.miscellaneous notes,#miscellaneous-notes,9.miscellaneous notes,9
9.1.thread safety,#thread-safety,"9.1.thread safety multiple threads can invoke nvrtc api functions concurrently, as long as there is no race
condition. in this context, a race condition is defined to occur if multiple threads
concurrently invoke nvrtc api functions with the same nvrtcprogram argument, where at
least one thread is invoking eithernvrtccompileprogramornvrtcaddnameexpression2. since cuda 12.3, nvrtc allows concurrent invocations ofnvrtccompileprogramto potentially concurrently also invoke the embedded nvvm optimizer/codegen phase.
setting the environment variablenvrtc_disable_concurrent_nvvmdisables this behavior,
i.e., invocations of the embedded nvvm optimizer/codegen phase will be serialized.",5
9.2.stack size,#stack-size,"9.2.stack size on linux, nvrtc will increase the stack size to the maximum allowed using thesetrlimit()function during compilation. this reduces the chance that the compiler will run out of stack when processing complex input sources. the stack size is reset to the previous value when compilation is completed. becausesetrlimit()changes the stack size for the entire process, it will also affect
other application threads that may be executing concurrently. the command line flag-modify-stack-limit=falsewill prevent nvrtc from modifying the stack limit.",6
9.3.nvrtc static library,#nvrtc-static-library,9.3.nvrtc static library the nvrtc static library references functions defined in the nvrtc-builtins static library and the ptx compiler static library. please see build instructions for an example.,6
10.example: saxpy,#example-saxpy,10.example: saxpy,9
10.1.code (saxpy.cpp),#code-saxpy-cpp,10.1.code (saxpy.cpp),6
10.2.host type name build instructions,#host-type-name-build-instructions,"10.2.host type name build instructions assuming the environment variablecuda_pathpoints to the cuda toolkit installation directory, build this example as:",0
11.example: using lowered name,#example-using-lowered-name,11.example: using lowered name,9
11.1.code (lowered-name.cpp),#code-lowered-name-cpp,11.1.code (lowered-name.cpp),9
11.2.lowered name build instructions,#lowered-name-build-instructions,"11.2.lowered name build instructions assuming the environment variablecuda_pathpoints to cuda toolkit installation directory, build this example as:",0
12.example: using nvrtcgettypename,#example-using-nvrtcgettypename,12.example: using nvrtcgettypename,6
12.1.code (host-type-name.cpp),#code-host-type-name-cpp,12.1.code (host-type-name.cpp),6
12.2.nvrtcgettypename build instructions,#nvrtcgettypename-build-instructions,"12.2.nvrtcgettypename build instructions assuming the environment variablecuda_pathpoints to cuda toolkit installation
directory, build this example as:",0
13.example: dynamic parallelism,#example-dynamic-parallelism,13.example: dynamic parallelism code (dynamic-parallelism.cpp),6
13.1.dynamic parallelism build instructions,#dynamic-parallelism-build-instructions,"13.1.dynamic parallelism build instructions assuming the environment variablecuda_pathpoints to cuda toolkit installation directory, build this example as:",0
14.example: device lto (link time optimization),#example-device-lto-link-time-optimization,"14.example: device lto (link time optimization) this section demonstrates device link time optimization (lto).
there are two units of lto ir. the first unit is generated offline using nvcc, by specifying the architecture as-archlto_xx(refer tocode (offline.cu)).
the generated lto ir is packaged in a fatbinary. the second unit is generated online using nvrtc, by specifying the flag-dlto(refer tocode (online.cpp)). these two units are then passed tolibnvjitlink*api functions, which link together the lto ir, run the optimizer on the linked ir and generate a cubin (refer tocode (online.cpp)). the cubin is then loaded on the gpu and executed.",5
14.1.code (offline.cu),#code-offline-cu,14.1.code (offline.cu),9
14.2.code (online.cpp),#code-online-cpp,14.2.code (online.cpp),9
14.3.device lto build instructions,#device-lto-build-instructions,"14.3.device lto build instructions assuming the environment variablecuda_pathpoints to the cuda toolkit installation directory, build this example as:",0
14.4.notices,#notices,14.4.notices,9
14.4.1.notice,#notice,"14.4.1.notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation (nvidia) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material (defined below), code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer (terms of sale). nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion and/or use of nvidia products in such equipment or applications and therefore such inclusion and/or use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions and/or requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the nvidia product in any manner that is contrary to this document or (ii) customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding third-party products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents (together and separately, materials) are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product.",4
14.4.2.opencl,#opencl,14.4.2.opencl opencl is a trademark of apple inc. used under license to the khronos group inc.,4
14.4.3.trademarks,#trademarks,14.4.3.trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.,4
3.6.device state alteration,https://docs.nvidia.com/cuda/debugger-api/group__WRITE.html#group__WRITE,3.6.device state alteration,9
3.4.breakpoints,https://docs.nvidia.com/cuda/debugger-api/group__BP.html#group__BP,3.4.breakpoints,9
1.introduction,#introduction,1.introduction,9
1.1.nvjpeg decoder,#nvjpeg-decoder,"1.1.nvjpeg decoder the nvjpeg library provides high-performance, gpu accelerated jpeg decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications. the library offers single and batched jpeg decoding capabilities which efficiently utilize the available gpu resources for optimum performance; and the flexibility for users to manage the memory allocation needed for decoding. the nvjpeg library enables the following functions: use the jpeg image data stream as input; retrieve the width and height of the image from the data stream, and use this retrieved information to manage the gpu memory allocation and the decoding. a dedicated api is provided for retrieving the image information from the raw jpeg image data stream. the nvjpeg library supports the following: jpeg options: features:",2
1.2.nvjpeg encoder,#nvjpeg-encoder,"1.2.nvjpeg encoder the encoding functions of the nvjpeg library perform gpu-accelerated compression of users image data to the jpeg bitstream. user can provide input data in a number of formats and colorspaces, and control the encoding process with parameters. encoding functionality will allocate temporary buffers using user-provided memory allocator. before calling the encoding functions the user should perform a few prerequisite steps using the helper functions described innvjpeg encoder helper api reference.",2
1.3.thread safety,#thread-safety,"1.3.thread safety not all nvjpeg types are thread safe. when using decoder apis across multiple threads, the following decoder types should be instantiated separately for each thread:nvjpegjpegstream_t,nvjpegjpegstate_t,nvjpegbufferdevice_t,nvjpegbufferpinned_t when using encoder apis across multiple threads,nvjpegencoderstate_tshould be instantiated separately for each thread. for user-provided allocators (inputs tonvjpegcreateex()), the user needs to ensure thread safety.",2
1.4.multi-gpu support,#multi-gpu-support,1.4.multi-gpu support the nvjpeg states and handles are bound to the device that was set as current during their creation. using these states and handles with another device set as current is undefined. the user is responsible of keeping track of the current device.,5
1.5.hardware acceleration,#hardware-acceleration,"1.5.hardware acceleration hardware accelerated jpeg decode is available on the following gpus - a100, a30, h100. platforms which support hardware accelerated jpeg decode:",2
2.jpeg decoding,#jpeg-decoding,2.jpeg decoding,2
2.1.using jpeg decoding,#using-jpeg-decoding,"2.1.using jpeg decoding the nvjpeg library provides functions for both the decoding of a single image, and batched decoding of multiple images.",2
2.1.1.single image decoding,#single-image-decoding,"2.1.1.single image decoding for single-image decoding you provide the data size and a pointer to the file data, and the decoded image is placed in the output buffer. to use the nvjpeg library, start by calling the helper functions for initialization.",2
2.1.2.decode using decoupled phases,#decode-using-decoupled-phases,2.1.2.decode using decoupled phases the nvjpeg library allows further separation of the host and device phases of the decode process. the host phase of the decoding will not need to access to device resources. a few examples of decoupled apis can be found underdecode api - decoupled decoding. below is the sequence of api calls to decode a single image,2
2.1.3.batched image decoding,#batched-image-decoding,"2.1.3.batched image decoding for the batched image decoding you provide pointers to multiple file data in the memory, and also provide the buffer sizes for each file data. the nvjpeg library will decode these multiple images, and will place the decoded data in the output buffers that you specified in the parameters.",2
2.2.nvjpeg type declarations,#nvjpeg-type-declarations,2.2.nvjpeg type declarations,6
2.2.1.nvjpeg backend,#nvjpeg-backend,"2.2.1.nvjpeg backend thenvjpegbackend_tenum is used to select either default back-end by default, or use gpu decoding for baseline jpeg images, or use cpu for huffman decoding.",2
2.2.2.nvjpeg bitstream handle,#nvjpeg-bitstream-handle,2.2.2.nvjpeg bitstream handle this handle stores the bit-stream parameters on the host. this helps retrieve bitstream meta-data using apis defined innvjpeg stream api.,2
2.2.3.nvjpeg decode device buffer handle,#nvjpeg-decode-device-buffer-handle,2.2.3.nvjpeg decode device buffer handle thisnvjpegbufferdevice_tis used by decoder states to store the intermediate information in device memory.,2
2.2.4.nvjpeg decode parameter handle,#nvjpeg-decode-parameter-handle,"2.2.4.nvjpeg decode parameter handle this decoder parameter handle stores the parameters like output format, and the roi decode parameters that are set using apis defined innvjpeg chroma subsampling.",2
2.2.5.nvjpeg decode pinned buffer handle,#nvjpeg-decode-pinned-buffer-handle,2.2.5.nvjpeg decode pinned buffer handle thisnvjpegbufferpinned_thandle is used by decoder states to store the intermediate information on pinned memory.,2
2.2.6.nvjpeg decoder handle,#nvjpeg-decoder-handle,"2.2.6.nvjpeg decoder handle this decoder handle stores the intermediate decoder data, which is shared across the decoding stages. this decoder handle is initialized for a givennvjpegbackend_t. it is used as input to thedecode apidecoupled decoding.",2
2.2.7.nvjpeg host pinned memory allocator interface,#nvjpeg-host-pinned-memory-allocator-interface,"2.2.7.nvjpeg host pinned memory allocator interface when thenvjpegpinnedallocator_t*allocatorparameter in thenvjpegcreateex()function is set as a pointer to the abovenvjpegpinnedallocator_tstructure, then this structure will be used for allocating and releasing host pinned memory for copying data to/from device. the function prototypes for the memory allocation and memory freeing functions are similar to thecudahostalloc()andcudafreehost()functions. they will return 0 in case of success, and non-zero otherwise. however, if thenvjpegpinnedallocator_t*allocatorparameter in thenvjpegcreateex()function is set to null, then the default memory allocation functionscudahostalloc()andcudafreehost()will be used. when usingnvjpegcreate()ornvjpegcreatesimple()function to create library handle, the default host pinned memory allocator will be used.",5
2.2.8.nvjpeg extended host pinned memory allocator interface,#nvjpeg-extended-host-pinned-memory-allocator-interface,"2.2.8.nvjpeg extended host pinned memory allocator interface extended pinned allocators support stream ordered allocations along with user defined context informationpinned_ctx. when invoking the allocators, nvjpeg will passpinned_ctxas input to the extended pinned allocators.",2
2.2.9.nvjpeg image,#nvjpeg-image,"2.2.9.nvjpeg image thenvjpegimage_tstructure (or structures, in the case of batched decode) is used to fill with the pointers and pitches of allocated buffers. thenvjpegimage_tstructure that holds the output pointers.",2
2.2.10.nvjpeg device memory allocator interface,#nvjpeg-device-memory-allocator-interface,"2.2.10.nvjpeg device memory allocator interface users can tell the library to use their own device memory allocator. the function prototypes for the memory allocation and memory freeing functions are similar to thecudamalloc()andcudafree()functions. they should return 0 in case of success, and non-zero otherwise. a pointer to thenvjpegdevallocator_tstructure, with properly filled fields, should be provided to thenvjpegcreate()function. null is accepted, in which case the default memory allocation functionscudamalloc()andcudafree()is used. when thenvjpegdevallocator_t*allocatorparameter in thenvjpegcreate()ornvjpegcreateex()function is set as a pointer to the abovenvjpegdevallocator_tstructure, then this structure is used for allocating and releasing the device memory. the function prototypes for the memory allocation and memory freeing functions are similar to thecudamalloc()andcudafree()functions. they should return 0 in case of success, and non-zero otherwise. however, if thenvjpegdevallocator_t*allocatorparameter in thenvjpegcreate()ornvjpegcreateex()function is set to null, then the default memory allocation functionscudamalloc()andcudafree()will be used. when usingnvjpegcreatesimple()function to create library handle the default device memory allocator will be used.",6
2.2.11.nvjpeg extended device memory allocator interface,#nvjpeg-extended-device-memory-allocator-interface,"2.2.11.nvjpeg extended device memory allocator interface extended device allocators support stream ordered allocations along with user defined context informationdev_ctx. when invoking the allocators, nvjpeg will passdev_ctxas input to the extended device allocators.",2
2.2.12.nvjpeg opaque jpeg decoding state handle,#nvjpeg-opaque-jpeg-decoding-state-handle,2.2.12.nvjpeg opaque jpeg decoding state handle thenvjpegjpegstatestructure stores the temporary jpeg information. it should be initialized before any usage. this jpeg state handle can be reused after being used in another decoding. the same jpeg handle should be used across the decoding phases for the same image or batch. multiple threads are allowed to share the jpeg state handle only when processing same batch during first phase (nvjpegdecodephaseone) .,2
2.2.13.nvjpeg opaque library handle struct,#nvjpeg-opaque-library-handle-struct,"2.2.13.nvjpeg opaque library handle struct the library handle is used in any consecutive nvjpeg library calls, and should be initialized first. the library handle is thread safe, and can be used by multiple threads simultaneously.",2
2.2.14.nvjpeg output pointer struct,#nvjpeg-output-pointer-struct,"2.2.14.nvjpeg output pointer struct thenvjpegimage_tstruct holds the pointers to the output buffers, and holds the corresponding strides of those buffers for the image decoding. seesingle image decodingon how to set up thenvjpegimage_tstruct.",2
2.2.15.nvjpeg jpeg encoding,#nvjpeg-jpeg-encoding,2.2.15.nvjpeg jpeg encoding thenvjpegjpegencoding_tenum lists the jpeg encoding types that are supported by the nvjpeg library the enum values are based on the markers defined in the jpeg specification,2
2.2.16.nvjpeg scale factor,#nvjpeg-scale-factor,2.2.16.nvjpeg scale factor thenvjpegscalefactor_tenum lists all the scale factors supported by the library. this feature is supported when nvjpeg handles are intstaniated using nvjpeg_backend_hardware,2
2.2.17.nvjpeg flags,#nvjpeg-flags,2.2.17.nvjpeg flags nvjpeg flags provide additional controls when initializing the library usingnvjpegcreateex()ornvjpegcreateexv2(). it is possible to combine the flags as they are bit fields.,2
2.2.18.nvjpeg exif orientation,#nvjpeg-exif-orientation,2.2.18.nvjpeg exif orientation thenvjpegexiforientation_tenum represents the exif orientation in a jfif(jpeg) file. exif orientation information is typically used to denote the digital camera sensor orientation at the time of image capture.,2
2.3.nvjpeg api reference,#nvjpeg-api-reference,2.3.nvjpeg api reference this section describes the nvjpeg decoder api.,2
2.3.1.nvjpeg helper api reference,#nvjpeg-helper-api-reference,2.3.1.nvjpeg helper api reference,2
2.3.2.retrieve encoded image information api,#retrieve-encoded-image-information-api,2.3.2.retrieve encoded image information api the helper functions for retrieving the encoded image information.,2
2.3.3.decode apisingle phase,#decode-apisingle-phase,2.3.3.decode apisingle phase functions for decoding single image or batched images in a single phase.,2
2.3.4.decode apidecoupled decoding,#decode-apidecoupled-decoding,"2.3.4.decode apidecoupled decoding this set of decoding api works with the bitstream handles, decode parameter handles, pinned and device buffers handles as input, thus decoupling jpeg bitstream parse, buffer management and setting up decoder parameters from the decode process itself. currently only multiphase decoding is available. multiphase decoupled single image decoding consists of three phases: each of the above decodings is carried on according to its individual semantics. phases on different images can be carried out with different decoding state handles simultaneously, while sharing of some helper objects is possible. see the details of semantics in the individual phases descriptions. below are a couple of examples of using decoupled api. the following snippet explains how to use the api to prefetch the host stage of the processing: first do all of the host work on the host, and then submit the rest of decoding work to the device. the following snippet explains how pinned and device buffers can be shared across two instances ofnvjpeg decoder handle.",2
2.3.5.nvjpeg decode parameters,#nvjpeg-decode-parameters,2.3.5.nvjpeg decode parameters this category of apis is used to set the decoding parameters. these apis should be used with the decode apis defined indecode apidecoupled decoding.,2
2.3.6.nvjpeg api return codes,#nvjpeg-api-return-codes,2.3.6.nvjpeg api return codes the nvjpeg api adheres to the following return codes and their indicators: description of the returned error codes:,2
2.3.7.nvjpeg chroma subsampling,#nvjpeg-chroma-subsampling,"2.3.7.nvjpeg chroma subsampling one of the outputs of thenvjpeggetimageinfo()api isnvjpegchromasubsampling_t. this parameter is anenumtype, and its enumerator list comprises of the chroma subsampling property retrieved from the encoded jpeg image. thenvjpeggetimageinfo()function currently supports the following chroma subsampling types:",2
2.3.8.reference documents,#reference-documents,2.3.8.reference documents refer to the jpeg standard:https://jpeg.org/jpeg/,2
2.4.examples of nvjpeg,#examples-of-nvjpeg,2.4.examples of nvjpeg nvjpeg decode sample can be found here:https://github.com/nvidia/cudalibrarysamples/tree/master/nvjpeg/nvjpeg-decoder,2
3.jpeg encoding,#jpeg-encoding,3.jpeg encoding this section describes the encoding functions of the nvjpeg library.,2
3.1.using the encoder,#using-the-encoder,3.1.using the encoder the user should perform the below prerequisite steps before calling the nvjpeg encoding functions. see alsonvjpeg encoder helper api reference.,2
3.1.1.encoding the parameters,#encoding-the-parameters,"3.1.1.encoding the parameters the user should create an encoding parameters structure withnvjpegencoderparamscreate()function. the function will be initialized with default parameters. user can use an appropriatenvjpegencoderparamsset*()function to set a specific parameter. the quality parameter can be set, using thenvjpegencoderparamssetquality()function, to an integer value between 1 and 100, and this quality parameter will be used as a base for generating the jpeg quantization tables. the parameters structure should be passed to compression functions.",2
3.1.2.encoding the state,#encoding-the-state,3.1.2.encoding the state the user should create the encoding state structure usingnvjpegencoderstatecreate()function. this function will hold intermediate buffers for the encoding process. this state should be passed to the compression functions.,2
3.1.3.encoding the image,#encoding-the-image,3.1.3.encoding the image the nvjpeg library provides a few interfaces for compressing the image in different formats and colorspaces. see below.,2
3.1.4.retrieving the compressed stream,#retrieving-the-compressed-stream,"3.1.4.retrieving the compressed stream often it is not feasible to accurately predict the final compressed data size of the final jpeg stream for any input data and parameters. the nvjpeg library, while encoding, will calculate the size of the final stream, allocate temporary buffer in the encoder state and save the compressed data in the encoding states buffer. in order to get final compressed jpeg stream, the user should provide the memory buffer large enough to store this compressed data. there are two options for how to do this:",2
3.1.5.jpeg encoding example,#jpeg-encoding-example,"3.1.5.jpeg encoding example see below the example code, and the block diagram shown infigure 1, for encoding with nvjpeg encoder.",2
3.2.nvjpeg encoder type declarations,#nvjpeg-encoder-type-declarations,3.2.nvjpeg encoder type declarations this section describes the nvjpeg encoder type declarations.,2
3.2.1.nvjpeginputformat_t,#nvjpeginputformat-t,3.2.1.nvjpeginputformat_t thenvjpeginputformat_tenum is used to select the color model and pixel format of the input image. it is used for conversion to ycbcr during encoding.,2
3.2.2.nvjpegencoderstate_t,#nvjpegencoderstate-t,3.2.2.nvjpegencoderstate_t thenvjpegencoderstate_tstructure stores intermediate buffers and variables used for compression.,2
3.2.3.nvjpegencoderparams_t,#nvjpegencoderparams-t,3.2.3.nvjpegencoderparams_t thenvjpegencoderparams_tstructure stores jpeg encode parameters.,2
3.3.nvjpeg encoder helper api reference,#nvjpeg-encoder-helper-api-reference,3.3.nvjpeg encoder helper api reference the nvjpeg encoder helper functions are used for initializing.,2
3.3.1.nvjpegencoderstatecreate(),#nvjpegencoderstatecreate,3.3.1.nvjpegencoderstatecreate() creates encoder state that stores intermediate buffers used in compression. signature: parameters:,2
3.3.2.nvjpegencoderstatedestroy(),#nvjpegencoderstatedestroy,3.3.2.nvjpegencoderstatedestroy() destroys the encoder state. signature: parameters:,2
3.3.3.nvjpegencoderparamscreate(),#nvjpegencoderparamscreate,3.3.3.nvjpegencoderparamscreate() creates the structure that holds the compression parameters. signature: parameters:,2
3.3.4.nvjpegencoderparamsdestroy(),#nvjpegencoderparamsdestroy,3.3.4.nvjpegencoderparamsdestroy() destroys the encoder parameters structure. signature: parameters:,2
3.3.5.nvjpegencoderparamssetencoding(),#nvjpegencoderparamssetencoding,3.3.5.nvjpegencoderparamssetencoding() sets the parameter quality in the encoder parameters structure. signature: parameters:,2
3.3.6.nvjpegencoderparamssetquality(),#nvjpegencoderparamssetquality,3.3.6.nvjpegencoderparamssetquality() sets the parameter quality in the encoder parameters structure. signature: parameters:,2
3.3.7.nvjpegencoderparamssetoptimizedhuffman(),#nvjpegencoderparamssetoptimizedhuffman,"3.3.7.nvjpegencoderparamssetoptimizedhuffman() sets whether or not to use optimized huffman. using optimized huffman produces smaller jpeg bitstream sizes with the same quality, but with slower performance. signature: parameters:",2
3.3.8.nvjpegencoderparamssetsamplingfactors(),#nvjpegencoderparamssetsamplingfactors,3.3.8.nvjpegencoderparamssetsamplingfactors() sets which chroma subsampling will be used for jpeg compression. signature: parameters:,2
3.4.nvjpeg encoder api reference,#nvjpeg-encoder-api-reference,3.4.nvjpeg encoder api reference this section describes the nvjpeg encoder api.,2
3.4.1.nvjpegencodegetbuffersize(),#nvjpegencodegetbuffersize,"3.4.1.nvjpegencodegetbuffersize() returns the maximum possible buffer size that is needed to store the compressed jpeg stream, for the given input parameters. signature: parameters:",2
3.4.2.nvjpegencodeyuv(),#id1,"3.4.2.nvjpegencodeyuv() compresses the image in yuv colorspace to jpeg stream using the provided parameters, and stores it in the state structure. signature: parameters:",2
3.4.3.nvjpegencodeimage(),#id2,"3.4.3.nvjpegencodeimage() compresses the image in the provided format to the jpeg stream using the provided parameters, and stores it in the state structure. signature: parameters:",2
3.4.4.nvjpegencoderetrievebitstream(),#nvjpegencoderetrievebitstream,3.4.4.nvjpegencoderetrievebitstream() retrieves the compressed stream from the encoder state that was previously used in one of the encoder functions. signature: parameters:,2
3.4.5.nvjpegencoderetrievebitstreamdevice(),#nvjpegencoderetrievebitstreamdevice,3.4.5.nvjpegencoderetrievebitstreamdevice() retrieves the compressed stream from the encoder state that was previously used in one of the encoder functions. signature: parameters:,2
4.jpeg transcoding,#jpeg-transcoding,4.jpeg transcoding this section describes the transcoding functions of the nvjpeg library.,2
4.1.nvjpeg transcoder helper api reference,#nvjpeg-transcoder-helper-api-reference,4.1.nvjpeg transcoder helper api reference this section describes the nvjpeg transcoder helper api.,2
4.1.1.nvjpegencoderparamscopymetadata(),#nvjpegencoderparamscopymetadata,"4.1.1.nvjpegencoderparamscopymetadata() copies the metadata (jfif, app, ext, and com markers) from the parsed stream. signature: parameters:",2
4.1.2.nvjpegencoderparamscopyquantizationtables(),#nvjpegencoderparamscopyquantizationtables,4.1.2.nvjpegencoderparamscopyquantizationtables() copies the quantization tables from the parsed stream. signature: parameters:,2
4.1.3.nvjpegencoderparamscopyhuffmantables() [deprecated],#nvjpegencoderparamscopyhuffmantables-deprecated,"4.1.3.nvjpegencoderparamscopyhuffmantables() [deprecated] nvjpegencoderparamscopyhuffmantables() is now deprecated. due to precision differences in the jpeg encode/decode process, the input huffman tables may no longer be valid for the image being encoded and may result in corrupt bitstream. signature: parameters:",2
4.2.jpeg transcoding example,#jpeg-transcoding-example,4.2.jpeg transcoding example see below the example code.,2
5.list of dropped apis,#list-of-dropped-apis,5.list of dropped apis the following apis are dropped starting cuda 11.0,0
6.known issues,#known-issues,"6.known issues decoupled apis, when initialized withnvjpeg_backend_gpu_hybrid, may not be able to correctly decode jpeg bitstreams which have out of bound run length codes.",2
7.notices,#notices,7.notices,9
7.1.notice,#notice,"7.1.notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation (nvidia) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material (defined below), code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer (terms of sale). nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion and/or use of nvidia products in such equipment or applications and therefore such inclusion and/or use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions and/or requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the nvidia product in any manner that is contrary to this document or (ii) customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding third-party products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents (together and separately, materials) are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product.",4
7.2.opencl,#opencl,7.2.opencl opencl is a trademark of apple inc. used under license to the khronos group inc.,4
7.3.trademarks,#trademarks,7.3.trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.,4
contents,#contents,contents,9
1.introduction,#introduction,1.introduction cudais a parallel computing platform and programming model invented by nvidia. it enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (gpu). cuda was developed with several design goals in mind: cuda-capable gpus have hundreds of cores that can collectively run thousands of computing threads. these cores have shared resources including a register file and a shared memory. the on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus. this guide will show you how to install and check the correct operation of the cuda development tools.,0
1.1.system requirements,#system-requirements,"1.1.system requirements to use cuda on your system, you will need the following installed: supported microsoft windowsoperating systems: * support for visual studio 2015 is deprecated in release 11.1; support for visual studio 2017 is deprecated in release 12.5. 32-bit compilation native and cross-compilation is removed from cuda 12.0 and later toolkit. use the cuda toolkit from earlier releases for 32-bit compilation. cuda driver will continue to support running 32-bit application binaries  on  geforce gpus until ada. ada will be the last architecture with driver support for 32-bit applications. hopper does not support 32-bit applications. support for running x86 32-bit applications on x86_64 windows is limited to use with:",0
1.2.about this document,#about-this-document,1.2.about this document this document is intended for readers familiar with microsoft windows operating systems and the microsoft visual studio environment. you do not need previous experience with cuda or experience with parallel computation.,0
2.installing cuda development tools,#installing-cuda-development-tools,2.installing cuda development tools basic instructions can be found in thequick start guide. read on for more detailed instructions. the setup of cuda development tools on a system running the appropriate version of windows consists of a few simple steps:,0
2.1.verify you have a cuda-capable gpu,#verify-you-have-a-cuda-capable-gpu,"2.1.verify you have a cuda-capable gpu you can verify that you have a cuda-capable gpu through thedisplay adapterssection in thewindows device manager. here you will find the vendor name and model of your graphics card(s). if you have an nvidia card that is listed inhttps://developer.nvidia.com/cuda-gpus, that gpu is cuda-capable. the release notes for the cuda toolkit also contain a list of supported products. thewindows device managercan be opened via the following steps:",0
2.2.download the nvidia cuda toolkit,#download-the-nvidia-cuda-toolkit,"2.2.download the nvidia cuda toolkit the nvidia cuda toolkit is available athttps://developer.nvidia.com/cuda-downloads. choose the platform you are using and one of the following installer formats: the cuda toolkit installs the cuda driver and tools needed to create, build and run a cuda application as well as libraries, header files, and other resources. download verification the download can be verified by comparing the md5 checksum posted athttps://developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txtwith that of the downloaded file. if either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again.",0
2.3.install the cuda software,#install-the-cuda-software,"2.3.install the cuda software before installing the toolkit, you should read the release notes, as they provide details on installation and software functionality. graphical installation install the cuda software by executing the cuda installer and following the on-screen prompts. silent installation the installer can be executed in silent mode by executing the package with the-sflag. additional parameters can be passed which will install specific subpackages instead of all packages. see the table below for a list of all the subpackage names. for example, to install only the compiler and driver components: use the-noption if you do not want to reboot automatically after install or uninstall, even if reboot is required. extracting and inspecting the files manually sometimes it may be desirable to extract or inspect the installable files directly, such as in enterprise deployment, or to browse the files before installation. the full installation package can be extracted using a decompression tool which supports the lzma compression method, such as7-ziporwinzip. once extracted, the cuda toolkit files will be in thecudatoolkitfolder, and similarily for cuda visual studio integration. within each directory is a .dll and .nvi file that can be ignored as they are not part of the installable files.",0
2.3.1.uninstalling the cuda software,#uninstalling-the-cuda-software,2.3.1.uninstalling the cuda software all subpackages can be uninstalled through the windows control panel by using the programs and features widget.,0
2.4.using conda to install the cuda software,#using-conda-to-install-the-cuda-software,2.4.using conda to install the cuda software this section describes the installation and configuration of cuda when using the conda installer. the conda packages are available athttps://anaconda.org/nvidia.,0
2.4.1.conda overview,#conda-overview,2.4.1.conda overview the conda installation installs the cuda toolkit. the installation steps are listed below.,0
2.4.2.installation,#installation,"2.4.2.installation to perform a basic install of all cuda toolkit components using conda, run the following command:",0
2.4.3.uninstallation,#uninstallation,"2.4.3.uninstallation to uninstall the cuda toolkit using conda, run the following command:",0
2.4.4.installing previous cuda releases,#installing-previous-cuda-releases,"2.4.4.installing previous cuda releases all conda packages released under a specific cuda version are labeled with that release version. to install a previous version, include that label in theinstallcommand such as:",0
2.5.use a suitable driver model,#use-a-suitable-driver-model,"2.5.use a suitable driver model on windows 10 and later, the operating system provides two driver models under which the nvidia driver may operate: tcc is enabled by default on most recent nvidia tesla gpus. to check which driver mode is in use and/or to switch driver modes, use thenvidia-smitool that is included with the nvidia driver installation (seenvidia-smi-hfor details).",0
2.6.verify the installation,#verify-the-installation,"2.6.verify the installation before continuing, it is important to verify that the cuda toolkit can find and communicate correctly with the cuda-capable hardware. to do this, you need to compile and run some of the included sample programs.",0
2.6.1.running the compiled examples,#running-the-compiled-examples,"2.6.1.running the compiled examples the version of the cuda toolkit can be checked by runningnvcc-vin a command prompt window. you can display a command prompt window by going to: start > all programs > accessories > command prompt cuda samples are located inhttps://github.com/nvidia/cuda-samples. to use the samples, clone the project, build the samples, and run them using the instructions on the github page. to verify a correct configuration of the hardware and software, it is highly recommended that you build and run thedevicequerysample program. the sample can be built using the provided vs solution files in thedevicequeryfolder. this assumes that you used the default installation directory structure. if cuda is installed and configured correctly, the output should look similar tofigure 1. the exact appearance and the output lines might be different on your system. the important outcomes are that a device was found, that the device(s) match what is installed in your system, and that the test passed. if a cuda-capable device and the cuda driver are installed butdevicequeryreports that no cuda-capable devices are present, ensure the deivce and driver are properly installed. running thebandwidthtestprogram, located in the same directory asdevicequeryabove, ensures that the system and the cuda-capable device are able to communicate correctly. the output should resemblefigure 2. the device name (second line) and the bandwidth numbers vary from system to system. the important items are the second line, which confirms a cuda device was found, and the second-to-last line, which confirms that all necessary tests passed. if the tests do not pass, make sure you do have a cuda-capable nvidia gpu on your system and make sure it is properly installed. to see a graphical representation of what cuda can do, run theparticlessample at",0
3.pip wheels,#pip-wheels,"3.pip wheels nvidia provides python wheels for installing cuda through pip, primarily for using cuda with python. these packages are intended for runtime use and do not currently include developer tools (these can be installed separately). please note that with this installation method, cuda installation environment is managed via pip and additional care must be taken to set up your host environment to use cuda outside the pip environment. prerequisites to install wheels, you must first install thenvidia-pyindexpackage, which is required in order to set up your pip installation to fetch additional python modules from the nvidia ngc pypi repo. if your pip and setuptools python modules are not up-to-date, then use the following command to upgrade these python modules. if these python modules are out-of-date then the commands which follow later in this section may fail. you should now be able to install thenvidia-pyindexmodule. if your project is using arequirements.txtfile, then you can add the following line to yourrequirements.txtfile as an alternative to installing thenvidia-pyindexpackage: procedure install the cuda runtime package: optionally, install additional packages as listed below using the following command: metapackages the following metapackages will install the latest version of the named component on windows for the indicated cuda version. cu12 should be read as cuda12. these metapackages install the following packages:",0
4.compiling cuda programs,#compiling-cuda-programs,"4.compiling cuda programs the project files in the cuda samples have been designed to provide simple, one-click builds of the programs that include all source code. to build the windows projects (for release or debug mode), use the provided*.slnsolution files for microsoft visual studio 2015 (deprecated in cuda 11.1), 2017, 2019, or 2022. you can use either the solution files located in each of the examples directories inhttps://github.com/nvidia/cuda-samples",0
4.1.compiling sample projects,#compiling-sample-projects,"4.1.compiling sample projects thebandwidthtestproject is a good sample project to build and run. it is located inhttps://github.com/nvidia/cuda-samples/tree/master/samples/1_utilities/bandwidthtest. if you elected to use the default installation location, the output is placed incudasamples\v12.5\bin\win64\release. build the program using the appropriate solution file and run the executable. if all works correctly, the output should be similar tofigure 2.",0
4.2.sample projects,#sample-projects,"4.2.sample projects the sample projects come in two configurations: debug and release (where release contains no debugging information) and different visual studio projects. a few of the example projects require some additional setup. these sample projects also make use of the$cuda_pathenvironment variable to locate where the cuda toolkit and the associated.propsfiles are. the environment variable is set automatically using the build customizationcuda12.5.propsfile, and is installed automatically as part of the cuda toolkit installation process. you can reference thiscuda12.5.propsfile when building your own cuda applications.",0
4.3.build customizations for new projects,#build-customizations-for-new-projects,"4.3.build customizations for new projects when creating a new cuda application, the visual studio project file must be configured to include cuda build customizations. to accomplish this, click file-> new | project nvidia-> cuda->, then select a template for your cuda toolkit version. for example, selecting the cuda 12.5 runtime template will configure your project for use with the cuda 12.5 toolkit. the new project is technically a c++ project (.vcxproj) that is preconfigured to use nvidias build customizations. all standard capabilities of visual studio c++ projects will be available. to specify a custom cuda toolkit location, undercuda c/c++, selectcommon, and set thecuda toolkit custom dirfield as desired. note that the selected toolkit must match the version of the build customizations.",0
4.4.build customizations for existing projects,#build-customizations-for-existing-projects,"4.4.build customizations for existing projects when adding cuda acceleration to existing applications, the relevant visual studio project files must be updated to include cuda build customizations. this can be done using one of the following two methods: while option 2 will allow your project to automatically use any new cuda toolkit version you may install in the future, selecting the toolkit version explicitly as in option 1 is often better in practice, because if there are new cuda configuration options added to the build customization rules accompanying the newer toolkit, you would not see those new options using option 2. if you use the$(cuda_path)environment variable to target a version of the cuda toolkit for building, and you perform an installation or uninstallation of any version of the cuda toolkit, you should validate that the$(cuda_path)environment variable points to the correct installation directory of the cuda toolkit for your purposes. you can access the value of the$(cuda_path)environment variable via the following steps: files which contain cuda code must be marked as acudac/c++file. this can done when adding the file by right clicking the project you wish to add the file to, selectingadd new item, selectingnvidia cuda 12.5\codecuda c/c++ file, and then selecting the file you wish to add. for advanced users, if you wish to try building your project against a newer cuda toolkit without making changes to any of your project files, go to the visual studio command prompt, change the current directory to the location of your project, and execute a command such as the following:",0
5.additional considerations,#additional-considerations,"5.additional considerations now that you have cuda-capable hardware and the nvidia cuda toolkit installed, you can examine and enjoy the numerous included programs. to begin using cuda to accelerate the performance of your own applications, consult the cudac programming guide, located in the cuda toolkit documentation directory. a number of helpful development tools are included in the cuda toolkit or are available for download from the nvidia developer zone to assist you as you develop your cuda programs, such as nvidiansight visual studio edition, and nvidia visual profiler. for technical support on programming questions, consult and participate in the developer forums athttps://developer.nvidia.com/cuda/.",0
6.notices,#notices,6.notices,9
6.1.notice,#notice,"6.1.notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation (nvidia) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material (defined below), code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer (terms of sale). nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion and/or use of nvidia products in such equipment or applications and therefore such inclusion and/or use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions and/or requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the nvidia product in any manner that is contrary to this document or (ii) customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding third-party products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents (together and separately, materials) are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product.",4
6.2.opencl,#opencl,6.2.opencl opencl is a trademark of apple inc. used under license to the khronos group inc.,4
6.3.trademarks,#trademarks,6.3.trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.,4
1.maxwell tuning guide,#id1,1.maxwell tuning guide,9
1.1.nvidia maxwell compute architecture,#nvidia-maxwell-compute-architecture,"1.1.nvidia maxwell compute architecture maxwell is nvidias next-generation architecture for cuda compute applications. maxwell retains and extends the same cuda programming model as in previous nvidia architectures such as fermi and kepler, and applications that follow the best practices for those architectures should typically see speedups on the maxwell architecture without any code changes. this guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging maxwell architectural features.1 maxwell introduces an all-new design for the streaming multiprocessor (sm) that dramatically improves energy efficiency. although the kepler smx design was extremely efficient for its generation, through its development, nvidias gpu architects saw an opportunity for another big leap forward in architectural efficiency; the maxwell sm is the realization of that vision. improvements to control logic partitioning, workload balancing, clock-gating granularity, compiler-based scheduling, number of instructions issued per clock cycle, and many other enhancements allow the maxwell sm (also calledsmm) to far exceed kepler smx efficiency. the first maxwell-based gpu is codenamedgm107and is designed for use in power-limited environments like notebooks and small form factor (sff) pcs. gm107 is described in a whitepaper entitlednvidia geforce gtx 750 ti: featuring first-generation maxwell gpu technology, designed for extreme performance per watt.2 the first gpu using the second-generation maxwell architecture is codenamedgm204. second-generation maxwell gpus retain the power efficiency of the earlier generation while delivering significantly higher performance. gm204 is described in a whitepaper entitlednvidia geforce gtx 980: featuring maxwell, the most advanced gpu ever made. compute programming features of gm204 are similar to those of gm107, except where explicitly noted in this guide. for details on the programming features discussed in this guide, please refer to thecuda c++ programming guide.",5
1.3.application compatibility,#application-compatibility,"1.3.application compatibility before addressing specific performance tuning issues covered in this guide, refer to themaxwell compatibility guide for cuda applicationsto ensure that your application is compiled in a way that is compatible with maxwell.",0
1.4.maxwell tuning,#maxwell-tuning,1.4.maxwell tuning,9
1.4.1.smm,#smm,"1.4.1.smm the maxwell streaming multiprocessor, smm, is similar in many respects to the kepler architectures smx. the key enhancements of smm over smx are geared toward improving efficiency without requiring significant increases in available parallelism per sm from the application.",5
1.4.2.memory throughput,#memory-throughput,1.4.2.memory throughput,5
1.4.3.shared memory,#shared-memory,1.4.3.shared memory,5
1.4.4.dynamic parallelism,#dynamic-parallelism,"1.4.4.dynamic parallelism gk110 introduced a new architectural feature called dynamic parallelism, which allows the gpu to create additional work for itself. a programming model enhancement leveraging this feature was introduced in cuda 5.0 to enable kernels running on gk110 to launch additional kernels onto the same gpu. smm brings dynamic parallelism into the mainstream by supporting it across the product line, even in lower-power chips such as gm107. this will benefit developers, as it means that applications will no longer need special-case algorithm implementations for high-end gpus that differ from those usable in more power-constrained environments.",5
2.revision history,#revision-history,2.revision history version 1.0 version 1.1 version 1.2,9
1.pascal tuning guide,#pascal-tuning-guide,1.pascal tuning guide,9
1.1.nvidia pascal compute architecture,#nvidia-pascal-compute-architecture,"1.1.nvidia pascal compute architecture pascal retains and extends the same cuda programming model provided by previous nvidia architectures such as maxwell, and applications that follow the best practices for those architectures should typically see speedups on the pascal architecture without any code changes. this guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging pascal architectural features.1 pascal architecture comprises two major variants: gp100 and gp104.2a detailed overview of the major improvements in gp100 and gp104 over earlier nvidia architectures are described in a pair of white papers entitlednvidia tesla p100: the most advanced datacenter accelerator ever builtfor gp100 andnvidia geforce gtx 1080: gaming perfectedfor gp104. for further details on the programming features discussed in this guide, please refer to thecuda c++ programming guide. some of the pascal features described in this guide are specific to either gp100 or gp104, as noted; if not specified, features apply to both pascal variants.",0
1.3.application compatibility,#application-compatibility,"1.3.application compatibility before addressing specific performance tuning issues covered in this guide, refer to thepascal compatibility guide for cuda applicationsto ensure that your application is compiled in a way that is compatible with pascal.",0
1.4.pascal tuning,#pascal-tuning,1.4.pascal tuning,9
1.4.1.streaming multiprocessor,#streaming-multiprocessor,1.4.1.streaming multiprocessor the pascal streaming multiprocessor (sm) is in many respects similar to that of maxwell. pascal further improves the already excellent power efficiency provided by the maxwell architecture through both an improved 16-nm finfet manufacturing process and various architectural modifications.,5
1.4.2.new arithmetic primitives,#new-arithmetic-primitives,1.4.2.new arithmetic primitives,6
1.4.4.atomic memory operations,#atomic-memory-operations,"1.4.4.atomic memory operations like maxwell, pascal provides nativesharedmemory atomic operations for 32-bit integer arithmetic, along with native 32 or 64-bit compare-and-swap (cas). developers coming from kepler, where shared memory atomics were implemented in software using a lock/update/unlock sequence, should see a large performance improvement particularly for heavily contended shared-memory atomics. pascal also extends atomic addition in global memory to function on fp64 data. theatomicadd()function in cuda has thus been generalized to support 32 and 64-bit integer and floating-point types. the rounding mode for all floating-point atomic operations is round-to-nearest-even in pascal. as in previous generations fp32atomicadd()flushes denormalized values to zero. for gp100 atomic operations may target the memories of peer gpus connected through nvlink. peer-to-peer atomics over nvlink use the same api as atomics targeting global memory. gpus connected via pcie do not support this feature. pascal gpus provide support system-wide atomic operations targetingmigratable allocations5if system-wide atomic visibility is desired, operations targeting migratable memory must specify a system scope by using theatomic[op]_system()intrinsics6. using the device-scope atomics (e.g.atomicadd()) on migratable memory remains valid, but enforces atomic visibility only within the local gpu. as implemented for pascal, system-wide atomics are intended to allow developers to experiment with enhanced memory models. they are implemented in software and some care is required to achieve good performance. when an atomic targets a migratable address backed by a remote memory space, the local processor page-faults so that the kernel can migrate the appropriate memory page to local memory. then the usual hardware instructions are used to execute the atomic. since the page is now locally resident, subsequent atomics from the same processor will not result in additional page-faults. however, atomic updates from different processors can incur frequent page-faults.",5
1.4.5.shared memory,#shared-memory,1.4.5.shared memory,5
1.4.6.inter-gpu communication,#inter-gpu-communication,1.4.6.inter-gpu communication,5
1.4.7.compute preemption,#compute-preemption,"1.4.7.compute preemption compute preemption is a new feature specific to gp100. compute preemption allows compute tasks running on the gpu to be interrupted at instruction-level granularity. the execution context (registers, shared memory, etc.) are swapped to gpu dram so that another application can be swapped in and run. compute preemption offers two key advantages for developers:",5
1.4.8.unified memory improvements,#unified-memory-improvements,"1.4.8.unified memory improvements pascal offers new hardware capabilities to extend unified memory (um) support. an extended 49-bit virtual addressing space allows pascal gpus to address the full 48-bit virtual address space of modern cpus as well as the memories of all gpus in the system through a single virtual address space, not limited by the physical memory sizes of any one processor. pascal gpus also support memory page faulting. page faulting allows applications to access the same managed memory allocations from both host and device without explicit synchronization. it also removes the need for the cuda runtime to pre-synchronizeallmanaged memory allocations before each kernel launch. instead, when a kernel accesses a non-resident memory page, it faults, and the page can be migrated to the gpu memory on-demand, or mapped into the gpu address space for access over pcie/nvlink interfaces. these features boost performance on pascal for many typical um workloads. in cases where the um heuristics prove suboptimal, further tuning is possible through a set of migration hints that can be added to the source code. on supporting operating system platforms, any memory allocated with the default os allocator (for example, malloc or new) can be accessed from both gpu and cpu code using the same pointer. in fact, all system virtual memory can be accessed from the gpu. on such systems, there is no need to explicitly allocate managed memory usingcudamallocmanaged().",5
1.nvidia hopper tuning guide,#nvidia-hopper-tuning-guide,1.nvidia hopper tuning guide,9
1.1.nvidia hopper gpu architecture,#nvidia-hopper-gpu-architecture,"1.1.nvidia hopper gpu architecture the nvidia hopper gpu architecture is nvidias latest architecture for cuda compute applications. the nvidia hopper gpu architecture retains and extends the same cuda programming model provided by previous nvidia gpu architectures such as nvidia ampere gpu architecture and nvidia turing, and applications that follow the best practices for those architectures should typically see speedups on the nvidia h100 gpu without any code changes. this guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the nvidia hopper gpu architectures features.1 for further details on the programming features discussed in this guide, refer to thecuda c++ programming guide.",5
1.3.application compatibility,#application-compatibility,"1.3.application compatibility before addressing specific performance tuning issues covered in this guide, refer to thehopper compatibility guide for cuda applicationsto ensure that your application is compiled in a way that is compatible with nvidia hopper.",0
1.4.nvidia hopper tuning,#nvidia-hopper-tuning,1.4.nvidia hopper tuning,9
1.4.1.streaming multiprocessor,#streaming-multiprocessor,1.4.1.streaming multiprocessor the nvidia hopper streaming multiprocessor (sm) provides the following improvements over turing and nvidia ampere gpu architectures.,5
1.4.2.memory system,#memory-system,1.4.2.memory system,5
1.4.3.fourth-generation nvlink,#fourth-generation-nvlink,"1.4.3.fourth-generation nvlink the fourth generation of nvidias high-speed nvlink interconnect is implemented in h100 gpus, which significantly enhances multi-gpu scalability, performance, and reliability with more links per gpu, much faster communication bandwidth, and improved error-detection and recovery features. the fourth-generation nvlink has the same bidirectional data rate of 50 gb/s per link. the total number of links available is increased to 18 in h100, compared to 12 in a100, yielding 900 gb/s bidirectional bandwidth compared to 600 gb/s for a100. nvlink operates transparently within the existing cuda model. transfers between nvlink-connected endpoints are automatically routed through nvlink, rather than pcie. thecudadeviceenablepeeraccess()api call remains necessary to enable direct transfers (over either pcie or nvlink) between gpus. thecudadevicecanaccesspeer()can be used to determine if peer access is possible between any pair of gpus.",5
2.revision history,#revision-history,2.revision history version 1.0,9
1.nvidia ada gpu architecture tuning guide,#nvidia-ada-gpu-architecture-tuning-guide,1.nvidia ada gpu architecture tuning guide,0
1.1.nvidia ada gpu architecture,#nvidia-ada-gpu-architecture,"1.1.nvidia ada gpu architecture the nvidiaada gpu architecture is nvidias latest architecture for cudacompute applications. the nvidia ada gpu architecture retains and extends the same cuda programming model provided by previous nvidia gpu architectures such as nvidia ampere and turing, and applications that follow the best practices for those architectures should typically see speedups on the nvidia ada architecture without any code changes. this guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the nvidia ada gpu architectures features.1 for further details on the programming features discussed in this guide, please refer to thecuda c++ programming guide.",0
1.3.application compatibility,#application-compatibility,"1.3.application compatibility before addressing specific performance tuning issues covered in this guide, refer to thenvidia ada gpu architecture compatibility guide for cuda applicationsto ensure that your application is compiled in a way that is compatible with the nvidia ada gpu architecture.",0
1.4.nvidia ada gpu architecture tuning,#nvidia-ada-gpu-architecture-tuning,1.4.nvidia ada gpu architecture tuning,5
1.4.1.streaming multiprocessor,#streaming-multiprocessor,1.4.1.streaming multiprocessor the nvidia ada gpu architectures streaming multiprocessor (sm) provides the following improvements over turing and nvidia ampere gpu architectures.,5
1.libnvvm api,#libnvvm-api,1.libnvvm api,6
1.1.introduction,#introduction,"1.1.introduction libnvvm api provides an interface for generating ptx code from both binary
and text nvvm ir inputs. compatible input can be generated by tools and
libraries that produce llvm 7.0 ir and bitcode. support for reading the text
nvvm ir representation is deprecated and may be removed in a later release.",7
1.2.thread safety,#thread-safety,"1.2.thread safety libnvvm api provides a thread-safe interface to libnvvm. clients can take
advantage of improved compilation speeds by spawning multiple compilation
threads concurrently.",5
1.3.module,#module,"1.3.module this chapter presents the api of the libnvvm library.
here is a list of all modules:",6
2.error handling,#error-handling,2.error handling enumerations functions,6
2.1.enumerations,#enumerations,2.1.enumerations,9
2.2.functions,#functions,2.2.functions,9
3.general information query,#general-information-query,3.general information query functions,9
3.1.functions,#id1,3.1.functions,9
4.3.cudbgevent::cases_st union reference,https://docs.nvidia.com/cuda/debugger-api/unionCUDBGEvent_1_1cases__st.html#unionCUDBGEvent_1_1cases__st,4.3.cudbgevent::cases_st union reference,8
4.compilation,#compilation,4.compilation functions typedefs,6
4.1.functions,#id2,4.1.functions,9
4.2.typedefs,#typedefs,4.2.typedefs,6
4.13.cudbgeventcallbackdata40 struct reference,https://docs.nvidia.com/cuda/debugger-api/structCUDBGEventCallbackData40.html#structCUDBGEventCallbackData40,4.13.cudbgeventcallbackdata40 struct reference,8
6.deprecated list,https://docs.nvidia.com/cuda/debugger-api/deprecated.html#deprecated,6.deprecated list,9
1.introduction,#introduction,"1.introduction the fatbin creator apis are a set of apis which can be used at runtime to combine multiple cuda objects into one cuda fat binary (fatbin). the apis accept inputs in multiple formats, either device cubins, ptx, or lto-ir.
the output is a fatbin that can be loaded bycumoduleloaddataof the cuda driver api. the functionality in this library is similar to thefatbinaryoffline tool in the cuda toolkit, with the following advantages:",0
2.1.system requirements,#system-requirements,2.1.system requirements the fatbin creator library requires no special system configuration. it does not require a gpu.,9
2.2.installation,#installation,2.2.installation the fatbin creator library is part of the cuda toolkit release and the components are organized as follows in the cuda toolkit installation directory:,0
3.user interface,#user-interface,3.user interface this chapter presents the fatbin creator apis. basic usage of the api is explained inbasic usage.,9
3.1.error codes,#error-codes,3.1.error codes enumerations functions,6
3.2.fatbinary creation,#fatbinary-creation,3.2.fatbinary creation functions typedefs,6
3.2.2.typedefs,#typedefs,3.2.2.typedefs,6
3.3.supported options,#supported-options,"3.3.supported options nvfatbin supports the options below. option names are prefixed with a single dash (-). options that take a value have an assignment operator (=) followed by the option value, with no spaces, e.g.""-host=windows"". the supported options are:",2
4.basic usage,#basic-usage,"4.basic usage this section of the document uses a simple example to explain how to use the fatbin creator apis to link a program. for brevity and readability, error checks on the api return values are not shown. this example assumes we want to create a fatbin with a cubin for sm_52, ptx for sm_61, and ltoir for sm_70. we can create an instance of the fatbin creator and obtain an api handle to it as shown infigure 1. figure 1. fatbin creator creation and initialization of a program assume that we already have three inputs stored instd::vectors (cubin, ptx, and ltoir), which could be from code created withnvrtcand stored into vectors. (they do not have to be in vectors, this merely illustrates that both the data itself and its size are needed.) we can add the inputs as shown infigure 2. figure 2. inputs to the fatbin creator the fatbin can now be obtained. to obtain this we first allocate memory for it. and to allocate memory, we need to query the size of the fatbin which is done as shown infigure 3. figure 3. query size of the created fatbin the fatbin can now be queried as shown infigure 4. this fatbin can then be executed on the gpu by passing this to the cuda driver apis. figure 4. query the created fatbin when the fatbin creator is not needed anymore, it can be destroyed as shown infigure 5. figure 5. destroy the fatbin creator",6
5.compatibility,#compatibility,"5.compatibility the nvfatbin library is compatible across releases. the library major version itself must be >= the maximum major version of the inputs. for example, you can create a fatbin from a cubin created with 11.8 and one with 12.4 if your nvfatbin library is at least version 12.x.",6
6.example: runtime fatbin creation,#example-runtime-fatbin-creation,"6.example: runtime fatbin creation this section demonstrates runtime fatbin creation. there are two cubins. the cubins are generated online using nvrtc. these two cubins are then passed tonvfatbin*api functions, which put the cubins into a fatbin. note that this example requires a compatible gpu with drivers and nvrtc to work, even though the library doesnt require either.",6
6.1.code (online.cpp),#code-online-cpp,6.1.code (online.cpp),9
6.2.build instructions,#build-instructions,"6.2.build instructions assuming the environment variablecuda_pathpoints to cuda toolkit installation directory, build this example as:",0
6.3.notices,#notices,6.3.notices,9
6.3.1.notice,#notice,"6.3.1.notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation (nvidia) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material (defined below), code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer (terms of sale). nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion and/or use of nvidia products in such equipment or applications and therefore such inclusion and/or use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions and/or requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the nvidia product in any manner that is contrary to this document or (ii) customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding third-party products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents (together and separately, materials) are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product.",4
6.3.2.opencl,#opencl,6.3.2.opencl opencl is a trademark of apple inc. used under license to the khronos group inc.,4
6.3.3.trademarks,#trademarks,6.3.3.trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.  2023 nvidia corporation & affiliates. all rights reserved.,4
3.9.dwarf utilities,https://docs.nvidia.com/cuda/debugger-api/group__DWARF.html#group__DWARF_1gb5bc313ab611a302aa214a94a960067e,3.9.dwarf utilities,9
1.preface,#preface,1.preface,9
1.1.what is this document?,#what-is-this-document,"1.1.what is this document? this best practices guide is a manual to help developers obtain the best performance from nvidiacudagpus. it presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for cuda-capable gpu architectures. while the contents can be used as a reference manual, you should be aware that some topics are revisited in different contexts as various programming and configuration topics are explored. as a result, it is recommended that first-time readers proceed through the guide sequentially. this approach will greatly improve your understanding of effective programming practices and enable you to better use the guide for reference later.",0
1.2.who should read this guide?,#who-should-read-this-guide,"1.2.who should read this guide? the discussions in this guide all use the c++ programming language, so you should be comfortable reading c++ code. this guide refers to and relies on several other documents that you should have at your disposal for reference, all of which are available at no cost from the cuda websitehttps://docs.nvidia.com/cuda/. the following documents are especially important resources: in particular, the optimization section of this guide assumes that you have already successfully downloaded and installed the cuda toolkit (if not, please refer to the relevant cuda installation guide for your platform) and that you have a basic familiarity with the cuda c++ programming language and environment (if not, please refer to the cuda c++ programming guide).",0
"1.3.assess, parallelize, optimize, deploy",#assess-parallelize-optimize-deploy,"1.3.assess, parallelize, optimize, deploy this guide introduces theassess, parallelize, optimize, deploy(apod)design cycle for applications with the goal of helping application developers to rapidly identify the portions of their code that would most readily benefit from gpu acceleration, rapidly realize that benefit, and begin leveraging the resulting speedups in production as early as possible. apod is a cyclical process: initial speedups can be achieved, tested, and deployed with only minimal initial investment of time, at which point the cycle can begin again by identifying further optimization opportunities, seeing additional speedups, and then deploying the even faster versions of the application into production.",5
1.3.1.assess,#assess,"1.3.1.assess for an existing project, the first step is to assess the application to locate the parts of the code that are responsible for the bulk of the execution time. armed with this knowledge, the developer can evaluate these bottlenecks for parallelization and start to investigate gpu acceleration. by understanding the end-users requirements and constraints and by applying amdahls and gustafsons laws, the developer can determine the upper bound of performance improvement from acceleration of the identified portions of the application.",5
1.3.2.parallelize,#parallelize,"1.3.2.parallelize having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code. depending on the original code, this can be as simple as calling into an existing gpu-optimized library such ascublas,cufft, orthrust, or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler. on the other hand, some applications designs will require some amount of refactoring to expose their inherent parallelism. as even cpu architectures will require exposing parallelism in order to improve or simply maintain the performance of sequential applications, the cuda family of parallel programming languages (cuda c++, cuda fortran, etc.) aims to make the expression of this parallelism as simple as possible, while simultaneously enabling operation on cuda-capable gpus designed for maximum parallel throughput.",5
1.3.3.optimize,#optimize,"1.3.3.optimize after each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance. since there are many possible optimizations that can be considered, having a good understanding of the needs of the application can help to make the process as smooth as possible. however, as with apod as a whole, program optimization is an iterative process (identify an opportunity for optimization, apply and test the optimization, verify the speedup achieved, and repeat), meaning that it is not necessary for a programmer to spend large amounts of time memorizing the bulk of all possible optimization strategies prior to seeing good speedups. instead, strategies can be applied incrementally as they are learned. optimizations can be applied at various levels, from overlapping data transfers with computation all the way down to fine-tuning floating-point operation sequences. the available profiling tools are invaluable for guiding this process, as they can help suggest a next-best course of action for the developers optimization efforts and provide references into the relevant portions of the optimization section of this guide.",5
1.3.4.deploy,#deploy,"1.3.4.deploy having completed the gpu acceleration of one or more components of the application it is possible to compare the outcome with the original expectation. recall that the initialassessstep allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots. before tackling other hotspots to improve the total speedup, the developer should consider taking the partially parallelized implementation and carry it through to production. this is important for a number of reasons; for example, it allows the user to profit from their investment as early as possible (the speedup may be partial but is still valuable), and it minimizes risk for the developer and the user by providing an evolutionary rather than revolutionary set of changes to the application.",5
1.4.recommendations and best practices,#recommendations-and-best-practices,"1.4.recommendations and best practices throughout this guide, specific recommendations are made regarding the design and implementation of cuda c++ code. these recommendations are categorized by priority, which is a blend of the effect of the recommendation and its scope. actions that present substantial improvements for most cuda applications have the highest priority, while small optimizations that affect only very specific situations are given a lower priority. before implementing lower priority recommendations, it is good practice to make sure all higher priority recommendations that are relevant have already been applied. this approach will tend to provide the best results for the time invested and will avoid the trap of premature optimization. the criteria of benefit and scope for establishing priority will vary depending on the nature of the program. in this guide, they represent a typical case. your code might reflect different priority factors. regardless of this possibility, it is good practice to verify that no higher-priority recommendations have been overlooked before undertaking lower-priority items.",5
1.5.assessing your application,#assessing-your-application,"1.5.assessing your application from supercomputers to mobile phones, modern processors increasingly rely on parallelism to provide performance. the core computational unit, which includes control, arithmetic, registers and typically some cache, is replicated some number of times and connected to memory via a network. as a result, all modern processors require parallel code in order to achieve good utilization of their computational power. while processors are evolving to expose more fine-grained parallelism to the programmer, many existing applications have evolved either as serial codes or as coarse-grained parallel codes (for example, where the data is decomposed into regions processed in parallel, with sub-regions shared using mpi). in order to profit from any modern processor architecture, gpus included, the first steps are to assess the application to identify the hotspots, determine whether they can be parallelized, and understand the relevant workloads both now and in the future.",5
2.heterogeneous computing,#heterogeneous-computing,"2.heterogeneous computing cuda programming involves running code on two different platforms concurrently: ahostsystem with one or more cpus and one or more cuda-enabled nvidia gpudevices. while nvidia gpus are frequently associated with graphics, they are also powerful arithmetic engines capable of running thousands of lightweight threads in parallel. this capability makes them well suited to computations that can leverage parallel execution. however, the device is based on a distinctly different design from the host system, and its important to understand those differences and how they determine the performance of cuda applications in order to use cuda effectively.",5
2.1.differences between host and device,#differences-between-host-and-device,2.1.differences between host and device the primary differences are in threading model and in separate physical memories: these are the primary hardware differences between cpu hosts and gpu devices with respect to parallel programming. other differences are discussed as they arise elsewhere in this document. applications composed with these differences in mind can treat the host and device together as a cohesive heterogeneous system wherein each processing unit is leveraged to do the kind of work it does best: sequential work on the host and parallel work on the device.,5
2.2.what runs on a cuda-enabled device?,#what-runs-on-a-cuda-enabled-device,2.2.what runs on a cuda-enabled device? the following issues should be considered when determining what parts of an application to run on the device:,0
3.application profiling,#application-profiling,3.application profiling,9
3.1.profile,#profile,"3.1.profile many codes accomplish a significant portion of the work with a relatively small amount of code. using a profiler, the developer can identify such hotspots and start to compile a list of candidates for parallelization.",5
3.1.1.creating the profile,#creating-the-profile,"3.1.1.creating the profile there are many possible approaches to profiling the code, but in all cases the objective is the same: to identify the function or functions in which the application is spending most of its execution time. the most important consideration with any profiling activity is to ensure that the workload is realistic - i.e., that information gained from the test and decisions based upon that information are relevant to real data. using unrealistic workloads can lead to sub-optimal results and wasted effort both by causing developers to optimize for unrealistic problem sizes and by causing developers to concentrate on the wrong functions. there are a number of tools that can be used to generate the profile. the following example is based ongprof, which is an open-source profiler for linux platforms from the gnu binutils collection.",5
3.1.2.identifying hotspots,#identifying-hotspots,"3.1.2.identifying hotspots in the example above, we can clearly see that the functiongentimestep()takes one-third of the total running time of the application. this should be our first candidate function for parallelization.understanding scalingdiscusses the potential benefit we might expect from such parallelization. it is worth noting that several of the other functions in the above example also take up a significant portion of the overall running time, such ascalcstats()andcalcsummarydata(). parallelizing these functions as well should increase our speedup potential. however, since apod is a cyclical process, we might opt to parallelize these functions in a subsequent apod pass, thereby limiting the scope of our work in any given pass to a smaller set of incremental changes.",5
3.1.3.understanding scaling,#understanding-scaling,"3.1.3.understanding scaling the amount of performance benefit an application will realize by running on cuda depends entirely on the extent to which it can be parallelized. code that cannot be sufficiently parallelized should run on the host, unless doing so would result in excessive transfers between the host and the device. by understanding how applications can scale it is possible to set expectations and plan an incremental parallelization strategy.strong scaling and amdahls lawdescribes strong scaling, which allows us to set an upper bound for the speedup with a fixed problem size.weak scaling and gustafsons lawdescribes weak scaling, where the speedup is attained by growing the problem size. in many applications, a combination of strong and weak scaling is desirable.",5
4.parallelizing your application,#parallelizing-your-application,"4.parallelizing your application having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code. depending on the original code, this can be as simple as calling into an existing gpu-optimized library such ascublas,cufft, orthrust, or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler. on the other hand, some applications designs will require some amount of refactoring to expose their inherent parallelism. as even cpu architectures require exposing this parallelism in order to improve or simply maintain the performance of sequential applications, the cuda family of parallel programming languages (cuda c++, cuda fortran, etc.) aims to make the expression of this parallelism as simple as possible, while simultaneously enabling operation on cuda-capable gpus designed for maximum parallel throughput.",5
5.getting started,#getting-started,"5.getting started there are several key strategies for parallelizing sequential code. while the details of how to apply these strategies to a particular application is a complex and problem-specific topic, the general themes listed here apply regardless of whether we are parallelizing code to run on for multicore cpus or for use on cuda gpus.",5
5.1.parallel libraries,#parallel-libraries,"5.1.parallel libraries the most straightforward approach to parallelizing an application is to leverage existing libraries that take advantage of parallel architectures on our behalf. the cuda toolkit includes a number of such libraries that have been fine-tuned for nvidia cuda gpus, such ascublas,cufft, and so on. the key here is that libraries are most useful when they match well with the needs of the application. applications already using other blas libraries can often quite easily switch tocublas, for example, whereas applications that do little to no linear algebra will have little use forcublas. the same goes for other cuda toolkit libraries:cuffthas an interface similar to that offftw, etc. also of note is the thrust library, which is a parallel c++ template library similar to the c++ standard template library. thrust provides a rich collection of data parallel primitives such as scan, sort, and reduce, which can be composed together to implement complex algorithms with concise, readable source code. by describing your computation in terms of these high-level abstractions you provide thrust with the freedom to select the most efficient implementation automatically. as a result, thrust can be utilized in rapid prototyping of cuda applications, where programmer productivity matters most, as well as in production, where robustness and absolute performance are crucial.",5
5.2.parallelizing compilers,#parallelizing-compilers,"5.2.parallelizing compilers another common approach to parallelization of sequential codes is to make use of parallelizing compilers. often this means the use of directives-based approaches, where the programmer uses a pragma or other similar notation to provide hints to the compiler about where parallelism can be found without needing to modify or adapt the underlying code itself. by exposing parallelism to the compiler, directives allow the compiler to do the detailed work of mapping the computation onto the parallel architecture. the openacc standard provides a set of compiler directives to specify loops and regions of code in standard c, c++ and fortran that should be offloaded from a host cpu to an attached accelerator such as a cuda gpu. the details of managing the accelerator device are handled implicitly by an openacc-enabled compiler and runtime. seehttp://www.openacc.org/for details.",5
5.3.coding to expose parallelism,#coding-to-expose-parallelism,"5.3.coding to expose parallelism for applications that need additional functionality or performance beyond what existing parallel libraries or parallelizing compilers can provide, parallel programming languages such as cuda c++ that integrate seamlessly with existing sequential code are essential. once we have located a hotspot in our applications profile assessment and determined that custom code is the best approach, we can use cuda c++ to expose the parallelism in that portion of our code as a cuda kernel. we can then launch this kernel onto the gpu and retrieve the results without requiring major rewrites to the rest of our application. this approach is most straightforward when the majority of the total running time of our application is spent in a few relatively isolated portions of the code. more difficult to parallelize are applications with a very flat profile - i.e., applications where the time spent is spread out relatively evenly across a wide portion of the code base. for the latter variety of application, some degree of code refactoring to expose the inherent parallelism in the application might be necessary, but keep in mind that this refactoring work will tend to benefit all future architectures, cpu and gpu alike, so it is well worth the effort should it become necessary.",5
6.getting the right answer,#getting-the-right-answer,"6.getting the right answer obtaining the right answer is clearly the principal goal of all computation. on parallel systems, it is possible to run into difficulties not typically found in traditional serial-oriented programming. these include threading issues, unexpected values due to the way floating-point values are computed, and challenges arising from differences in the way cpu and gpu processors operate. this chapter examines issues that can affect the correctness of returned data and points to appropriate solutions.",5
6.1.verification,#verification,6.1.verification,9
6.1.1.reference comparison,#reference-comparison,"6.1.1.reference comparison a key aspect of correctness verification for modifications to any existing program is to establish some mechanism whereby previous known-good reference outputs from representative inputs can be compared to new results. after each change is made, ensure that the results match using whatever criteria apply to the particular algorithm. some will expect bitwise identical results, which is not always possible, especially where floating-point arithmetic is concerned; seenumerical accuracy and precisionregarding numerical accuracy. for other algorithms, implementations may be considered correct if they match the reference within some small epsilon. note that the process used for validating numerical results can easily be extended to validate performance results as well. we want to ensure that each change we make is correctandthat it improves performance (and by how much). checking these things frequently as an integral part of our cyclical apod process will help ensure that we achieve the desired results as rapidly as possible.",6
6.1.2.unit testing,#unit-testing,"6.1.2.unit testing a useful counterpart to the reference comparisons described above is to structure the code itself in such a way that is readily verifiable at the unit level. for example, we can write our cuda kernels as a collection of many short__device__functions rather than one large monolithic__global__function; each device function can be tested independently before hooking them all together. for example, many kernels have complex addressing logic for accessing memory in addition to their actual computation. if we validate our addressing logic separately prior to introducing the bulk of the computation, then this will simplify any later debugging efforts. (note that the cuda compiler considers any device code that does not contribute to a write to global memory as dead code subject to elimination, so we must at least writesomethingout to global memory as a result of our addressing logic in order to successfully apply this strategy.) going a step further, if most functions are defined as__host____device__rather than just__device__functions, then these functions can be tested on both the cpu and the gpu, thereby increasing our confidence that the function is correct and that there will not be any unexpected differences in the results. if therearedifferences, then those differences will be seen early and can be understood in the context of a simple function. as a useful side effect, this strategy will allow us a means to reduce code duplication should we wish to include both cpu and gpu execution paths in our application: if the bulk of the work of our cuda kernels is done in__host____device__functions, we can easily call those functions from both the host codeandthe device code without duplication.",5
6.2.debugging,#debugging,"6.2.debugging cuda-gdb is a port of the gnu debugger that runs on linux and mac; see:https://developer.nvidia.com/cuda-gdb. the nvidia nsight visual studio edition for microsoft windows 7, windows hpc server 2008, windows 8.1, and windows 10 is available as a free plugin for microsoft visual studio; see:https://developer.nvidia.com/nsight-visual-studio-edition. several third-party debuggers support cuda debugging as well; see:https://developer.nvidia.com/debugging-solutionsfor more details.",0
6.3.numerical accuracy and precision,#numerical-accuracy-and-precision,6.3.numerical accuracy and precision incorrect or unexpected results arise principally from issues of floating-point accuracy due to the way floating-point values are computed and stored. the following sections explain the principal items of interest. other peculiarities of floating-point arithmetic are presented in features and technical specifications of the cuda c++ programming guide as well as in a whitepaper and accompanying webinar on floating-point precision and performance available fromhttps://developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus.,0
6.3.1.single vs. double precision,#single-vs-double-precision,"6.3.1.single vs. double precision devices ofcompute capability1.3 and higher provide native support for double-precision floating-point values (that is, values 64 bits wide). results obtained using double-precision arithmetic will frequently differ from the same operation performed via single-precision arithmetic due to the greater precision of the former and due to rounding issues. therefore, it is important to be sure to compare values of like precision and to express the results within a certain tolerance rather than expecting them to be exact.",6
6.3.2.floating point math is not associative,#floating-point-math-is-not-associative,"6.3.2.floating point math is not associative each floating-point arithmetic operation involves a certain amount of rounding. consequently, the order in which arithmetic operations are performed is important. if a, b, and c are floating-point values, (a+b)+c is not guaranteed to equal a+(b+c) as it is in symbolic math. when you parallelize computations, you potentially change the order of operations and therefore the parallel results might not match sequential results. this limitation is not specific to cuda, but an inherent part of parallel computation on floating-point values.",5
6.3.3.ieee 754 compliance,#ieee-754-compliance,"6.3.3.ieee 754 compliance all cuda compute devices follow the ieee 754 standard for binary floating-point representation, with some small exceptions. these exceptions, which are detailed in features and technical specifications of the cuda c++ programming guide, can lead to results that differ from ieee 754 values computed on the host system. one of the key differences is the fused multiply-add (fma) instruction, which combines multiply-add operations into a single instruction execution. its result will often differ slightly from results obtained by doing the two operations separately.",6
6.3.4.x86 80-bit computations,#x86-80-bit-computations,"6.3.4.x86 80-bit computations x86 processors can use an 80-bitdouble extended precisionmath when performing floating-point calculations. the results of these calculations can frequently differ from pure 64-bit operations performed on the cuda device. to get a closer match between values, set the x86 host processor to use regular double or single precision (64 bits and 32 bits, respectively). this is done with thefldcwx86 assembly instruction or the equivalent operating system api.",6
7.optimizing cuda applications,#optimizing-cuda-applications,"7.optimizing cuda applications after each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance. since there are many possible optimizations that can be considered, having a good understanding of the needs of the application can help to make the process as smooth as possible. however, as with apod as a whole, program optimization is an iterative process (identify an opportunity for optimization, apply and test the optimization, verify the speedup achieved, and repeat), meaning that it is not necessary for a programmer to spend large amounts of time memorizing the bulk of all possible optimization strategies prior to seeing good speedups. instead, strategies can be applied incrementally as they are learned. optimizations can be applied at various levels, from overlapping data transfers with computation all the way down to fine-tuning floating-point operation sequences. the available profiling tools are invaluable for guiding this process, as they can help suggest a next-best course of action for the developers optimization efforts and provide references into the relevant portions of the optimization section of this guide.",5
8.performance metrics,#performance-metrics,"8.performance metrics when attempting to optimize cuda code, it pays to know how to measure performance accurately and to understand the role that bandwidth plays in performance measurement. this chapter discusses how to correctly measure performance using cpu timers and cuda events. it then explores how bandwidth affects performance metrics and how to mitigate some of the challenges it poses.",5
8.1.timing,#timing,"8.1.timing cuda calls and kernel executions can be timed using either cpu or gpu timers. this section examines the functionality, advantages, and pitfalls of both approaches.",5
8.1.1.using cpu timers,#using-cpu-timers,"8.1.1.using cpu timers any cpu timer can be used to measure the elapsed time of a cuda call or kernel execution. the details of various cpu timing approaches are outside the scope of this document, but developers should always be aware of the resolution their timing calls provide. when using cpu timers, it is critical to remember that many cuda api functions are asynchronous; that is, they return control back to the calling cpu thread prior to completing their work. all kernel launches are asynchronous, as are memory-copy functions with theasyncsuffix on their names. therefore, to accurately measure the elapsed time for a particular call or sequence of cuda calls, it is necessary to synchronize the cpu thread with the gpu by callingcudadevicesynchronize()immediately before starting and stopping the cpu timer.cudadevicesynchronize()blocks the calling cpu thread until all cuda calls previously issued by the thread are completed. although it is also possible to synchronize the cpu thread with a particular stream or event on the gpu, these synchronization functions are not suitable for timing code in streams other than the default stream.cudastreamsynchronize()blocks the cpu thread until all cuda calls previously issued into the given stream have completed.cudaeventsynchronize()blocks until a given event in a particular stream has been recorded by the gpu. because the driver may interleave execution of cuda calls from other non-default streams, calls in other streams may be included in the timing. because the default stream, stream 0, exhibits serializing behavior for work on the device (an operation in the default stream can begin only after all preceding calls in any stream have completed; and no subsequent operation in any stream can begin until it finishes), these functions can be used reliably for timing in the default stream. be aware that cpu-to-gpu synchronization points such as those mentioned in this section imply a stall in the gpus processing pipeline and should thus be used sparingly to minimize their performance impact.",5
8.1.2.using cuda gpu timers,#using-cuda-gpu-timers,"8.1.2.using cuda gpu timers the cuda event api provides calls that create and destroy events, record events (including a timestamp), and convert timestamp differences into a floating-point value in milliseconds.how to time code using cuda eventsillustrates their use. how to time code using cuda events herecudaeventrecord()is used to place thestartandstopevents into the default stream, stream 0. the device will record a timestamp for the event when it reaches that event in the stream. thecudaeventelapsedtime()function returns the time elapsed between the recording of thestartandstopevents. this value is expressed in milliseconds and has a resolution of approximately half a microsecond. like the other calls in this listing, their specific operation, parameters, and return values are described in thecuda toolkit reference manual. note that the timings are measured on the gpu clock, so the timing resolution is operating-system-independent.",0
8.2.bandwidth,#bandwidth,"8.2.bandwidth bandwidth - the rate at which data can be transferred - is one of the most important gating factors for performance. almost all changes to code should be made in the context of how they affect bandwidth. as described inmemory optimizationsof this guide, bandwidth can be dramatically affected by the choice of memory in which data is stored, how the data is laid out and the order in which it is accessed, as well as other factors. to measure performance accurately, it is useful to calculate theoretical and effective bandwidth. when the latter is much lower than the former, design or implementation details are likely to reduce bandwidth, and it should be the primary goal of subsequent optimization efforts to increase it.",5
8.2.1.theoretical bandwidth calculation,#theoretical-bandwidth-calculation,"8.2.1.theoretical bandwidth calculation theoretical bandwidth can be calculated using hardware specifications available in the product literature. for example, the nvidia tesla v100 uses hbm2 (double data rate) ram with a memory clock rate of 877 mhz and a 4096-bit-wide memory interface. using these data items, the peak theoretical memory bandwidth of the nvidia tesla v100 is 898 gb/s: \(\left. \left( 0.877 \times 10^{9} \right. \times (4096/8) \times 2 \right) \div 10^{9} = 898\text{gb/s}\) in this calculation, the memory clock rate is converted in to hz, multiplied by the interface width (divided by 8, to convert bits to bytes) and multiplied by 2 due to the double data rate. finally, this product is divided by 109to convert the result to gb/s.",5
8.2.2.effective bandwidth calculation,#effective-bandwidth-calculation,"8.2.2.effective bandwidth calculation effective bandwidth is calculated by timing specific program activities and by knowing how data is accessed by the program. to do so, use this equation: \(\text{effective\ bandwidth} = \left( {\left( b_{r} + b_{w} \right) \div 10^{9}} \right) \div \text{time}\) here, the effective bandwidth is in units of gb/s, bris the number of bytes read per kernel, bwis the number of bytes written per kernel, and time is given in seconds. for example, to compute the effective bandwidth of a 2048 x 2048 matrix copy, the following formula could be used: \(\text{effective\ bandwidth} = \left( {\left( 2048^{2} \times 4 \times 2 \right) \div 10^{9}} \right) \div \text{time}\) the number of elements is multiplied by the size of each element (4 bytes for a float), multiplied by 2 (because of the readandwrite), divided by 109(or 1,0243) to obtain gb of memory transferred. this number is divided by the time in seconds to obtain gb/s.",5
8.2.3.throughput reported by visual profiler,#throughput-reported-by-visual-profiler,"8.2.3.throughput reported by visual profiler for devices withcompute capabilityof 2.0 or greater, the visual profiler can be used to collect several different memory throughput measures. the following throughput metrics can be displayed in the details or detail graphs view: the requested global load throughput and requested global store throughput values indicate the global memory throughput requested by the kernel and therefore correspond to the effective bandwidth obtained by the calculation shown undereffective bandwidth calculation. because the minimum memory transaction size is larger than most word sizes, the actual memory throughput required for a kernel can include the transfer of data not used by the kernel. for global memory accesses, this actual throughput is reported by the global load throughput and global store throughput values. its important to note that both numbers are useful. the actual memory throughput shows how close the code is to the hardware limit, and a comparison of the effective or requested bandwidth to the actual bandwidth presents a good estimate of how much bandwidth is wasted by suboptimal coalescing of memory accesses (seecoalesced access to global memory). for global memory accesses, this comparison of requested memory bandwidth to actual memory bandwidth is reported by the global memory load efficiency and global memory store efficiency metrics.",5
9.memory optimizations,#memory-optimizations,9.memory optimizations memory optimizations are the most important area for performance. the goal is to maximize the use of the hardware by maximizing bandwidth. bandwidth is best served by using as much fast memory and as little slow-access memory as possible. this chapter discusses the various kinds of memory on the host and device and how best to set up data items to use the memory effectively.,5
9.1.data transfer between host and device,#data-transfer-between-host-and-device,"9.1.data transfer between host and device the peak theoretical bandwidth between the device memory and the gpu is much higher (898 gb/s on the nvidia tesla v100, for example) than the peak theoretical bandwidth between host memory and device memory (16 gb/s on the pcie x16 gen3). hence, for best overall application performance, it is important to minimize data transfer between the host and the device, even if that means running kernels on the gpu that do not demonstrate any speedup compared with running them on the host cpu. intermediate data structures should be created in device memory, operated on by the device, and destroyed without ever being mapped by the host or copied to host memory. also, because of the overhead associated with each transfer, batching many small transfers into one larger transfer performs significantly better than making each transfer separately, even if doing so requires packing non-contiguous regions of memory into a contiguous buffer and then unpacking after the transfer. finally, higher bandwidth between the host and the device is achieved when usingpage-locked(orpinned) memory, as discussed in the cuda c++ programming guide and thepinned memorysection of this document.",5
9.1.1.pinned memory,#pinned-memory,"9.1.1.pinned memory page-locked or pinned memory transfers attain the highest bandwidth between the host and the device. on pcie x16 gen3 cards, for example, pinned memory can attain roughly 12 gb/s transfer rates. pinned memory is allocated using thecudahostalloc()functions in the runtime api. thebandwidthtestcuda sample shows how to use these functions as well as how to measure memory transfer performance. for regions of system memory that have already been pre-allocated,cudahostregister()can be used to pin the memory on-the-fly without the need to allocate a separate buffer and copy the data into it. pinned memory should not be overused. excessive use can reduce overall system performance because pinned memory is a scarce resource, but how much is too much is difficult to know in advance. furthermore, the pinning of system memory is a heavyweight operation compared to most normal system memory allocations, so as with all optimizations, test the application and the systems it runs on for optimal performance parameters.",5
9.1.2.asynchronous and overlapping transfers with computation,#asynchronous-and-overlapping-transfers-with-computation,"9.1.2.asynchronous and overlapping transfers with computation data transfers between the host and the device usingcudamemcpy()are blocking transfers; that is, control is returned to the host thread only after the data transfer is complete. thecudamemcpyasync()function is a non-blocking variant ofcudamemcpy()in which control is returned immediately to the host thread. in contrast withcudamemcpy(), the asynchronous transfer versionrequirespinned host memory (seepinned memory), and it contains an additional argument, a stream id. astreamis simply a sequence of operations that are performed in order on the device. operations in different streams can be interleaved and in some cases overlapped - a property that can be used to hide data transfers between the host and the device. asynchronous transfers enable overlap of data transfers with computation in two different ways. on all cuda-enabled devices, it is possible to overlap host computation with asynchronous data transfers and with device computations. for example,overlapping computation and data transfersdemonstrates how host computation in the routinecpufunction()is performed while data is transferred to the device and a kernel using the device is executed. overlapping computation and data transfers the last argument to thecudamemcpyasync()function is the stream id, which in this case uses the default stream, stream 0. the kernel also uses the default stream, and it will not begin execution until the memory copy completes; therefore, no explicit synchronization is needed. because the memory copy and the kernel both return control to the host immediately, the host functioncpufunction()overlaps their execution. inoverlapping computation and data transfers, the memory copy and kernel execution occur sequentially. on devices that are capable of concurrent copy and compute, it is possible to overlap kernel execution on the device with data transfers between the host and the device. whether a device has this capability is indicated by theasyncenginecountfield of thecudadevicepropstructure (or listed in the output of thedevicequerycuda sample). on devices that have this capability, the overlap once again requires pinned host memory, and, in addition, the data transfer and kernel must use different, non-default streams (streams with non-zero stream ids). non-default streams are required for this overlap because memory copy, memory set functions, and kernel calls that use the default stream begin only after all preceding calls on the device (in any stream) have completed, and no operation on the device (in any stream) commences until they are finished. concurrent copy and executeillustrates the basic technique. concurrent copy and execute in this code, two streams are created and used in the data transfer and kernel executions as specified in the last arguments of thecudamemcpyasynccall and the kernels execution configuration. concurrent copy and executedemonstrates how to overlap kernel execution with asynchronous data transfer. this technique could be used when the data dependency is such that the data can be broken into chunks and transferred in multiple stages, launching multiple kernels to operate on each chunk as it arrives.sequential copy and executeandstaged concurrent copy and executedemonstrate this. they produce equivalent results. the first segment shows the reference sequential implementation, which transfers and operates on an array ofnfloats (wherenis assumed to be evenly divisible by nthreads). sequential copy and execute staged concurrent copy and executeshows how the transfer and kernel execution can be broken up into nstreams stages. this approach permits some overlapping of the data transfer and execution. staged concurrent copy and execute (instaged concurrent copy and execute, it is assumed thatnis evenly divisible bynthreads*nstreams.) because execution within a stream occurs sequentially, none of the kernels will launch until the data transfers in their respective streams complete. current gpus can simultaneously process asynchronous data transfers and execute kernels. gpus with a single copy engine can perform one asynchronous data transfer and execute kernels whereas gpus with two copy engines can simultaneously perform one asynchronous data transfer from the host to the device, one asynchronous data transfer from the device to the host, and execute kernels. the number of copy engines on a gpu is given by theasyncenginecountfield of thecudadevicepropstructure, which is also listed in the output of thedevicequerycuda sample. (it should be mentioned that it is not possible to overlap a blocking transfer with an asynchronous transfer, because the blocking transfer occurs in the default stream, so it will not begin until all previous cuda calls complete. it will not allow any other cuda call to begin until it has completed.) a diagram depicting the timeline of execution for the two code segments is shown infigure 1, andnstreamsis equal to 4 forstaged concurrent copy and executein the bottom half of the figure. for this example, it is assumed that the data transfer and kernel execution times are comparable. in such cases, and when the execution time (te) exceeds the transfer time (tt), a rough estimate for the overall time iste + tt/nstreamsfor the staged version versuste + ttfor the sequential version. if the transfer time exceeds the execution time, a rough estimate for the overall time istt + te/nstreams.",5
9.1.3.zero copy,#zero-copy,"9.1.3.zero copy zero copyis a feature that was added in version 2.2 of the cuda toolkit. it enables gpu threads to directly access host memory. for this purpose, it requires mapped pinned (non-pageable) memory. on integrated gpus (i.e., gpus with the integrated field of the cuda device properties structure set to 1), mapped pinned memory is always a performance gain because it avoids superfluous copies as integrated gpu and cpu memory are physically the same. on discrete gpus, mapped pinned memory is advantageous only in certain cases. because the data is not cached on the gpu, mapped pinned memory should be read or written only once, and the global loads and stores that read and write the memory should be coalesced. zero copy can be used in place of streams because kernel-originated data transfers automatically overlap kernel execution without the overhead of setting up and determining the optimal number of streams. the host code inzero-copy host codeshows how zero copy is typically set up. zero-copy host code in this code, thecanmaphostmemoryfield of the structure returned bycudagetdeviceproperties()is used to check that the device supports mapping host memory to the devices address space. page-locked memory mapping is enabled by callingcudasetdeviceflags()withcudadevicemaphost. note thatcudasetdeviceflags()must be called prior to setting a device or making a cuda call that requires state (that is, essentially, before a context is created). page-locked mapped host memory is allocated usingcudahostalloc(), and the pointer to the mapped device address space is obtained via the functioncudahostgetdevicepointer(). in the code inzero-copy host code,kernel()can reference the mapped pinned host memory using the pointera_mapin exactly the same was as it would if a_map referred to a location in device memory.",5
9.1.4.unified virtual addressing,#unified-virtual-addressing,"9.1.4.unified virtual addressing devices ofcompute capability2.0 and later support a special addressing mode calledunified virtual addressing(uva) on 64-bit linux and windows. with uva, the host memory and the device memories of all installed supported devices share a single virtual address space. prior to uva, an application had to keep track of which pointers referred to device memory (and for which device) and which referred to host memory as a separate bit of metadata (or as hard-coded information in the program) for each pointer. using uva, on the other hand, the physical memory space to which a pointer points can be determined simply by inspecting the value of the pointer usingcudapointergetattributes(). under uva, pinned host memory allocated withcudahostalloc()will have identical host and device pointers, so it is not necessary to callcudahostgetdevicepointer()for such allocations. host memory allocations pinned after-the-fact viacudahostregister(), however, will continue to have different device pointers than their host pointers, socudahostgetdevicepointer()remains necessary in that case. uva is also a necessary precondition for enabling peer-to-peer (p2p) transfer of data directly across the pcie bus or nvlink for supported gpus in supported configurations, bypassing host memory. see the cuda c++ programming guide for further explanations and software requirements for uva and p2p.",5
9.2.device memory spaces,#device-memory-spaces,"9.2.device memory spaces cuda devices use several memory spaces, which have different characteristics that reflect their distinct usages in cuda applications. these memory spaces include global, local, shared, texture, and registers, as shown infigure 2. of these different memory spaces, global memory is the most plentiful; see features and technical specifications of the cuda c++ programming guide for the amounts of memory available in each memory space at eachcompute capabilitylevel. global, local, and texture memory have the greatest access latency, followed by constant memory, shared memory, and the register file. the various principal traits of the memory types are shown intable 1. in the case of texture access, if a texture reference is bound to a linear array in global memory, then the device code can write to the underlying array. texture references that are bound to cuda arrays can be written to via surface-write operations by binding a surface to the same underlying cuda array storage). reading from a texture while writing to its underlying global memory array in the same kernel launch should be avoided because the texture caches are read-only and are not invalidated when the associated global memory is modified.",5
9.2.1.coalesced access to global memory,#coalesced-access-to-global-memory,"9.2.1.coalesced access to global memory a very important performance consideration in programming for cuda-capable gpu architectures is the coalescing of global memory accesses. global memory loads and stores by threads of a warp are coalesced by the device into as few as possible transactions. the access requirements for coalescing depend on the compute capability of the device and are documented in the cuda c++ programming guide. for devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp. for certain devices of compute capability 5.2, l1-caching of accesses to global memory can be optionally enabled. if l1-caching is enabled on these devices, the number of required transactions is equal to the number of required 128-byte aligned segments. on devices with gddr memory, accessing memory in a coalesced way is even more important when ecc is turned on. scattered accesses increase ecc memory transfer overhead, especially when writing data to global memory. coalescing concepts are illustrated in the following simple examples. these examples assume compute capability 6.0 or higher and that accesses are for 4-byte words, unless otherwise noted.",5
9.2.2.l2 cache,#l2-cache,"9.2.2.l2 cache starting with cuda 11.0, devices of compute capability 8.0 and above have the capability to influence persistence of data in the l2 cache. because l2 cache is on-chip, it potentially provides higher bandwidth and lower latency accesses to global memory. for more details refer to the l2 access management section in thecuda c++ programming guide.",5
9.2.3.shared memory,#shared-memory,"9.2.3.shared memory because it is on-chip, shared memory has much higher bandwidth and lower latency than local and global memory - provided there are no bank conflicts between the threads, as detailed in the following section.",5
9.2.4.local memory,#local-memory,"9.2.4.local memory local memory is so named because its scope is local to the thread, not because of its physical location. in fact, local memory is off-chip. hence, access to local memory is as expensive as access to global memory. in other words, the termlocalin the name does not imply faster access. local memory is used only to hold automatic variables. this is done by thenvcccompiler when it determines that there is insufficient register space to hold the variable. automatic variables that are likely to be placed in local memory are large structures or arrays that would consume too much register space and arrays that the compiler determines may be indexed dynamically. inspection of the ptx assembly code (obtained by compiling with-ptxor-keepcommand-line options tonvcc) reveals whether a variable has been placed in local memory during the first compilation phases. if it has, it will be declared using the.localmnemonic and accessed using theld.localandst.localmnemonics. if it has not, subsequent compilation phases might still decide otherwise, if they find the variable consumes too much register space for the targeted architecture. there is no way to check this for a specific variable, but the compiler reports total local memory usage per kernel (lmem) when run with the--ptxas-options=-voption.",7
9.2.5.texture memory,#texture-memory,"9.2.5.texture memory the read-only texture memory space is cached. therefore, a texture fetch costs one device memory read only on a cache miss; otherwise, it just costs one read from the texture cache. the texture cache is optimized for 2d spatial locality, so threads of the same warp that read texture addresses that are close together will achieve best performance. texture memory is also designed for streaming fetches with a constant latency; that is, a cache hit reduces dram bandwidth demand, but not fetch latency. in certain addressing situations, reading device memory through texture fetching can be an advantageous alternative to reading device memory from global or constant memory.",5
9.2.6.constant memory,#constant-memory,"9.2.6.constant memory there is a total of 64 kb constant memory on a device. the constant memory space is cached. as a result, a read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. as such, the constant cache is best when threads in the same warp accesses only a few distinct locations. if all threads of a warp access the same location, then constant memory can be as fast as a register access.",5
9.2.7.registers,#registers,"9.2.7.registers generally, accessing a register consumes zero extra clock cycles per instruction, but delays may occur due to register read-after-write dependencies and register memory bank conflicts. the compiler and hardware thread scheduler will schedule instructions as optimally as possible to avoid register memory bank conflicts. an application has no direct control over these bank conflicts. in particular, there is no register-related reason to pack data into vector data types such asfloat4orint4types.",5
9.3.allocation,#allocation,9.3.allocation device memory allocation and de-allocation viacudamalloc()andcudafree()are expensive operations. it is recommended to usecudamallocasync()andcudafreeasync()which are stream ordered pool allocators to manage device memory.,5
9.4.numa best practices,#numa-best-practices,"9.4.numa best practices some recent linux distributions enable automatic numa balancing (or autonuma) by default. in some instances, operations performed by automatic numa balancing may degrade the performance of applications running on nvidia gpus. for optimal performance, users should manually tune the numa characteristics of their application. the optimal numa tuning will depend on the characteristics and desired hardware affinities of each application and node, but in general applications computing on nvidia gpus are advised to choose a policy that disables automatic numa balancing. for example, on ibm newell power9 nodes (where the cpus correspond to numa nodes 0 and 8), use: to bind memory allocations to the cpus.",5
10.execution configuration optimizations,#execution-configuration-optimizations,"10.execution configuration optimizations one of the keys to good performance is to keep the multiprocessors on the device as busy as possible. a device in which work is poorly balanced across the multiprocessors will deliver suboptimal performance. hence, its important to design your application to use threads and blocks in a way that maximizes hardware utilization and to limit practices that impede the free distribution of work. a key concept in this effort is occupancy, which is explained in the following sections. hardware utilization can also be improved in some cases by designing your application so that multiple, independent kernels can execute at the same time. multiple kernels executing at the same time is known as concurrent kernel execution. concurrent kernel execution is described below. another important concept is the management of system resources allocated for a particular task. how to manage this resource utilization is discussed in the final sections of this chapter.",5
10.1.occupancy,#occupancy,"10.1.occupancy thread instructions are executed sequentially in cuda, and, as a result, executing other warps when one warp is paused or stalled is the only way to hide latencies and keep the hardware busy. some metric related to the number of active warps on a multiprocessor is therefore important in determining how effectively the hardware is kept busy. this metric isoccupancy. occupancy is the ratio of the number of active warps per multiprocessor to the maximum number of possible active warps. (to determine the latter number, see thedevicequerycuda sample or refer to compute capabilities in the cuda c++ programming guide.) another way to view occupancy is the percentage of the hardwares ability to process warps that is actively in use. higher occupancy does not always equate to higher performance-there is a point above which additional occupancy does not improve performance. however, low occupancy always interferes with the ability to hide memory latency, resulting in performance degradation. per thread resources required by a cuda kernel might limit the maximum block size in an unwanted way. in order to maintain forward compatibility to future hardware and toolkits and to ensure that at least one thread block can run on an sm, developers should include the single argument__launch_bounds__(maxthreadsperblock)which specifies the largest block size that the kernel will be launched with. failure to do so could lead to too many resources requested for launch errors. providing the two argument version of__launch_bounds__(maxthreadsperblock,minblockspermultiprocessor)can improve performance in some cases. the right value forminblockspermultiprocessorshould be determined using a detailed per kernel analysis.",5
10.1.1.calculating occupancy,#calculating-occupancy,"10.1.1.calculating occupancy one of several factors that determine occupancy is register availability. register storage enables threads to keep local variables nearby for low-latency access. however, the set of registers (known as theregister file) is a limited commodity that all threads resident on a multiprocessor must share. registers are allocated to an entire block all at once. so, if each thread block uses many registers, the number of thread blocks that can be resident on a multiprocessor is reduced, thereby lowering the occupancy of the multiprocessor. the maximum number of registers per thread can be set manually at compilation time per-file using the-maxrregcountoption or per-kernel using the__launch_bounds__qualifier (seeregister pressure). for purposes of calculating occupancy, the number of registers used by each thread is one of the key factors. for example, on devices ofcompute capability7.0 each multiprocessor has 65,536 32-bit registers and can have a maximum of 2048 simultaneous threads resident (64 warps x 32 threads per warp). this means that in one of these devices, for a multiprocessor to have 100% occupancy, each thread can use at most 32 registers. however, this approach of determining how register count affects occupancy does not take into account the register allocation granularity. for example, on a device of compute capability 7.0, a kernel with 128-thread blocks using 37 registers per thread results in an occupancy of 75% with 12 active 128-thread blocks per multi-processor, whereas a kernel with 320-thread blocks using the same 37 registers per thread results in an occupancy of 63% because only four 320-thread blocks can reside on a multiprocessor. furthermore, register allocations are rounded up to the nearest 256 registers per warp. the number of registers available, the maximum number of simultaneous threads resident on each multiprocessor, and the register allocation granularity vary over different compute capabilities. because of these nuances in register allocation and the fact that a multiprocessors shared memory is also partitioned between resident thread blocks, the exact relationship between register usage and occupancy can be difficult to determine. the--ptxasoptions=voption ofnvccdetails the number of registers used per thread for each kernel. see hardware multithreading of the cuda c++ programming guide for the register allocation formulas for devices of various compute capabilities and features and technical specifications of the cuda c++ programming guide for the total number of registers available on those devices. alternatively, nvidia provides an occupancy calculator in the form of an excel spreadsheet that enables developers to hone in on the optimal balance and to test different possible scenarios more easily. this spreadsheet, shown infigure 15, is calledcuda_occupancy_calculator.xlsand is located in the tools subdirectory of the cuda toolkit installation. in addition to the calculator spreadsheet, occupancy can be determined using the nvidia nsight compute profiler. details about occupancy are displayed in the occupancy section. an application can also use the occupancy api from the cuda runtime, e.g.cudaoccupancymaxactiveblockspermultiprocessor, to dynamically select launch configurations based on runtime parameters.",5
10.2.hiding register dependencies,#hiding-register-dependencies,"10.2.hiding register dependencies register dependencies arise when an instruction uses a result stored in a register written by an instruction before it. the latency of most arithmetic instructions is typically 4 cycles on devices of compute capability 7.0. so threads must wait approximatly 4 cycles before using an arithmetic result. however, this latency can be completely hidden by the execution of threads in other warps. seeregistersfor details.",5
10.3.thread and block heuristics,#thread-and-block-heuristics,"10.3.thread and block heuristics the dimension and size of blocks per grid and the dimension and size of threads per block are both important factors. the multidimensional aspect of these parameters allows easier mapping of multidimensional problems to cuda and does not play a role in performance. as a result, this section discusses size but not dimension. latency hiding and occupancy depend on the number of active warps per multiprocessor, which is implicitly determined by the execution parameters along with resource (register and shared memory) constraints. choosing execution parameters is a matter of striking a balance between latency hiding (occupancy) and resource utilization. choosing the execution configuration parameters should be done in tandem; however, there are certain heuristics that apply to each parameter individually. when choosing the first execution configuration parameter-the number of blocks per grid, orgrid size- the primary concern is keeping the entire gpu busy. the number of blocks in a grid should be larger than the number of multiprocessors so that all multiprocessors have at least one block to execute. furthermore, there should be multiple active blocks per multiprocessor so that blocks that arent waiting for a__syncthreads()can keep the hardware busy. this recommendation is subject to resource availability; therefore, it should be determined in the context of the second execution parameter - the number of threads per block, orblock size- as well as shared memory usage. to scale to future devices, the number of blocks per kernel launch should be in the thousands. when choosing the block size, it is important to remember that multiple concurrent blocks can reside on a multiprocessor, so occupancy is not determined by block size alone. in particular, a larger block size does not imply a higher occupancy. as mentioned inoccupancy, higher occupancy does not always equate to better performance. for example, improving occupancy from 66 percent to 100 percent generally does not translate to a similar increase in performance. a lower occupancy kernel will have more registers available per thread than a higher occupancy kernel, which may result in less register spilling to local memory; in particular, with a high degree of exposed instruction-level parallelism (ilp) it is, in some cases, possible to fully cover latency with a low occupancy. there are many such factors involved in selecting block size, and inevitably some experimentation is required. however, a few rules of thumb should be followed: note that when a thread block allocates more registers than are available on a multiprocessor, the kernel launch fails, as it will when too much shared memory or too many threads are requested.",5
10.4.effects of shared memory,#effects-of-shared-memory,"10.4.effects of shared memory shared memory can be helpful in several situations, such as helping to coalesce or eliminate redundant access to global memory. however, it also can act as a constraint on occupancy. in many cases, the amount of shared memory required by a kernel is related to the block size that was chosen, but the mapping of threads to shared memory elements does not need to be one-to-one. for example, it may be desirable to use a 64x64 element shared memory array in a kernel, but because the maximum number of threads per block is 1024, it is not possible to launch a kernel with 64x64 threads per block. in such cases, kernels with 32x32 or 64x16 threads can be launched with each thread processing four elements of the shared memory array. the approach of using a single thread to process multiple elements of a shared memory array can be beneficial even if limits such as threads per block are not an issue. this is because some operations common to each element can be performed by the thread once, amortizing the cost over the number of shared memory elements processed by the thread. a useful technique to determine the sensitivity of performance to occupancy is through experimentation with the amount of dynamically allocated shared memory, as specified in the third parameter of the execution configuration. by simply increasing this parameter (without modifying the kernel), it is possible to effectively reduce the occupancy of the kernel and measure its effect on performance.",5
10.5.concurrent kernel execution,#concurrent-kernel-execution,"10.5.concurrent kernel execution as described inasynchronous and overlapping transfers with computation, cuda streams can be used to overlap kernel execution with data transfers. on devices that are capable of concurrent kernel execution, streams can also be used to execute multiple kernels simultaneously to more fully take advantage of the devices multiprocessors. whether a device has this capability is indicated by theconcurrentkernelsfield of thecudadevicepropstructure (or listed in the output of thedevicequerycuda sample). non-default streams (streams other than stream 0) are required for concurrent execution because kernel calls that use the default stream begin only after all preceding calls on the device (in any stream) have completed, and no operation on the device (in any stream) commences until they are finished. the following example illustrates the basic technique. becausekernel1andkernel2are executed in different, non-default streams, a capable device can execute the kernels at the same time.",5
10.6.multiple contexts,#multiple-contexts,"10.6.multiple contexts cuda work occurs within a process space for a particular gpu known as acontext. the context encapsulates kernel launches and memory allocations for that gpu as well as supporting constructs such as the page tables. the context is explicit in the cuda driver api but is entirely implicit in the cuda runtime api, which creates and manages contexts automatically. with the cuda driver api, a cuda application process can potentially create more than one context for a given gpu. if multiple cuda application processes access the same gpu concurrently, this almost always implies multiple contexts, since a context is tied to a particular host process unlessmulti-process serviceis in use. while multiple contexts (and their associated resources such as global memory allocations) can be allocated concurrently on a given gpu, only one of these contexts can execute work at any given moment on that gpu; contexts sharing the same gpu are time-sliced. creating additional contexts incurs memory overhead for per-context data and time overhead for context switching. furthermore, the need for context switching can reduce utilization when work from several contexts could otherwise execute concurrently (see alsoconcurrent kernel execution). therefore, it is best to avoid multiple contexts per gpu within the same cuda application. to assist with this, the cuda driver api provides methods to access and manage a special context on each gpu called theprimary context. these are the same contexts used implicitly by the cuda runtime when there is not already a current context for a thread.",5
11.instruction optimization,#instruction-optimization,"11.instruction optimization awareness of how instructions are executed often permits low-level optimizations that can be useful, especially in code that is run frequently (the so-called hot spot in a program). best practices suggest that this optimization be performed after all higher-level optimizations have been completed.",5
11.1.arithmetic instructions,#arithmetic-instructions,"11.1.arithmetic instructions single-precision floats provide the best performance, and their use is highly encouraged. the throughput of individual arithmetic operations is detailed in the cuda c++ programming guide.",6
11.1.1.division modulo operations,#division-modulo-operations,"11.1.1.division modulo operations integer division and modulo operations are particularly costly and should be avoided or replaced with bitwise operations whenever possible: if\(n\)is a power of 2, (\(i/n\)) is equivalent to (\(i \gg {log2}(n)\)) and (\(i\% n\)) is equivalent to (\(i\&\left( {n - 1} \right)\)). the compiler will perform these conversions if n is literal. (for further information, refer to performance guidelines in the cuda c++ programming guide).",6
11.1.2.loop counters signed vs. unsigned,#loop-counters-signed-vs-unsigned,"11.1.2.loop counters signed vs. unsigned in the c language standard, unsigned integer overflow semantics are well defined, whereas signed integer overflow causes undefined results. therefore, the compiler can optimize more aggressively with signed arithmetic than it can with unsigned arithmetic. this is of particular note with loop counters: since it is common for loop counters to have values that are always positive, it may be tempting to declare the counters as unsigned. for slightly better performance, however, they should instead be declared as signed. for example, consider the following code: here, the sub-expressionstride*icould overflow a 32-bit integer, so ifiis declared as unsigned, the overflow semantics prevent the compiler from using some optimizations that might otherwise have applied, such as strength reduction. if insteadiis declared as signed, where the overflow semantics are undefined, the compiler has more leeway to use these optimizations.",6
11.1.3.reciprocal square root,#reciprocal-square-root,11.1.3.reciprocal square root the reciprocal square root should always be invoked explicitly asrsqrtf()for single precision andrsqrt()for double precision. the compiler optimizes1.0f/sqrtf(x)intorsqrtf()only when this does not violate ieee-754 semantics.,6
11.1.4.other arithmetic instructions,#other-arithmetic-instructions,"11.1.4.other arithmetic instructions the compiler must on occasion insert conversion instructions, introducing additional execution cycles. this is the case for: the latter case can be avoided by using single-precision floating-point constants, defined with anfsuffix such as3.141592653589793f,1.0f,0.5f. for single-precision code, use of the float type and the single-precision math functions are highly recommended. it should also be noted that the cuda math librarys complementary error function,erfcf(), is particularly fast with full single-precision accuracy.",6
11.1.5.exponentiation with small fractional arguments,#exponentiation-with-small-fractional-arguments,"11.1.5.exponentiation with small fractional arguments for some fractional exponents, exponentiation can be accelerated significantly compared to the use ofpow()by using square roots, cube roots, and their inverses. for those exponentiations where the exponent is not exactly representable as a floating-point number, such as 1/3, this can also provide much more accurate results, as use ofpow()magnifies the initial representational error. the formulas in the table below are valid forx>=0,x!=-0, that is,signbit(x)==0.",6
11.1.6.math libraries,#math-libraries,"11.1.6.math libraries two types of runtime math operations are supported. they can be distinguished by their names: some have names with prepended underscores, whereas others do not (e.g.,__functionname()versusfunctionname()). functions following the__functionname()naming convention map directly to the hardware level. they are faster but provide somewhat lower accuracy (e.g.,__sinf(x)and__expf(x)). functions followingfunctionname()naming convention are slower but have higher accuracy (e.g.,sinf(x)andexpf(x)). the throughput of__sinf(x),__cosf(x), and__expf(x)is much greater than that ofsinf(x),cosf(x), andexpf(x). the latter become even more expensive (about an order of magnitude slower) if the magnitude of the argumentxneeds to be reduced. moreover, in such cases, the argument-reduction code uses local memory, which can affect performance even more because of the high latency of local memory. more details are available in thecuda c++ programming guide. note also that whenever sine and cosine of the same argument are computed, thesincosfamily of instructions should be used to optimize performance: the-use_fast_mathcompiler option ofnvcccoerces everyfunctionname()call to the equivalent__functionname()call. it also disables single-precision denormal support and lowers the precision of single-precision division in general. this is an aggressive optimization that can both reduce numerical accuracy and alter special case handling. a more robust approach is to selectively introduce calls to fast intrinsic functions only if merited by performance gains and where altered behavior can be tolerated. note this switch is effective only on single-precision floating point. for small integer powers (e.g.,x2orx3), explicit multiplication is almost certainly faster than the use of general exponentiation routines such aspow(). while compiler optimization improvements continually seek to narrow this gap, explicit multiplication (or the use of an equivalent purpose-built inline function or macro) can have a significant advantage. this advantage is increased when several powers of the same base are needed (e.g., where bothx2andx5are calculated in close proximity), as this aids the compiler in its common sub-expression elimination (cse) optimization. for exponentiation using base 2 or 10, use the functionsexp2()orexpf2()andexp10()orexpf10()rather than the functionspow()orpowf(). bothpow()andpowf()are heavy-weight functions in terms of register pressure and instruction count due to the numerous special cases arising in general exponentiation and the difficulty of achieving good accuracy across the entire ranges of the base and the exponent. the functionsexp2(),exp2f(),exp10(), andexp10f(), on the other hand, are similar toexp()andexpf()in terms of performance, and can be as much as ten times faster than theirpow()/powf()equivalents. for exponentiation with an exponent of 1/3, use thecbrt()orcbrtf()function rather than the generic exponentiation functionspow()orpowf(), as the former are significantly faster than the latter. likewise, for exponentation with an exponent of -1/3, usercbrt()orrcbrtf(). replacesin(*<expr>)withsinpi(<expr>),cos(*<expr>)withcospi(<expr>), andsincos(*<expr>)withsincospi(<expr>). this is advantageous with regard to both accuracy and performance. as a particular example, to evaluate the sine function in degrees instead of radians, usesinpi(x/180.0). similarly, the single-precision functionssinpif(),cospif(), andsincospif()should replace calls tosinf(),cosf(), andsincosf()when the function argument is of the form*<expr>. (the performance advantagesinpi()has oversin()is due to simplified argument reduction; the accuracy advantage is becausesinpi()multiplies byonly implicitly, effectively using an infinitely precise mathematicalrather than a single- or double-precision approximation thereof.)",6
11.1.7.precision-related compiler flags,#precision-related-compiler-flags,"11.1.7.precision-related compiler flags by default, thenvcccompiler generates ieee-compliant code, but it also provides options to generate code that somewhat less accurate but faster: another, more aggressive, option is-use_fast_math, which coerces everyfunctionname()call to the equivalent__functionname()call. this makes the code run faster at the cost of diminished precision and accuracy. seemath libraries.",6
11.2.memory instructions,#memory-instructions,"11.2.memory instructions memory instructions include any instruction that reads from or writes to shared, local, or global memory. when accessing uncached local or global memory, there are hundreds of clock cycles of memory latency. as an example, the assignment operator in the following sample code has a high throughput, but, crucially, there is a latency of hundreds of clock cycles to read data from global memory: much of this global memory latency can be hidden by the thread scheduler if there are sufficient independent arithmetic instructions that can be issued while waiting for the global memory access to complete. however, it is best to avoid accessing global memory whenever possible.",5
12.control flow,#control-flow,12.control flow,9
12.1.branching and divergence,#branching-and-divergence,"12.1.branching and divergence flow control instructions (if,switch,do,for,while) can significantly affect the instruction throughput by causing threads of the same warp to diverge; that is, to follow different execution paths. if this happens, the different execution paths must be executed separately; this increases the total number of instructions executed for this warp. to obtain best performance in cases where the control flow depends on the thread id, the controlling condition should be written so as to minimize the number of divergent warps. this is possible because the distribution of the warps across the block is deterministic as mentioned in simt architecture of the cuda c++ programming guide. a trivial example is when the controlling condition depends only on (threadidx/wsize) wherewsizeis the warp size. in this case, no warp diverges because the controlling condition is perfectly aligned with the warps. for branches including just a few instructions, warp divergence generally results in marginal performance losses. for example, the compiler may use predication to avoid an actual branch. instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. threads with a false predicate do not write results, and also do not evaluate addresses or read operands. starting with the volta architecture, independent thread scheduling allows a warp to remain diverged outside of the data-dependent conditional block. an explicit__syncwarp()can be used to guarantee that the warp has reconverged for subsequent instructions.",5
12.2.branch predication,#branch-predication,"12.2.branch predication sometimes, the compiler may unroll loops or optimize outiforswitchstatements by using branch predication instead. in these cases, no warp can ever diverge. the programmer can also control loop unrolling using for more information on this pragma, refer to the cuda c++ programming guide. when using branch predication, none of the instructions whose execution depends on the controlling condition is skipped. instead, each such instruction is associated with a per-thread condition code or predicate that is set to true or false according to the controlling condition. although each of these instructions is scheduled for execution, only the instructions with a true predicate are actually executed. instructions with a false predicate do not write results, and they also do not evaluate addresses or read operands. the compiler replaces a branch instruction with predicated instructions only if the number of instructions controlled by the branch condition is less than or equal to a certain threshold.",5
13.deploying cuda applications,#deploying-cuda-applications,"13.deploying cuda applications having completed the gpu acceleration of one or more components of the application it is possible to compare the outcome with the original expectation. recall that the initialassessstep allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots. before tackling other hotspots to improve the total speedup, the developer should consider taking the partially parallelized implementation and carry it through to production. this is important for a number of reasons; for example, it allows the user to profit from their investment as early as possible (the speedup may be partial but is still valuable), and it minimizes risk for the developer and the user by providing an evolutionary rather than revolutionary set of changes to the application.",5
14.understanding the programming environment,#understanding-the-programming-environment,"14.understanding the programming environment with each generation of nvidia processors, new features are added to the gpu that cuda can leverage. consequently, its important to understand the characteristics of the architecture. programmers should be aware of two version numbers. the first is thecompute capability, and the second is the version number of the cuda runtime and cuda driver apis.",0
14.1.cuda compute capability,#cuda-compute-capability,"14.1.cuda compute capability thecompute capabilitydescribes the features of the hardware and reflects the set of instructions supported by the device as well as other specifications, such as the maximum number of threads per block and the number of registers per multiprocessor. higher compute capability versions are supersets of lower (that is, earlier) versions, so they are backward compatible. the compute capability of the gpu in the device can be queried programmatically as illustrated in thedevicequerycuda sample. the output for that program is shown infigure 16. this information is obtained by callingcudagetdeviceproperties()and accessing the information in the structure it returns. the major and minor revision numbers of the compute capability are shown on the seventh line offigure 16. device 0 of this system has compute capability 7.0. more details about the compute capabilities of various gpus are in cuda-enabled gpus and compute capabilities of the cuda c++ programming guide. in particular, developers should note the number of multiprocessors on the device, the number of registers and the amount of memory available, and any special capabilities of the device.",0
14.2.additional hardware data,#additional-hardware-data,"14.2.additional hardware data certain hardware features are not described by the compute capability. for example, the ability to overlap kernel execution with asynchronous data transfers between the host and the device is available on most but not all gpus irrespective of the compute capability. in such cases, callcudagetdeviceproperties()to determine whether the device is capable of a certain feature. for example, theasyncenginecountfield of the device property structure indicates whether overlapping kernel execution and data transfers is possible (and, if so, how many concurrent transfers are possible); likewise, thecanmaphostmemoryfield indicates whether zero-copy data transfers can be performed.",5
14.3.which compute capability target,#which-compute-capability-target,"14.3.which compute capability target to target specific versions of nvidia hardware and cuda software, use the-arch,-code, and-gencodeoptions ofnvcc. code that uses the warp shuffle operation, for example, must be compiled with-arch=sm_30(or higher compute capability). seebuilding for maximum compatibilityfor further discussion of the flags used for building code for multiple generations of cuda-capable device simultaneously.",0
14.4.cuda runtime,#cuda-runtime,"14.4.cuda runtime the host runtime component of the cuda software environment can be used only by host functions. it provides functions to handle the following: as compared to the lower-level cuda driver api, the cuda runtime greatly eases device management by providing implicit initialization, context management, and device code module management. the c++ host code generated bynvccutilizes the cuda runtime, so applications that link to this code will depend on the cuda runtime; similarly, any code that uses thecublas,cufft, and other cuda toolkit libraries will also depend on the cuda runtime, which is used internally by these libraries. the functions that make up the cuda runtime api are explained in the cuda toolkit reference manual. the cuda runtime handles kernel loading and setting up kernel parameters and launch configuration before the kernel is launched. the implicit driver version checking, code initialization, cuda context management, cuda module management (cubin to function mapping), kernel configuration, and parameter passing are all performed by the cuda runtime. it comprises two principal parts: for more information on the runtime api, refer to cuda runtime of the cuda c++ programming guide.",0
15.cuda compatibility developers guide,#cuda-compatibility-developer-s-guide,"15.cuda compatibility developers guide cuda toolkit is released on a monthly release cadence to deliver new features, performance improvements, and critical bug fixes. cuda compatibility allows users to update the latest cuda toolkit software (including the compiler, libraries, and tools) without requiring update to the entire driver stack. the cuda software environment consists of three parts: on linux systems, the cuda driver and kernel mode components are delivered together in the nvidia display driver package. this is shown in figure 1. the cuda compiler (nvcc), provides a way to handle cuda and non-cuda code (by splitting and steering compilation), along with the cuda runtime, is part of the cuda compiler toolchain. the cuda runtime api provides developers with high-level c++ interface for simplified management of devices, kernel executions etc., while the cuda driver api provides (cuda driver api) a low-level programming interface for applications to target nvidia hardware. built on top of these technologies are cuda libraries, some of which are included in the cuda toolkit, while others such as cudnn may be released independently of the cuda toolkit.",0
15.1.cuda toolkit versioning,#cuda-toolkit-versioning,"15.1.cuda toolkit versioning starting with cuda 11, the toolkit versions are based on an industry-standard semantic versioning scheme: .x.y.z, where: each component in the toolkit is recommended to be semantically versioned. from cuda 11.3 nvrtc is also semantically versioned. we will note some of them later on in the document. the versions of the components in the toolkit are available in thistable. compatibility of the cuda platform is thus intended to address a few scenarios: cuda supports several compatibility choices:",0
15.2.source compatibility,#source-compatibility,"15.2.source compatibility we define source compatibility as a set of guarantees provided by the library, where a well-formed application built against a specific version of the library (using the sdk) will continue to build and run without errors when a newer version of the sdk is installed. both the cuda driver and the cuda runtime are not source compatible across the different sdk releases. apis can be deprecated and removed. therefore, an application that compiled successfully on an older version of the toolkit may require changes in order to compile against a newer version of the toolkit. developers are notified through deprecation and documentation mechanisms of any current or upcoming changes. this does not mean that application binaries compiled using an older toolkit will not be supported anymore. application binaries rely on cuda driver api interface and even though the cuda driver api itself may also have changed across toolkit versions, cuda guarantees binary compatibility of the cuda driver api interface.",0
15.3.binary compatibility,#binary-compatibility,"15.3.binary compatibility we define binary compatibility as a set of guarantees provided by the library, where an application targeting the said library will continue to work when dynamically linked against a different version of the library. the cuda driver api has a versioned c-style abi, which guarantees that applications that were running against an older driver (for example cuda 3.2) will still run and function correctly against a modern driver (for example one shipped with cuda 11.0). this means that even though an application source might need to be changed if it has to be recompiled against a newer cuda toolkit in order to use the newer features, replacing the driver components installed in a system with a newer version will always support existing applications and its functions. the cuda driver api thus is binary-compatible (the os loader can pick up a newer version and the application continues to work) but not source-compatible (rebuilding your application against a newer sdk might require source changes). before we proceed further on this topic, its important for developers to understand the concept of minimum driver version and how that may affect them. each version of the cuda toolkit (and runtime) requires a minimum version of the nvidia driver. applications compiled against a cuda toolkit version will only run on systems with the specified minimum driver version for that toolkit version. prior to cuda 11.0, the minimum driver version for a toolkit was the same as the driver shipped with that version of the cuda toolkit. so, when an application is built with cuda 11.0, it can only run on a system with an r450 or later driver. if such an application is run on a system with the r418 driver installed, cuda initialization will return an error as can be seen in the example below. in this example, the devicequery sample is compiled with cuda 11.1 and is run on a system with r418. in this scenario, cuda initialization returns an error due to the minimum driver requirement. refer to thecuda toolkit release notesfor details for the minimum driver version and the version of the driver shipped with the toolkit.",0
15.3.1.cuda binary (cubin) compatibility,#cuda-binary-cubin-compatibility,"15.3.1.cuda binary (cubin) compatibility a slightly related but important topic is one of application binary compatibility across gpu architectures in cuda. cuda c++ provides a simple path for users familiar with the c++ programming language to easily write programs for execution by the device. kernels can be written using the cuda instruction set architecture, called ptx, which is described in the ptx reference manual. it is however usually more effective to use a high-level programming language such as c++. in both cases, kernels must be compiled into binary code by nvcc (called cubins) to execute on the device. the cubins are architecture-specific. binary compatibility for cubins is guaranteed from one compute capability minor revision to the next one, but not from one compute capability minor revision to the previous one or across major compute capability revisions. in other words, a cubin object generated for compute capabilityx.ywill only execute on devices of compute capabilityx.zwherezy. to execute code on devices of specific compute capability, an application must load binary or ptx code that is compatible with this compute capability. for portability, that is, to be able to execute code on future gpu architectures with higher compute capability (for which no binary code can be generated yet), an application must load ptx code that will be just-in-time compiled by the nvidia driver for these future devices. more information on cubins, ptx and application compatibility can be found in thecuda c++ programming guide.",0
15.4.cuda compatibility across minor releases,#cuda-compatibility-across-minor-releases,"15.4.cuda compatibility across minor releases by leveraging the semantic versioning, starting with cuda 11, components in the cuda toolkit will remain binary compatible across the minor versions of the toolkit. in order to maintain binary compatibility across minor versions, the cuda runtime no longer bumps up the minimum driver version required for every minor release - this only happens when a major release is shipped. one of the main reasons a new toolchain requires a new minimum driver is to handle the jit compilation of ptx code and the jit linking of binary code. in this section, we will review the usage patterns that may require new user workflows when taking advantage of the compatibility features of the cuda platform.",0
15.4.1.existing cuda applications within minor versions of cuda,#existing-cuda-applications-within-minor-versions-of-cuda,"15.4.1.existing cuda applications within minor versions of cuda when our cuda 11.1 application (i.e. cudart 11.1 is statically linked) is run on the system, we see that it runs successfully even when the driver reports a 11.0 version - that is, without requiring the driver or other toolkit components to be updated on the system. by using new cuda versions, users can benefit from new cuda programming model apis, compiler optimizations and math library features. the following sections discuss some caveats and considerations.",0
16.preparing for deployment,#preparing-for-deployment,16.preparing for deployment,9
16.1.testing for cuda availability,#testing-for-cuda-availability,"16.1.testing for cuda availability when deploying a cuda application, it is often desirable to ensure that the application will continue to function properly even if the target machine does not have a cuda-capable gpu and/or a sufficient version of the nvidia driver installed. (developers targeting a single machine with known configuration may choose to skip this section.) detecting a cuda-capable gpu when an application will be deployed to target machines of arbitrary/unknown configuration, the application should explicitly test for the existence of a cuda-capable gpu in order to take appropriate action when no such device is available. thecudagetdevicecount()function can be used to query for the number of available devices. like all cuda runtime api functions, this function will fail gracefully and returncudaerrornodeviceto the application if there is no cuda-capable gpu orcudaerrorinsufficientdriverif there is not an appropriate version of the nvidia driver installed. ifcudagetdevicecount()reports an error, the application should fall back to an alternative code path. a system with multiple gpus may contain gpus of different hardware versions and capabilities. when using multiple gpus from the same application, it is recommended to use gpus of the same type, rather than mixing hardware generations. thecudachoosedevice()function can be used to select the device that most closely matches a desired set of features. detecting hardware and software configuration when an application depends on the availability of certain hardware or software capabilities to enable certain functionality, the cuda api can be queried for details about the configuration of the available device and for the installed software versions. thecudagetdeviceproperties()function reports various features of the available devices, including thecuda compute capabilityof the device (see also the compute capabilities section of the cuda c++ programming guide). seeversion managementfor details on how to query the available cuda software api versions.",0
16.2.error handling,#error-handling,"16.2.error handling all cuda runtime api calls return an error code of typecudaerror_t; the return value will be equal tocudasuccessif no errors have occurred. (the exceptions to this are kernel launches, which return void, andcudageterrorstring(), which returns a character string describing thecudaerror_tcode that was passed into it.) the cuda toolkit libraries (cublas,cufft, etc.) likewise return their own sets of error codes. since some cuda api calls and all kernel launches are asynchronous with respect to the host code, errors may be reported to the host asynchronously as well; often this occurs the next time the host and device synchronize with each other, such as during a call tocudamemcpy()or tocudadevicesynchronize(). always check the error return values on all cuda api functions, even for functions that are not expected to fail, as this will allow the application to detect and recover from errors as soon as possible should they occur. to check for errors occurring during kernel launches using the<<<...>>>syntax, which does not return any error code, the return code ofcudagetlasterror()should be checked immediately after the kernel launch. applications that do not check for cuda api errors could at times run to completion without having noticed that the data calculated by the gpu is incomplete, invalid, or uninitialized.",0
16.3.building for maximum compatibility,#building-for-maximum-compatibility,"16.3.building for maximum compatibility each generation of cuda-capable device has an associatedcompute capabilityversion that indicates the feature set supported by the device (seecuda compute capability). one or morecompute capabilityversions can be specified to the nvcc compiler while building a file; compiling for the native compute capability for the target gpu(s) of the application is important to ensure that application kernels achieve the best possible performance and are able to use the features that are available on a given generation of gpu. when an application is built for multiple compute capabilities simultaneously (using several instances of the-gencodeflag to nvcc), the binaries for the specified compute capabilities are combined into the executable, and the cuda driver selects the most appropriate binary at runtime according to the compute capability of the present device. if an appropriate native binary (cubin) is not available, but the intermediateptxcode (which targets an abstract virtual instruction set and is used for forward-compatibility) is available, then the kernel will be compiledjust in time(jit) (seecompiler jit cache management tools) from the ptx to the native cubin for the device. if the ptx is also not available, then the kernel launch will fail. windows mac/linux alternatively, thenvcccommand-line option-arch=sm_xxcan be used as a shorthand equivalent to the following more explicit-gencode=command-line options described above: however, while the-arch=sm_xxcommand-line option does result in inclusion of a ptx back-end target by default (due to thecode=compute_xxtarget it implies), it can only specify a single targetcubinarchitecture at a time, and it is not possible to use multiple-arch=options on the samenvcccommand line, which is why the examples above use-gencode=explicitly.",0
16.4.distributing the cuda runtime and libraries,#distributing-the-cuda-runtime-and-libraries,"16.4.distributing the cuda runtime and libraries cuda applications are built against the cuda runtime library, which handles device, memory, and kernel management. unlike the cuda driver, the cuda runtime guarantees neither forward nor backward binary compatibility across versions. it is therefore best toredistributethe cuda runtime library with the application when using dynamic linking or else to statically link against the cuda runtime. this will ensure that the executable will be able to run even if the user does not have the same cuda toolkit installed that the application was built against. statically-linked cuda runtime the easiest option is to statically link against the cuda runtime. this is the default if usingnvccto link in cuda 5.5 and later. static linking makes the executable slightly larger, but it ensures that the correct version of runtime library functions are included in the application binary without requiring separate redistribution of the cuda runtime library. dynamically-linked cuda runtime if static linking against the cuda runtime is impractical for some reason, then a dynamically-linked version of the cuda runtime library is also available. (this was the default and only option provided in cuda versions 5.0 and earlier.) to use dynamic linking with the cuda runtime when using thenvccfrom cuda 5.5 or later to link the application, add the--cudart=sharedflag to the link command line; otherwise thestatically-linked cuda runtime libraryis used by default. after the application is dynamically linked against the cuda runtime, this version of the runtime library should bebundled withthe application. it can be copied into the same directory as the application executable or into a subdirectory of that installation path. other cuda libraries although the cuda runtime provides the option of static linking, some libraries included in the cuda toolkit are available only in dynamically-linked form. as with thedynamically-linked version of the cuda runtime library, these libraries should bebundled withthe application executable when distributing that application.",0
16.4.1.cuda toolkit library redistribution,#cuda-toolkit-library-redistribution,"16.4.1.cuda toolkit library redistribution the cuda toolkits end-user license agreement (eula) allows for redistribution of many of the cuda libraries under certain terms and conditions. this allows applications that depend on these librariesto redistribute the exact versionsof the libraries against which they were built and tested, thereby avoiding any trouble for end users who might have a different version of the cuda toolkit (or perhaps none at all) installed on their machines. please refer to the eula for details.",0
17.deployment infrastructure tools,#deployment-infrastructure-tools,17.deployment infrastructure tools,9
17.1.nvidia-smi,#nvidia-smi,"17.1.nvidia-smi the nvidia system management interface (nvidia-smi) is a command line utility that aids in the management and monitoring of nvidia gpu devices. this utility allows administrators to query gpu device state and, with the appropriate privileges, permits administrators to modify gpu device state.nvidia-smiis targeted at tesla and certain quadro gpus, though limited support is also available on other nvidia gpus.nvidia-smiships with nvidia gpu display drivers on linux, and with 64-bit windows server 2008 r2 and windows 7.nvidia-smican output queried information as xml or as human-readable plain text either to standard output or to a file. see the nvidia-smi documenation for details. please note that new versions of nvidia-smi are not guaranteed to be backward-compatible with previous versions.",4
17.1.1.queryable state,#queryable-state,17.1.1.queryable state,9
17.1.2.modifiable state,#modifiable-state,17.1.2.modifiable state,9
17.2.nvml,#nvml,"17.2.nvml the nvidia management library (nvml) is a c-based interface that provides direct access to the queries and commands exposed vianvidia-smiintended as a platform for building 3rd-party system management applications. the nvml api is shipped with the cuda toolkit (since version 8.0) and is also available standalone on the nvidia developer website as part of the gpu deployment kit through a single header file accompanied by pdf documentation, stub libraries, and sample applications; seehttps://developer.nvidia.com/gpu-deployment-kit. each new version of nvml is backward-compatible. an additional set of perl and python bindings are provided for the nvml api. these bindings expose the same features as the c-based interface and also provide backwards compatibility. the perl bindings are provided via cpan and the python bindings via pypi. all of these products (nvidia-smi, nvml, and the nvml language bindings) are updated with each new cuda release and provide roughly the same functionality. seehttps://developer.nvidia.com/nvidia-management-library-nvmlfor additional information.",0
17.3.cluster management tools,#cluster-management-tools,"17.3.cluster management tools managing your gpu cluster will help achieve maximum gpu utilization and help you and your users extract the best possible performance. many of the industrys most popular cluster management tools support cuda gpus via nvml. for a listing of some of these tools, seehttps://developer.nvidia.com/cluster-management.",0
17.4.compiler jit cache management tools,#compiler-jit-cache-management-tools,"17.4.compiler jit cache management tools any ptx device code loaded by an application at runtime is compiled further to binary code by the device driver. this is calledjust-in-time compilation(jit). just-in-time compilation increases application load time but allows applications to benefit from latest compiler improvements. it is also the only way for applications to run on devices that did not exist at the time the application was compiled. when jit compilation of ptx device code is used, the nvidia driver caches the resulting binary code on disk. some aspects of this behavior such as cache location and maximum cache size can be controlled via the use of environment variables; see just in time compilation of the cuda c++ programming guide.",5
17.5.cuda_visible_devices,#cuda-visible-devices,"17.5.cuda_visible_devices it is possible to rearrange the collection of installed cuda devices that will be visible to and enumerated by a cuda application prior to the start of that application by way of thecuda_visible_devicesenvironment variable. devices to be made visible to the application should be included as a comma-separated list in terms of the system-wide list of enumerable devices. for example, to use only devices 0 and 2 from the system-wide list of devices, setcuda_visible_devices=0,2before launching the application. the application will then enumerate these devices as device 0 and device 1, respectively.",0
18.recommendations and best practices,#id3,18.recommendations and best practices this chapter contains a summary of the recommendations for optimization that are explained in this document.,9
18.1.overall performance optimization strategies,#overall-performance-optimization-strategies,"18.1.overall performance optimization strategies performance optimization revolves around three basic strategies: maximizing parallel execution starts with structuring the algorithm in a way that exposes as much parallelism as possible. once the parallelism of the algorithm has been exposed, it needs to be mapped to the hardware as efficiently as possible. this is done by carefully choosing the execution configuration of each kernel launch. the application should also maximize parallel execution at a higher level by explicitly exposing concurrent execution on the device through streams, as well as maximizing concurrent execution between the host and the device. optimizing memory usage starts with minimizing data transfers between the host and the device because those transfers have much lower bandwidth than internal device data transfers. kernel access to global memory also should be minimized by maximizing the use of shared memory on the device. sometimes, the best optimization might even be to avoid any data transfer in the first place by simply recomputing the data whenever it is needed. the effective bandwidth can vary by an order of magnitude depending on the access pattern for each type of memory. the next step in optimizing memory usage is therefore to organize memory accesses according to the optimal memory access patterns. this optimization is especially important for global memory accesses, because latency of access costs hundreds of clock cycles. shared memory accesses, in counterpoint, are usually worth optimizing only when there exists a high degree of bank conflicts. as for optimizing instruction usage, the use of arithmetic instructions that have low throughput should be avoided. this suggests trading precision for speed when it does not affect the end result, such as using intrinsics instead of regular functions or single precision instead of double precision. finally, particular attention must be paid to control flow instructions due to the simt (single instruction multiple thread) nature of the device.",5
19.nvcc compiler switches,#nvcc-compiler-switches,19.nvcc compiler switches,6
19.1.nvcc,#nvcc,"19.1.nvcc the nvidianvcccompiler driver converts.cufiles into c++ for the host system and cuda assembly or binary instructions for the device. it supports a number of command-line parameters, of which the following are especially useful for optimization and related best practices:",0
20.notices,#notices,20.notices,9
20.1.notice,#notice,"20.1.notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation (nvidia) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material (defined below), code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer (terms of sale). nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion and/or use of nvidia products in such equipment or applications and therefore such inclusion and/or use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions and/or requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the nvidia product in any manner that is contrary to this document or (ii) customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding third-party products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents (together and separately, materials) are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product.",4
20.2.opencl,#opencl,20.2.opencl opencl is a trademark of apple inc. used under license to the khronos group inc.,4
20.3.trademarks,#trademarks,20.3.trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.,4
contents,#contents,contents white paper covering the most common issues related to nvidia gpus.,4
1.hopper architecture compatibility,#hopper-architecture-compatibility,1.hopper architecture compatibility,9
1.1.about this document,#about-this-document,"1.1.about this document this application note, hopper architecture compatibility guide for cuda applications, is intended to help developers ensure that their nvidiacudaapplications will run on the nvidiahopper architecture based gpus. this document provides guidance to developers who are familiar with programming in cuda c++ and want to make sure that their software applications are compatible with hopper architecture.",0
1.2.application compatibility on hopper architecture,#application-compatibility-on-hopper-architecture,"1.2.application compatibility on hopper architecture a cuda application binary (with one or more gpu kernels) can contain the compiled gpu code in two forms, binary cubin objects and forward-compatible ptx assembly for each kernel. both cubin and ptx are generated for a certain target compute capability. a cubin generated for a certain compute capability is supported to run on any gpu with the same major revision and same or higher minor revision of compute capability. for example, a cubin generated for compute capability 8.0 is supported to run on a gpu with compute capability 8.6, however a cubin generated for compute capability 8.6 isnotsupported to run on a gpu with compute capability 8.0, and a cubin generated with compute capability 8.x isnotsupported to run on a gpu with compute capability 9.0. kernel can also be compiled to a ptx form. at the application load time, ptx is compiled to cubin and the cubin is used for kernel execution. unlike cubin, ptx is forward-compatible. meaning ptx is supported to run on any gpu with compute capability higher than the compute capability assumed for generation of that ptx. for example, ptx code generated for compute capability 8.x is supported to run on compute capability 8.x or any higher revision (major or minor), including compute capability 9.0. therefore although it is optional,it is recommended that all applications should include ptx of the kernels to ensure forward-compatibility.to read more about cubin and ptx compatibilities seecompilation with nvccfrom thecuda c++ programming guide. when a cuda application launches a kernel on a gpu, the cuda runtime determines the compute capability of the gpu in the system and uses this information to find the best matching cubin or ptx version of the kernel. if a cubin compatible with that gpu is present in the binary, the cubin is used as-is for execution. otherwise, the cuda runtime first generates compatible cubin by jit-compiling1the ptx and then the cubin is used for the execution. if neither compatible cubin nor ptx is available, kernel launch results in a failure. application binaries that include ptx version of kernels, should work as-is on the hopper gpus. in such cases, rebuilding the application is not required. however application binaries which do not include ptx (only include cubins), need to be rebuilt to run on the hopper gpus. to know more about building compatible applications readbuilding applications with hopper architecture support application binaries that include ptx version of kernels with architecture conditional features usingsm_90aorcompute_90ain order to take full advantage of hopper gpu architecture, are not forward or backward compatible.",5
1.3.verifying hopper compatibility for existing applications,#verifying-hopper-compatibility-for-existing-applications,1.3.verifying hopper compatibility for existing applications the first step towards making a cuda application compatible with hopper architecture is to check if the application binary already contains compatible gpu code (at least the ptx). the following sections explain how to accomplish this for an already built cuda application.,0
1.3.1.applications built using cuda toolkit 11.7 or earlier,#applications-built-using-cuda-toolkit-11-7-or-earlier,"1.3.1.applications built using cuda toolkit 11.7 or earlier cuda applications built using cuda toolkit versions 2.1 through 11.7 are compatible with hopper gpus as long as they are built to include ptx versions of their kernels. this can be tested by forcing the ptx to jit-compile at application load time with following the steps: withcuda_force_ptx_jit=1, gpu binary code embedded in an application binary is ignored. instead ptx code for each kernel is jit-compiled to produce gpu binary code. an application fails to execute if it does not include ptx. this means the application is not hopper architecture compatible and needs to be rebuilt for compatibility. on the other hand, if the application works properly with this environment variable set, then the application is hopper compatible.",0
1.3.2.applications built using cuda toolkit 11.8,#applications-built-using-cuda-toolkit-11-8,1.3.2.applications built using cuda toolkit 11.8 cuda applications built using cuda toolkit 11.8 are compatible with hopper architecture as long as they are built to include kernels in native cubin (compute capability 9.0) or ptx form or both.,0
1.4.building applications with hopper architecture support,#building-applications-with-hopper-architecture-support,"1.4.building applications with hopper architecture support depending on the version of the cuda toolkit used for building the application, it can be built to include ptx and/or native cubin for the hopper architecture. although it is enough to just include ptx, including native cubin also has the following advantages:",0
1.4.1.building applications using cuda toolkit 11.7 or earlier,#building-applications-using-cuda-toolkit-11-7-or-earlier,"1.4.1.building applications using cuda toolkit 11.7 or earlier thenvcccompiler included with version 11.7 or earlier (11.0-11.7) of the cuda toolkit can generate cubins native to the nvidia ampere gpu architectures (compute capability 8.x). when using cuda toolkit 11.7 or earlier, to ensure thatnvccwill generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate-gencode=parameters on thenvcccommand line as shown in the examples below. windows mac/linux alternatively, the simplifiednvcccommand-line option-arch=sm_xxcan be used. it is a shorthand equivalent to the following more explicit-gencode=command-line options used above.-arch=sm_xxexpands to the following: however, while the-arch=sm_xxcommand-line option does result in inclusion of a ptx back-end target binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple-arch=options on the samenvcccommand line, which is why the examples above use-gencode=explicitly. for cuda toolkits prior to 11.0, one or more of the-gencodeoptions need to be removed according to the architectures supported by the specific toolkit version (for example, cuda toolkit 10.x supports architectures up to sm_72 and sm_75). the final-gencodeto generate ptx also needs to be updated. for further information and examples see the documentation for the specific cuda toolkit version.",0
1.4.2.building applications using cuda toolkit 11.8,#building-applications-using-cuda-toolkit-11-8,"1.4.2.building applications using cuda toolkit 11.8 with versions 11.8 of the cuda toolkit,nvcccan generate cubin native to the hopper architecture (compute capability 9.0). when using cuda toolkit 11.8, to ensure thatnvccwill generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate-gencode=parameters on thenvcccommand line as shown in the examples below. windows mac/linux",0
1.4.3.independent thread scheduling compatibility,#independent-thread-scheduling-compatibility,"1.4.3.independent thread scheduling compatibility nvidia gpus since volta architecture have independent thread scheduling among threads in a warp. if the developer made assumptions about warp-synchronicity3, this feature can alter the set of threads participating in the executed code compared to previous architectures. please seecompute capability 7.xin thecuda c++ programming guidefor details and corrective actions. to aid migration to the hopper architecture, developers can opt-in to the pascal scheduling model with the following combination of compiler options.",5
4.data structures,https://docs.nvidia.com/cuda/debugger-api/annotated.html#annotated,4.data structures,9
4.10.cudbgevent::cases_st::kernelfinished_st struct reference,https://docs.nvidia.com/cuda/debugger-api/structCUDBGEvent_1_1cases__st_1_1kernelFinished__st.html#structCUDBGEvent_1_1cases__st_1_1kernelFinished__st,4.10.cudbgevent::cases_st::kernelfinished_st struct reference,8
4.9.cudbgevent::cases_st::internalerror_st struct reference,https://docs.nvidia.com/cuda/debugger-api/structCUDBGEvent_1_1cases__st_1_1internalError__st.html#structCUDBGEvent_1_1cases__st_1_1internalError__st,4.9.cudbgevent::cases_st::internalerror_st struct reference,8
4.11.cudbgevent::cases_st::kernelready_st struct reference,https://docs.nvidia.com/cuda/debugger-api/structCUDBGEvent_1_1cases__st_1_1kernelReady__st.html#structCUDBGEvent_1_1cases__st_1_1kernelReady__st,4.11.cudbgevent::cases_st::kernelready_st struct reference,8
4.6.cudbgevent::cases_st::contextpop_st struct reference,https://docs.nvidia.com/cuda/debugger-api/structCUDBGEvent_1_1cases__st_1_1contextPop__st.html#structCUDBGEvent_1_1cases__st_1_1contextPop__st,4.6.cudbgevent::cases_st::contextpop_st struct reference,8
4.14.cudbggridinfo struct reference,https://docs.nvidia.com/cuda/debugger-api/structCUDBGGridInfo.html#structCUDBGGridInfo,4.14.cudbggridinfo struct reference,8
[grid properties],group__GRID.html,[grid properties],9
7.16.cuda_ext_sem_signal_node_params_v2 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__EXT__SEM__SIGNAL__NODE__PARAMS__v2.html#structCUDA__EXT__SEM__SIGNAL__NODE__PARAMS__v2,7.16.cuda_ext_sem_signal_node_params_v2 struct reference,8
[data types used by cuda driver],group__CUDA__TYPES.html,[data types used by cuda driver],0
7.data structures,https://docs.nvidia.com/cuda/cuda-driver-api/annotated.html#annotated,7.data structures,9
7.23.cuda_external_semaphore_signal_params_v1 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__EXTERNAL__SEMAPHORE__SIGNAL__PARAMS__v1.html#structCUDA__EXTERNAL__SEMAPHORE__SIGNAL__PARAMS__v1,7.23.cuda_external_semaphore_signal_params_v1 struct reference,8
3.stream synchronization behavior,https://docs.nvidia.com/cuda/cuda-driver-api/stream-sync-behavior.html#stream-sync-behavior,3.stream synchronization behavior,5
default stream,https://docs.nvidia.com/cuda/cuda-driver-api/stream-sync-behavior.html#stream-sync-behavior,"default stream the default stream, used when0is passed as acudastream_tor by apis that operate on a stream implicitly, can be configured to have eitherlegacyorper-threadsynchronization behavior as described below. the behavior can be controlled per compilation unit with the--default-streamnvcc option. alternatively, per-thread behavior can be enabled by defining thecuda_api_per_thread_default_streammacro before including any cuda headers. either way, thecuda_api_per_thread_default_streammacro will be defined in compilation units using per-thread synchronization behavior.",5
legacy default stream,https://docs.nvidia.com/cuda/cuda-driver-api/stream-sync-behavior.html#stream-sync-behavior,"legacy default stream the legacy default stream is an implicit stream which synchronizes with all other streams in the samecucontextexcept for non-blocking streams, described below.
                           (for applications using the runtime apis only, there will be one context per device.)
                           when an action is taken in the legacy stream such as a kernel launch orcudastreamwaitevent(), the legacy stream first waits on all blocking streams, the action is queued in the legacy stream, and then all blocking
                           streams wait on the legacy stream. for example, the following code launches a kernelk_1in streams, thenk_2in the legacy stream, thenk_3in streams: the resulting behavior is thatk_2will block onk_1andk_3will block onk_2. non-blocking streams which do not synchronize with the legacy stream can be created using thecudastreamnonblockingflag with the stream creation apis. the legacy default stream can be used explicitly with thecustream(cudastream_t) handlecu_stream_legacy(cudastreamlegacy).",5
per-thread default stream,https://docs.nvidia.com/cuda/cuda-driver-api/stream-sync-behavior.html#stream-sync-behavior,"per-thread default stream the per-thread default stream is an implicit stream local to both the thread and thecucontext, and which does not synchronize with other streams
                           (just like explicitly created streams). the per-thread default stream is not a
                           non-blocking stream and will synchronize with the legacy default stream if both are
                           used in a program. the per-thread default stream can be used explicitly with thecustream(cudastream_t) handlecu_stream_per_thread(cudastreamperthread).",5
6.23.execution control [deprecated],https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EXEC__DEPRECATED.html#group__CUDA__EXEC__DEPRECATED,6.23.execution control [deprecated],6
functions,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EXEC__DEPRECATED.html#group__CUDA__EXEC__DEPRECATED,functions,9
6.30.tensor map object managment,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html#group__CUDA__TENSOR__MEMORY,6.30.tensor map object managment,9
6.18.stream management,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__STREAM.html#group__CUDA__STREAM,6.18.stream management,9
4.1.cudbgapi_st struct reference,https://docs.nvidia.com/cuda/debugger-api/structCUDBGAPI__st.html,4.1.cudbgapi_st struct reference,8
1.introduction,#introduction,"1.introduction this document describes ptx, a low-levelparallel thread executionvirtual machine and instruction
set architecture (isa). ptx exposes the gpu as a data-parallel computingdevice.",7
1.1.scalable data-parallel computing using gpus,#scalable-data-parallel-computing-using-gpus,"1.1.scalable data-parallel computing using gpus driven by the insatiable market demand for real-time, high-definition 3d graphics, the programmable
gpu has evolved into a highly parallel, multithreaded, many-core processor with tremendous
computational horsepower and very high memory bandwidth. the gpu is especially well-suited to
address problems that can be expressed as data-parallel computations - the same program is executed
on many data elements in parallel - with high arithmetic intensity - the ratio of arithmetic
operations to memory operations. because the same program is executed for each data element, there
is a lower requirement for sophisticated flow control; and because it is executed on many data
elements and has high arithmetic intensity, the memory access latency can be hidden with
calculations instead of big data caches. data-parallel processing maps data elements to parallel processing threads. many applications that
process large data sets can use a data-parallel programming model to speed up the computations. in
3d rendering large sets of pixels and vertices are mapped to parallel threads. similarly, image and
media processing applications such as post-processing of rendered images, video encoding and
decoding, image scaling, stereo vision, and pattern recognition can map image blocks and pixels to
parallel processing threads. in fact, many algorithms outside the field of image rendering and
processing are accelerated by data-parallel processing, from general signal processing or physics
simulation to computational finance or computational biology. ptxdefines a virtual machine and isa for general purpose parallel thread execution. ptx programs
are translated at install time to the target hardware instruction set. the ptx-to-gpu translator
and driver enable nvidia gpus to be used as programmable parallel computers.",5
1.2.goals of ptx,#goals-of-ptx,"1.2.goals of ptx ptxprovides a stable programming model and instruction set for general purpose parallel
programming. it is designed to be efficient on nvidia gpus supporting the computation features
defined by the nvidia tesla architecture. high level language compilers for languages such as cuda
and c/c++ generate ptx instructions, which are optimized for and translated to native
target-architecture instructions. the goals for ptx include the following:",7
1.3.ptx isa version 8.5,#ptx-isa-version-8-5,1.3.ptx isa version 8.5 ptx isa version 8.5 introduces the following new features:,7
1.4.document structure,#document-structure,1.4.document structure the information in this document is organized into the following chapters: references,9
2.programming model,#programming-model,2.programming model,9
2.1.a highly multithreaded coprocessor,#highly-multithreaded-coprocessor,"2.1.a highly multithreaded coprocessor the gpu is a compute device capable of executing a very large number of threads in parallel. it
operates as a coprocessor to the main cpu, or host: in other words, data-parallel, compute-intensive
portions of applications running on the host are off-loaded onto the device. more precisely, a portion of an application that is executed many times, but independently on
different data, can be isolated into a kernel function that is executed on the gpu as many different
threads. to that effect, such a function is compiled to the ptx instruction set and the resulting
kernel is translated at install time to the target gpu instruction set.",5
2.2.thread hierarchy,#thread-hierarchy,"2.2.thread hierarchy the batch of threads that executes a kernel is organized as a grid. a grid consists of either
cooperative thread arrays or clusters of cooperative thread arrays as described in this section and
illustrated infigure 1andfigure 2.cooperative thread arrays (ctas)implement cuda
thread blocks and clusters implement cuda thread block clusters.",5
2.2.1.cooperative thread arrays,#cooperative-thread-arrays,"2.2.1.cooperative thread arrays theparallel thread execution (ptx)programming model is explicitly parallel: a ptx program
specifies the execution of a given thread of a parallel thread array. acooperative thread array,
or cta, is an array of threads that execute a kernel concurrently or in parallel. threads within a cta can communicate with each other. to coordinate the communication of the threads
within the cta, one can specify synchronization points where threads wait until all threads in the
cta have arrived. each thread has a unique thread identifier within the cta. programs use a data parallel
decomposition to partition inputs, work, and results across the threads of the cta. each cta thread
uses its thread identifier to determine its assigned role, assign specific input and output
positions, compute addresses, and select work to perform. the thread identifier is a three-element
vectortid, (with elementstid.x,tid.y, andtid.z) that specifies the threads
position within a 1d, 2d, or 3d cta. each thread identifier component ranges from zero up to the
number of thread ids in that cta dimension. each cta has a 1d, 2d, or 3d shape specified by a three-element vectorntid(with elementsntid.x,ntid.y, andntid.z). the vectorntidspecifies the number of threads in each
cta dimension. threads within a cta execute in simt (single-instruction, multiple-thread) fashion in groups calledwarps. awarpis a maximal subset of threads from a single cta, such that the threads execute
the same instructions at the same time. threads within a warp are sequentially numbered. the warp
size is a machine-dependent constant. typically, a warp has 32 threads. some applications may be
able to maximize performance with knowledge of the warp size, so ptx includes a run-time immediate
constant,warp_sz, which may be used in any instruction where an immediate operand is allowed.",5
2.2.2.cluster of cooperative thread arrays,#cluster-of-cooperative-thread-arrays,"2.2.2.cluster of cooperative thread arrays cluster is a group of ctas that run concurrently or in parallel and can synchronize and communicate
with each other via shared memory. the executing cta has to make sure that the shared memory of the
peer cta exists before communicating with it via shared memory and the peer cta hasnt exited before
completing the shared memory operation. threads within the different ctas in a cluster can synchronize and communicate with each other via
shared memory. cluster-wide barriers can be used to synchronize all the threads within the
cluster. each cta in a cluster has a unique cta identifier within its cluster
(cluster_ctaid). each cluster of ctas has 1d, 2d or 3d shape specified by the parametercluster_nctaid. each cta in the cluster also has a unique cta identifier (cluster_ctarank)
across all dimensions. the total number of ctas across all the dimensions in the cluster is
specified bycluster_nctarank. threads may read and use these values through predefined, read-only
special registers%cluster_ctaid,%cluster_nctaid,%cluster_ctarank,%cluster_nctarank. cluster level is applicable only on target architecturesm_90or higher. specifying cluster
level during launch time is optional. if the user specifies the cluster dimensions at launch time
then it will be treated as explicit cluster launch, otherwise it will be treated as implicit cluster
launch with default dimension 1x1x1. ptx provides read-only special register%is_explicit_clusterto differentiate between explicit and implicit cluster launch.",5
2.2.3.grid of clusters,#grid-of-clusters,"2.2.3.grid of clusters there is a maximum number of threads that a cta can contain and a maximum number of ctas that a
cluster can contain. however, clusters with ctas that execute the same kernel can be batched
together into a grid of clusters, so that the total number of threads that can be launched in a
single kernel invocation is very large. this comes at the expense of reduced thread communication
and synchronization, because threads in different clusters cannot communicate and synchronize with
each other. each cluster has a unique cluster identifier (clusterid) within a grid of clusters. each grid of
clusters has a 1d, 2d , or 3d shape specified by the parameternclusterid. each grid also has a
unique temporal grid identifier (gridid). threads may read and use these values through
predefined, read-only special registers%tid,%ntid,%clusterid,%nclusterid, and%gridid. each cta has a unique identifier (ctaid) within a grid. each grid of ctas has 1d, 2d, or 3d shape
specified by the parameternctaid. thread may use and read these values through predefined,
read-only special registers%ctaidand%nctaid. each kernel is executed as a batch of threads organized as a grid of clusters consisting of ctas
where cluster is optional level and is applicable only for target architecturessm_90and
higher.figure 1shows a grid consisting of ctas andfigure 2shows a grid consisting of clusters. grids may be launched with dependencies between one another - a grid may be a dependent grid and/or
a prerequisite grid. to understand how grid dependencies may be defined, refer to the section oncuda graphsin thecuda programming guide.",5
2.3.memory hierarchy,#memory-hierarchy,"2.3.memory hierarchy ptx threads may access data from multiple state spaces during their execution as illustrated byfigure 3where cluster level is introduced from
target architecturesm_90onwards. each thread has a private local memory. each thread block
(cta) has a shared memory visible to all threads of the block and to all active blocks in the
cluster and with the same lifetime as the block. finally, all threads have access to the same global
memory. there are additional state spaces accessible by all threads: the constant, param, texture, and
surface state spaces.  constant and texture memory are read-only; surface memory is readable and
writable. the global, constant, param, texture, and surface state spaces are optimized for different
memory usages. for example, texture memory offers different addressing modes as well as data
filtering for specific data formats. note that texture and surface memory is cached, and within the
same kernel call, the cache is not kept coherent with respect to global memory writes and surface
memory writes, so any texture fetch or surface read to an address that has been written to via a
global or a surface write in the same kernel call returns undefined data. in other words, a thread
can safely read some texture or surface memory location only if this memory location has been
updated by a previous kernel call or memory copy, but not if it has been previously updated by the
same thread or another thread from the same kernel call. the global, constant, and texture state spaces are persistent across kernel launches by the same
application. both the host and the device maintain their own local memory, referred to ashost memoryanddevice memory, respectively. the device memory may be mapped and read or written by the host, or,
for more efficient transfer, copied from the host memory through optimized api calls that utilize
the devices high-performancedirect memory access (dma)engine.",5
3.ptx machine model,#ptx-machine-model,3.ptx machine model,7
3.1.a set of simt multiprocessors,#set-of-simt-multiprocessors,"3.1.a set of simt multiprocessors the nvidia gpu architecture is built around a scalable array of multithreadedstreaming
multiprocessors (sms). when a host program invokes a kernel grid, the blocks of the grid are
enumerated and distributed to multiprocessors with available execution capacity. the threads of a
thread block execute concurrently on one multiprocessor. as thread blocks terminate, new blocks are
launched on the vacated multiprocessors. a multiprocessor consists of multiplescalar processor (sp)cores, a multithreaded instruction
unit, and on-chip shared memory. the multiprocessor creates, manages, and executes concurrent
threads in hardware with zero scheduling overhead. it implements a single-instruction barrier
synchronization. fast barrier synchronization together with lightweight thread creation and
zero-overhead thread scheduling efficiently support very fine-grained parallelism, allowing, for
example, a low granularity decomposition of problems by assigning one thread to each data element
(such as a pixel in an image, a voxel in a volume, a cell in a grid-based computation). to manage hundreds of threads running several different programs, the multiprocessor employs an
architecture we callsimt (single-instruction, multiple-thread). the multiprocessor maps each
thread to one scalar processor core, and each scalar thread executes independently with its own
instruction address and register state. the multiprocessor simt unit creates, manages, schedules,
and executes threads in groups of parallel threads calledwarps. (this term originates from
weaving, the first parallel thread technology.) individual threads composing a simt warp start
together at the same program address but are otherwise free to branch and execute independently. when a multiprocessor is given one or more thread blocks to execute, it splits them into warps that
get scheduled by the simt unit. the way a block is split into warps is always the same; each warp
contains threads of consecutive, increasing thread ids with the first warp containing thread 0. at every instruction issue time, the simt unit selects a warp that is ready to execute and issues
the next instruction to the active threads of the warp. a warp executes one common instruction at a
time, so full efficiency is realized when all threads of a warp agree on their execution path. if
threads of a warp diverge via a data-dependent conditional branch, the warp serially executes each
branch path taken, disabling threads that are not on that path, and when all paths complete, the
threads converge back to the same execution path. branch divergence occurs only within a warp;
different warps execute independently regardless of whether they are executing common or disjointed
code paths. simt architecture is akin to simd (single instruction, multiple data) vector organizations in that a
single instruction controls multiple processing elements. a key difference is that simd vector
organizations expose the simd width to the software, whereas simt instructions specify the execution
and branching behavior of a single thread. in contrast with simd vector machines, simt enables
programmers to write thread-level parallel code for independent, scalar threads, as well as
data-parallel code for coordinated threads. for the purposes of correctness, the programmer can
essentially ignore the simt behavior; however, substantial performance improvements can be realized
by taking care that the code seldom requires threads in a warp to diverge. in practice, this is
analogous to the role of cache lines in traditional code: cache line size can be safely ignored when
designing for correctness but must be considered in the code structure when designing for peak
performance. vector architectures, on the other hand, require the software to coalesce loads into
vectors and manage divergence manually. how many blocks a multiprocessor can process at once depends on how many registers per thread and
how much shared memory per block are required for a given kernel since the multiprocessors
registers and shared memory are split among all the threads of the batch of blocks. if there are not
enough registers or shared memory available per multiprocessor to process at least one block, the
kernel will fail to launch. a set of simt multiprocessors with on-chip shared memory.",5
3.2.independent thread scheduling,#independent-thread-scheduling,"3.2.independent thread scheduling on architectures prior to volta, warps used a single program counter shared amongst all 32 threads
in the warp together with an active mask specifying the active threads of the warp. as a result,
threads from the same warp in divergent regions or different states of execution cannot signal each
other or exchange data, and algorithms requiring fine-grained sharing of data guarded by locks or
mutexes can easily lead to deadlock, depending on which warp the contending threads come from. starting with the volta architecture,independent thread schedulingallows full concurrency
between threads, regardless of warp. withindependent thread scheduling, the gpu maintains
execution state per thread, including a program counter and call stack, and can yield execution at a
per-thread granularity, either to make better use of execution resources or to allow one thread to
wait for data to be produced by another. a schedule optimizer determines how to group active threads
from the same warp together into simt units. this retains the high throughput of simt execution as
in prior nvidia gpus, but with much more flexibility: threads can now diverge and reconverge at
sub-warp granularity. independent thread schedulingcan lead to a rather different set of threads participating in the
executed code than intended if the developer made assumptions about warp-synchronicity of previous
hardware architectures. in particular, any warp-synchronous code (such as synchronization-free,
intra-warp reductions) should be revisited to ensure compatibility with volta and beyond. see the
section on compute capability 7.x in thecuda programming guidefor further details.",5
3.3.on-chip shared memory,#on-chip-shared-memory,"3.3.on-chip shared memory as illustrated byfigure 4, each multiprocessor has
on-chip memory of the four following types: the local and global memory spaces are read-write regions of device memory.",5
4.syntax,#syntax,"4.syntax ptx programs are a collection of text source modules (files). ptx source modules have an
assembly-language style syntax with instruction operation codes and operands. pseudo-operations
specify symbol and addressing management. the ptxas optimizing backend compiler optimizes and
assembles ptx source modules to produce corresponding binary object files.",7
4.1.source format,#source-format,"4.1.source format source modules are ascii text. lines are separated by the newline character (\n). all whitespace characters are equivalent; whitespace is ignored except for its use in separating
tokens in the language. the c preprocessor cpp may be used to process ptx source modules. lines beginning withare
preprocessor directives. the following are common preprocessor directives: include,define,if,ifdef,else,endif,line,file c: a reference manualby harbison and steele provides a good description of the c preprocessor. ptx is case sensitive and uses lowercase for keywords. each ptx module must begin with a.versiondirective specifying the ptx language version,
followed by a.targetdirective specifying the target architecture assumed. seeptx module
directivesfor a more information on these directives.",7
4.2.comments,#comments,"4.2.comments comments in ptx follow c/c++ syntax, using non-nested/*and*/for comments that may span
multiple lines, and using//to begin a comment that extends up to the next newline character,
which terminates the current line. comments cannot occur within character constants, string
literals, or within other comments. comments in ptx are treated as whitespace.",7
4.3.statements,#statements,"4.3.statements a ptx statement is either a directive or an instruction. statements begin with an optional label and
end with a semicolon. examples",7
4.3.1.directive statements,#directive-statements,"4.3.1.directive statements directive keywords begin with a dot, so no conflict is possible with user-defined identifiers. the
directives in ptx are listed intable 1and
described instate spaces, types, and variablesanddirectives.",7
4.3.2.instruction statements,#instruction-statements,"4.3.2.instruction statements instructions are formed from an instruction opcode followed by a comma-separated list of zero or
more operands, and terminated with a semicolon. operands may be register variables, constant
expressions, address expressions, or label names. instructions have an optional guard predicate
which controls conditional execution. the guard predicate follows the optional label and precedes
the opcode, and is written as@p, wherepis a predicate register. the guard predicate may
be optionally negated, written as@!p. the destination operand is first, followed by source operands. instruction keywords are listed intable 2. all instruction keywords are
reserved tokens in ptx.",7
4.4.identifiers,#identifiers,"4.4.identifiers user-defined identifiers follow extended c++ rules: they either start with a letter followed by zero
or more letters, digits, underscore, or dollar characters; or they start with an underscore, dollar,
or percentage character followed by one or more letters, digits, underscore, or dollar characters: ptx does not specify a maximum length for identifiers and suggests that all implementations support
a minimum length of at least 1024 characters. many high-level languages such as c and c++ follow similar rules for identifier names, except that
the percentage sign is not allowed. ptx allows the percentage sign as the first character of an
identifier. the percentage sign can be used to avoid name conflicts, e.g., between user-defined
variable names and compiler-generated names. ptx predefines one constant and a small number of special registers that begin with the percentage
sign, listed intable 3.",7
4.5.constants,#constants,"4.5.constants ptx supports integer and floating-point constants and constant expressions. these constants may be
used in data initialization and as operands to instructions. type checking rules remain the same for
integer, floating-point, and bit-size types. for predicate-type data and instructions, integer
constants are allowed and are interpreted as in c, i.e., zero values arefalseand non-zero
values aretrue.",7
4.5.1.integer constants,#integer-constants,"4.5.1.integer constants integer constants are 64-bits in size and are either signed or unsigned, i.e., every integer
constant has type.s64or.u64. the signed/unsigned nature of an integer constant is needed
to correctly evaluate constant expressions containing operations such as division and ordered
comparisons, where the behavior of the operation depends on the operand types. when used in an
instruction or data initialization, each integer constant is converted to the appropriate size based
on the data or instruction type at its use. integer literals may be written in decimal, hexadecimal, octal, or binary notation. the syntax
follows that of c. integer literals may be followed immediately by the letteruto indicate that
the literal is unsigned. integer literals are non-negative and have a type determined by their magnitude and optional type
suffix as follows: literals are signed (.s64) unless the value cannot be fully represented in.s64or the unsigned suffix is specified, in which case the literal is unsigned (.u64). the predefined integer constantwarp_szspecifies the number of threads per warp for the target
platform; to date, all target architectures have awarp_szvalue of 32.",6
4.5.2.floating-point constants,#floating-point-constants,"4.5.2.floating-point constants floating-point constants are represented as 64-bit double-precision values, and all floating-point
constant expressions are evaluated using 64-bit double precision arithmetic. the only exception is
the 32-bit hex notation for expressing an exact single-precision floating-point value; such values
retain their exact 32-bit single-precision value and may not be used in constant expressions. each
64-bit floating-point constant is converted to the appropriate floating-point size based on the data
or instruction type at its use. floating-point literals may be written with an optional decimal point and an optional signed
exponent. unlike c and c++, there is no suffix letter to specify size; literals are always
represented in 64-bit double-precision format. ptx includes a second representation of floating-point constants for specifying the exact machine
representation using a hexadecimal constant. to specify ieee 754 double-precision floating point
values, the constant begins with0dor0dfollowed by 16 hex digits. to specify ieee 754
single-precision floating point values, the constant begins with0for0ffollowed by 8 hex
digits. example",6
4.5.3.predicate constants,#predicate-constants,"4.5.3.predicate constants in ptx, integer constants may be used as predicates. for predicate-type data initializers and
instruction operands, integer constants are interpreted as in c, i.e., zero values arefalseand
non-zero values aretrue.",7
4.5.4.constant expressions,#constant-expressions,"4.5.4.constant expressions in ptx, constant expressions are formed using operators as in c and are evaluated using rules
similar to those in c, but simplified by restricting types and sizes, removing most casts, and
defining full semantics to eliminate cases where expression evaluation in c is implementation
dependent. constant expressions are formed from constant literals, unary plus and minus, basic arithmetic
operators (addition, subtraction, multiplication, division), comparison operators, the conditional
ternary operator (?:), and parentheses. integer constant expressions also allow unary logical
negation (!), bitwise complement (~), remainder (%), shift operators (<<and>>), bit-type operators (&,|, and^), and logical operators (&&,||). constant expressions in ptx do not support casts between integer and floating-point. constant expressions are evaluated using the same operator precedence as
in c.table 4gives operator precedence and
associativity. operator precedence is highest for unary operators and decreases with each line in
the chart. operators on the same line have the same precedence and are evaluated right-to-left for
unary operators and left-to-right for binary operators.",7
4.5.5.integer constant expression evaluation,#integer-constant-expression-evaluation,"4.5.5.integer constant expression evaluation integer constant expressions are evaluated at compile time according to a set of rules that
determine the type (signed.s64versus unsigned.u64) of each sub-expression. these rules
are based on the rules in c, but theyve been simplified to apply only to 64-bit integers, and
behavior is fully defined in all cases (specifically, for remainder and shift operators).",6
4.5.6.summary of constant expression evaluation rules,#summary-of-constant-expression-evaluation-rules,4.5.6.summary of constant expression evaluation rules table 5contains a summary of the constant expression evaluation rules.,6
"5.state spaces, types, and variables",#state-spaces-types-and-variables,"5.state spaces, types, and variables while the specific resources available in a given target gpu will vary, the kinds of resources will
be common across platforms, and these resources are abstracted in ptx through state spaces and data
types.",7
5.1.state spaces,#state-spaces,"5.1.state spaces a state space is a storage area with particular characteristics. all variables reside in some state
space. the characteristics of a state space include its size, addressability, access speed, access
rights, and level of sharing between threads. the state spaces defined in ptx are a byproduct of parallel programming and graphics
programming. the list of state spaces is shown intable 6,and
properties of state spaces are shown intable 7.",7
5.1.1.register state space,#register-state-space,"5.1.1.register state space registers (.regstate space) are fast storage locations. the number of registers is limited, and
will vary from platform to platform. when the limit is exceeded, register variables will be spilled
to memory, causing changes in performance. for each architecture, there is a recommended maximum
number of registers to use (see thecuda programming guidefor details). registers may be typed (signed integer, unsigned integer, floating point, predicate) or
untyped. register size is restricted; aside from predicate registers which are 1-bit, scalar
registers have a width of 8-, 16-, 32-, 64-, or 128-bits, and vector registers have a width of
16-, 32-, 64-, or 128-bits. the most common use of 8-bit registers is withld,st, andcvtinstructions, or as elements of vector tuples. registers differ from the other state spaces in that they are not fully addressable, i.e., it is not
possible to refer to the address of a register. when compiling to use the application binary
interface (abi), register variables are restricted to function scope and may not be declared at
module scope. when compiling legacy ptx code (isa versions prior to 3.0) containing module-scoped.regvariables, the compiler silently disables use of the abi. registers may have alignment
boundaries required by multi-word loads and stores.",6
5.1.2.special register state space,#special-register-state-space,"5.1.2.special register state space the special register (.sreg) state space holds predefined, platform-specific registers, such as
grid, cluster, cta, and thread parameters, clock counters, and performance monitoring registers. all
special registers are predefined.",7
5.1.3.constant state space,#constant-state-space,"5.1.3.constant state space the constant (.const) state space is a read-only memory initialized by the host. constant memory
is accessed with ald.constinstruction. constant memory is restricted in size, currently
limited to 64 kb which can be used to hold statically-sized constant variables. there is an
additional 640 kb of constant memory, organized as ten independent 64 kb regions. the driver may
allocate and initialize constant buffers in these regions and pass pointers to the buffers as kernel
function parameters. since the ten regions are not contiguous, the driver must ensure that constant
buffers are allocated so that each buffer fits entirely within a 64 kb region and does not span a
region boundary. statically-sized constant variables have an optional variable initializer; constant variables with
no explicit initializer are initialized to zero by default. constant buffers allocated by the driver
are initialized by the host, and pointers to such buffers are passed to the kernel as
parameters. see the description of kernel parameter attributes inkernel function parameter
attributesfor more details on passing pointers
to constant buffers as kernel parameters.",6
5.1.4.global state space,#global-state-space,"5.1.4.global state space the global (.global) state space is memory that is accessible by all threads in a context. it is
the mechanism by which threads in different ctas, clusters, and grids can communicate. useld.global,st.global, andatom.globalto access global variables. global variables have an optional variable initializer; global variables with no explicit
initializer are initialized to zero by default.",5
5.1.5.local state space,#local-state-space,"5.1.5.local state space the local state space (.local) is private memory for each thread to keep its own data. it is
typically standard memory with cache. the size is limited, as it must be allocated on a per-thread
basis. useld.localandst.localto access local variables. when compiling to use theapplication binary interface (abi),.localstate-space variables
must be declared within function scope and are allocated on the stack. in implementations that do
not support a stack, all local memory variables are stored at fixed addresses, recursive function
calls are not supported, and.localvariables may be declared at module scope. when compiling
legacy ptx code (isa versions prior to 3.0) containing module-scoped.localvariables, the
compiler silently disables use of the abi.",7
5.1.6.parameter state space,#parameter-state-space,"5.1.6.parameter state space the parameter (.param) state space is used (1) to pass input arguments from the host to the
kernel, (2a) to declare formal input and return parameters for device functions called from within
kernel execution, and (2b) to declare locally-scoped byte array variables that serve as function
call arguments, typically for passing large structures by value to a function. kernel function
parameters differ from device function parameters in terms of access and sharing (read-only versus
read-write, per-kernel versus per-thread). note that ptx isa versions 1.x supports only kernel
function parameters in .param space; device function parameters were previously restricted to the
register state space. the use of parameter state space for device function parameters was introduced
in ptx isa version 2.0 and requires target architecturesm_20or higher. additional sub-qualifiers::entryor::funccan be specified on instructions with.paramstate space to indicate
whether the address refers to kernel function parameter or device function parameter. if no
sub-qualifier is specified with the.paramstate space, then the default sub-qualifier is specific
to and dependent on the exact instruction. for example,st.paramis equivalent tost.param::funcwhereasisspacep.paramis equivalent toisspacep.param::entry. refer to the instruction
description for more details on default sub-qualifier assumption.",7
5.1.7.shared state space,#shared-state-space,"5.1.7.shared state space the shared (.shared) state space is a memory that is owned by an executing cta and is accessible
to the threads of all the ctas within a cluster. an address in shared memory can be read and written
by any thread in a cta cluster. additional sub-qualifiers::ctaor::clustercan be specified on instructions with.sharedstate space to indicate whether the address belongs to the shared memory window of the
executing cta or of any cta in the cluster respectively. the addresses in the.shared::ctawindow also fall within the.shared::clusterwindow. if no sub-qualifier is specified with the.sharedstate space, then it defaults to::cta. for example,ld.sharedis equivalent told.shared::cta. variables declared in.sharedstate space refer to the memory addresses in the current
cta. instructionmapagives the.shared::clusteraddress of the corresponding variable in
another cta in the cluster. shared memory typically has some optimizations to support the sharing. one example is broadcast;
where all threads read from the same address. another is sequential access from sequential threads.",5
5.1.8.texture state space (deprecated),#texture-state-space-deprecated,"5.1.8.texture state space (deprecated) the texture (.tex) state space is global memory accessed via the texture instruction. it is
shared by all threads in a context. texture memory is read-only and cached, so accesses to texture
memory are not coherent with global memory stores to the texture image. the gpu hardware has a fixed number of texture bindings that can be accessed within a single kernel
(typically 128). the .tex directive will bind the named texture memory variable to a hardware
texture identifier, where texture identifiers are allocated sequentially beginning with
zero. multiple names may be bound to the same physical texture identifier. an error is generated if
the maximum number of physical resources is exceeded. the texture name must be of type.u32or.u64. physical texture resources are allocated on a per-kernel granularity, and.texvariables are
required to be defined in the global scope. texture memory is read-only. a textures base address is assumed to be aligned to a 16 byte
boundary. example for example, a legacy ptx definitions such as is equivalent to: seetexture sampler and surface typesfor the
description of the.texreftype andtexture instructionsfor its use in texture instructions.",5
5.2.types,#types,5.2.types,9
5.2.1.fundamental types,#fundamental-types,"5.2.1.fundamental types in ptx, the fundamental types reflect the native data types supported by the target architectures. a
fundamental type specifies both a basic type and a size. register variables are always of a
fundamental type, and instructions operate on these types. the same type-size specifiers are used
for both variable definitions and for typing instructions, so their names are intentionally short. table 8lists the fundamental type specifiers for
each basic type: most instructions have one or more type specifiers, needed to fully specify instruction
behavior. operand types and sizes are checked against instruction types for compatibility. two fundamental types are compatible if they have the same basic type and are the same size. signed
and unsigned integer types are compatible if they have the same size. the bit-size type is
compatible with any fundamental type having the same size. in principle, all variables (aside from predicates) could be declared using only bit-size types, but
typed variables enhance program readability and allow for better operand type checking.",7
5.2.2.restricted use of sub-word sizes,#restricted-use-of-sub-word-sizes,"5.2.2.restricted use of sub-word sizes the.u8,.s8, and.b8instruction types are restricted told,st, andcvtinstructions. the.f16floating-point type is allowed only in conversions to and from.f32,.f64types, in half precision floating point instructions and texture fetch instructions. the.f16x2floating point type is allowed only in half precision floating point arithmetic
instructions and texture fetch instructions. for convenience,ld,st, andcvtinstructions permit source and destination data
operands to be wider than the instruction-type size, so that narrow values may be loaded, stored,
and converted using regular-width registers. for example, 8-bit or 16-bit values may be held
directly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and
sizes.",6
5.2.3.alternate floating-point data formats,#alternate-floating-point-data-formats,"5.2.3.alternate floating-point data formats the fundamental floating-point types supported in ptx have implicit bit representations that
indicate the number of bits used to store exponent and mantissa. for example, the.f16type
indicates 5 bits reserved for exponent and 10 bits reserved for mantissa. in addition to the
floating-point representations assumed by the fundamental types, ptx allows the following alternate
floating-point data formats: alternate data formats cannot be used as fundamental types. they are supported as source or
destination formats by certain instructions.",7
5.2.4.packed data types,#packed-data-types,"5.2.4.packed data types certain ptx instructions operate on two sets of inputs in parallel, and produce two outputs. such
instructions can use the data stored in a packed format. ptx supports packing two values of the same
scalar data type into a single, larger value. the packed value is considered as a value of apacked
data type. in this section we describe the packed data types supported in ptx.",7
5.3.texture sampler and surface types,#texture-sampler-and-surface-types,"5.3.texture sampler and surface types ptx includes built-inopaquetypes for defining texture, sampler, and surface descriptor
variables. these types have named fields similar to structures, but all information about layout,
field ordering, base address, and overall size is hidden to a ptx program, hence the termopaque. the use of these opaque types is limited to: the three built-in types are.texref,.samplerref, and.surfref. for working with
textures and samplers, ptx has two modes of operation. in theunified mode,texture and sampler
information is accessed through a single.texrefhandle. in theindependent mode, texture and
sampler information each have their own handle, allowing them to be defined separately and combined
at the site of usage in the program. in independent mode, the fields of the.texreftype that
describe sampler properties are ignored, since these properties are defined by.samplerrefvariables. table 9andtable 10list the named members
of each type for unified and independent texture modes. these members and their values have
precise mappings to methods and values defined in the texturehwclass as well as
exposed values via the api.",7
5.3.1.texture and surface properties,#texture-surface-properties,"5.3.1.texture and surface properties fieldswidth,height, anddepthspecify the size of the texture or surface in number of
elements in each dimension. thechannel_data_typeandchannel_orderfields specify these properties of the texture or
surface using enumeration types corresponding to the source language api. for example, seechannel
data type and channel order fieldsfor
the opencl enumeration types currently supported in ptx.",6
5.3.2.sampler properties,#sampler-properties,"5.3.2.sampler properties thenormalized_coordsfield indicates whether the texture or surface uses normalized coordinates
in the range [0.0, 1.0) instead of unnormalized coordinates in the range [0, n). if no value is
specified, the default is set by the runtime system based on the source language. thefilter_modefield specifies how the values returned by texture reads are computed based on
the input texture coordinates. theaddr_mode_{0,1,2}fields define the addressing mode in each dimension, which determine how
out-of-range coordinates are handled. see thecuda c++ programming guidefor more details of these properties. in independent texture mode, the sampler properties are carried in an independent.samplerrefvariable, and these fields are disabled in the.texrefvariables. one additional sampler
property,force_unnormalized_coords, is available in independent texture mode. theforce_unnormalized_coordsfield is a property of.samplerrefvariables that allows the
sampler to override the texture headernormalized_coordsproperty. this field is defined only in
independent texture mode. whentrue, the texture header setting is overridden and unnormalized
coordinates are used; whenfalse, the texture header setting is used. theforce_unnormalized_coordsproperty is used in compiling opencl; in opencl, the property of
normalized coordinates is carried in sampler headers. to compile opencl to ptx, texture headers are
always initialized withnormalized_coordsset to true, and the opencl sampler-basednormalized_coordsflag maps (negated) to the ptx-levelforce_unnormalized_coordsflag. variables using these types may be declared at module scope or within kernel entry parameter
lists. at module scope, these variables must be in the.globalstate space. as kernel
parameters, these variables are declared in the.paramstate space. example when declared at module scope, the types may be initialized using a list of static expressions
assigning values to the named members. example",6
5.3.3.channel data type and channel order fields,#channel-data-type-and-channel-order-fields,"5.3.3.channel data type and channel order fields thechannel_data_typeandchannel_orderfields have enumeration types corresponding to the
source language api. currently, opencl is the only source language that defines these
fields.table 12andtable 11show the
enumeration values defined in opencl version 1.0 for channel data type and channel order.",6
5.4.variables,#variables,"5.4.variables in ptx, a variable declaration describes both the variables type and its state space. in addition
to fundamental types, ptx supports types for simple aggregate objects such as vectors and arrays.",7
5.4.1.variable declarations,#variable-declarations,"5.4.1.variable declarations all storage for data is specified with variable declarations. every variable must reside in one of
the state spaces enumerated in the previous section. a variable declaration names the space in which the variable resides, its type and size, its name,
an optional array size, an optional initializer, and an optional fixed address for the variable. predicate variables may only be declared in the register state space. examples",6
5.4.2.vectors,#vectors,"5.4.2.vectors limited-length vector types are supported. vectors of length 2 and 4 of any non-predicate
fundamental type can be declared by prefixing the type with.v2or.v4. vectors must be
based on a fundamental type, and they may reside in the register space. vectors cannot exceed
128-bits in length; for example,.v4.f64is not allowed. three-element vectors may be
handled by using a.v4vector, where the fourth element provides padding. this is a common case
for three-dimensional grids, textures, etc. examples by default, vector variables are aligned to a multiple of their overall size (vector length times
base-type size), to enable vector load and store instructions which require addresses aligned to a
multiple of the access size.",6
5.4.3.array declarations,#array-declarations,"5.4.3.array declarations array declarations are provided to allow the programmer to reserve space. to declare an array, the
variable name is followed with dimensional declarations similar to fixed-size array declarations
in c. the size of each dimension is a constant expression. examples the size of the array specifies how many elements should be reserved. for the declaration of arraykernelabove, 19*19 = 361 halfwords are reserved, for a total of 722 bytes. when declared with an initializer, the first dimension of the array may be omitted. the size of the
first array dimension is determined by the number of elements in the array initializer. examples arrayindexhas eight elements, and arrayoffsetis a 4x2 array.",6
5.4.4.initializers,#initializers,"5.4.4.initializers declared variables may specify an initial value using a syntax similar to c/c++, where the variable
name is followed by an equals sign and the initial value or values for the variable. a scalar takes
a single value, while vectors and arrays take nested lists of values inside of curly braces (the
nesting matches the dimensionality of the declaration). as in c, array initializers may be incomplete, i.e., the number of initializer elements may be less
than the extent of the corresponding array dimension, with remaining array locations initialized to
the default value for the specified array type. examples is equivalent to currently, variable initialization is supported only for constant and global state spaces. variables
in constant and global state spaces with no explicit initializer are initialized to zero by
default. initializers are not allowed in external variable declarations. variable names appearing in initializers represent the address of the variable; this can be used to
statically initialize a pointer to a variable. initializers may also containvar+offsetexpressions, whereoffsetis a byte offset added to the address ofvar. only variables in.globalor.conststate spaces may be used in initializers. by default, the resulting
address is the offset in the variables state space (as is the case when taking the address of a
variable with amovinstruction). an operator,generic(), is provided to create a generic
address for variables used in initializers. starting ptx isa version 7.1, an operatormask()is provided, wheremaskis an integer
immediate. the only allowed expressions in themask()operator are integer constant expression
and symbol expression representing address of variable. themask()operator extractsnconsecutive bits from the expression used in initializers and inserts these bits at the lowest
position of the initialized variable. the numbernand the starting position of the bits to be
extracted is specified by the integer immediatemask. ptx isa version 7.1 only supports
extracting a single byte starting at byte boundary from the address of the variable. ptx isa version
7.3 supports integer constant expression as an operand in themask()operator. supported values formaskare: 0xff, 0xff00, 0xff0000, 0xff000000, 0xff00000000, 0xff0000000000,
0xff000000000000, 0xff00000000000000. examples device function names appearing in initializers represent the address of the first instruction in
the function; this can be used to initialize a table of function pointers to be used with indirect
calls. beginning in ptx isa version 3.1, kernel function names can be used as initializers e.g. to
initialize a table of kernel function pointers, to be used with cuda dynamic parallelism to launch
kernels from gpu. see thecuda dynamic parallelism programming guidefor details. labels cannot be used in initializers. variables that hold addresses of variables or functions should be of type.u8or.u32or.u64. type.u8is allowed only if themask()operator is used. initializers are allowed for all types except.f16,.f16x2and.pred. examples",6
5.4.5.alignment,#alignment,"5.4.5.alignment byte alignment of storage for all addressable variables can be specified in the variable
declaration. alignment is specified using an optional.alignbyte-countspecifier immediately
following the state-space specifier. the variable will be aligned to an address which is an integer
multiple of byte-count. the alignment value byte-count must be a power of two. for arrays, alignment
specifies the address alignment for the starting address of the entire array, not for individual
elements. the default alignment for scalar and array variables is to a multiple of the base-type size. the
default alignment for vector variables is to a multiple of the overall vector size. examples note that all ptx instructions that access memory require that the address be aligned to a multiple
of the access size. the access size of a memory instruction is the total number of bytes accessed in
memory. for example, the access size ofld.v4.b32is 16 bytes, while the access size ofatom.f16x2is 4 bytes.",6
5.4.6.parameterized variable names,#parameterized-variable-names,"5.4.6.parameterized variable names since ptx supports virtual registers, it is quite common for a compiler frontend to generate a large
number of register names. rather than require explicit declaration of every name, ptx supports a
syntax for creating a set of variables having a common prefix string appended with integer suffixes. for example, suppose a program uses a large number, say one hundred, of.b32variables, named%r0,%r1, ,%r99. these 100 register variables can be declared as follows: this shorthand syntax may be used with any of the fundamental types and with any state space, and
may be preceded by an alignment specifier. array variables cannot be declared this way, nor are
initializers permitted.",7
5.4.7.variable attributes,#variable-attributes,"5.4.7.variable attributes variables may be declared with an optional.attributedirective which allows specifying special
attributes of variables. keyword.attributeis followed by attribute specification inside
parenthesis. multiple attributes are separated by comma. variable and function attribute directive: .attributedescribes the.attributedirective.",6
5.4.8.variable and function attribute directive: .attribute,#variable-and-function-attribute-directive-attribute,5.4.8.variable and function attribute directive: .attribute .attribute variable and function attributes description used to specify special attributes of a variable or a function. the following attributes are supported. ptx isa notes target isa notes examples,7
5.5.tensors,#tensors,"5.5.tensors a tensor is a multi-dimensional matrix structure in the memory. tensor is defined by the following
properties: ptx supports instructions which can operate on the tensor data. ptx tensor instructions include: the tensor data can be operated on by variouswmma.mma,mmaandwgmma.mma_asyncinstructions. ptx tensor instructions treat the tensor data in the global memory as a multi-dimensional structure
and treat the data in the shared memory as a linear data.",7
"5.5.1.tensor dimension, size and format",#tensor-dimension-size-format,"5.5.1.tensor dimension, size and format tensors can have dimensions: 1d, 2d, 3d, 4d or 5d. each dimension has a size which represents the number of elements along the dimension. the elements
can have one the following types: tensor can have padding at the end in each of the dimensions to provide alignment for the data in
the subsequent dimensions. tensor stride can be used to specify the amount of padding in each
dimension.",6
5.5.2.tensor access modes,#tensor-access-modes,5.5.2.tensor access modes tensor data can be accessed in two modes:,6
5.5.3.tiled mode,#tensor-tiled-mode,5.5.3.tiled mode this section talks about how tensor and tensor access work in tiled mode.,6
5.5.4.im2col mode,#tensor-im2col-mode,"5.5.4.im2col mode im2col mode supports the following tensor dimensions : 3d, 4d and 5d. in this mode, the tensor data
is treated as a batch of images with the following properties: the above properties are associated with 3d, 4d and 5d tensors as follows:",6
5.5.5.interleave layout,#tensor-interleaved-layout,"5.5.5.interleave layout tensor can be interleaved and the following interleave layouts are supported: thecinformation is organized in slices where sequential c elements are grouped in 16 byte or 32
byte quantities. if the total number of channels is not a multiple of the number of channels per slice, then the last
slice must be padded with zeros to make it complete 16b or 32b slice. interleaved layouts are supported only for the dimensionalities : 3d, 4d and 5d.",6
5.5.6.swizzling modes,#tensor-swizzling-modes,"5.5.6.swizzling modes the layout of the data in the shared memory can be different to that of global memory, for access
performance reasons. the following describes various swizzling modes:",5
5.5.7.tensor-map,#tensor-tensormap,"5.5.7.tensor-map the tensor-map is a 128-byte opaque object either in.constspace or.param(kernel function
parameter) space or.globalspace which describes the tensor properties and the access properties
of the tensor data described in previous sections. tensor-map can be created using cuda apis. refer tocuda programming guidefor more details.",0
6.instruction operands,#instruction-operands,6.instruction operands,9
6.1.operand type information,#operand-type-information,"6.1.operand type information all operands in instructions have a known type from their declarations. each operand type must be
compatible with the type determined by the instruction template and instruction type. there is no
automatic conversion between types. the bit-size type is compatible with every type having the same size. integer types of a common size
are compatible with each other. operands having type different from but compatible with the
instruction type are silently cast to the instruction type.",6
6.2.source operands,#source-operands,"6.2.source operands the source operands are denoted in the instruction descriptions by the namesa,b, andc. ptx describes a load-store machine, so operands for alu instructions must all be in variables
declared in the.regregister state space. for most operations, the sizes of the operands must
be consistent. thecvt(convert) instruction takes a variety of operand types and sizes, as its job is to
convert from nearly any data type to any other data type (and size). theld,st,mov, andcvtinstructions copy data from one location to
another. instructionsldandstmove data from/to addressable state spaces to/from
registers. themovinstruction copies data between registers. most instructions have an optional predicate guard that controls conditional execution, and a few
instructions have additional predicate source operands. predicate operands are denoted by the namesp,q,r,s.",7
6.3.destination operands,#destination-operands,"6.3.destination operands ptx instructions that produce a single result store the result in the field denoted byd(for
destination) in the instruction descriptions. the result operand is a scalar or vector variable in
the register state space.",7
"6.4.using addresses, arrays, and vectors",#using-addresses-arrays-and-vectors,"6.4.using addresses, arrays, and vectors using scalar variables as operands is straightforward. the interesting capabilities begin with
addresses, arrays, and vectors.",6
6.4.1.addresses as operands,#addresses-as-operands,"6.4.1.addresses as operands all the memory instructions take an address operand that specifies the memory location being
accessed. this addressable operand is one of: the register containing an address may be declared as a bit-size type or integer type. the access size of a memory instruction is the total number of bytes accessed in memory. for
example, the access size ofld.v4.b32is 16 bytes, while the access size ofatom.f16x2is 4
bytes. the address must be naturally aligned to a multiple of the access size. if an address is not
properly aligned, the resulting behavior is undefined. for example, among other things, the access
may proceed by silently masking off low-order address bits to achieve proper rounding, or the
instruction may fault. the address size may be either 32-bit or 64-bit. 128-bit adresses are not supported. addresses are
zero-extended to the specified width as needed, and truncated if the register width exceeds the
state space address width for the target architecture. address arithmetic is performed using integer arithmetic and logical instructions. examples include
pointer arithmetic and pointer comparisons. all addresses and address computations are byte-based;
there is no support for c-style pointer arithmetic. themovinstruction can be used to move the address of a variable into a pointer. the address is
an offset in the state space in which the variable is declared. load and store operations move data
between registers and locations in addressable state spaces. the syntax is similar to that used in
many assembly languages, where scalar variables are simply named and addresses are de-referenced by
enclosing the address expression in square brackets. address expressions include variable names,
address registers, address register plus byte offset, and immediate address expressions which
evaluate at compile-time to a constant address. here are a few examples:",6
6.4.2.arrays as operands,#arrays-as-operands,"6.4.2.arrays as operands arrays of all types can be declared, and the identifier becomes an address constant in the space
where the array is declared. the size of the array is a constant in the program. array elements can be accessed using an explicitly calculated byte address, or by indexing into the
array using square-bracket notation. the expression within square brackets is either a constant
integer, a register variable, or a simpleregister with constant offsetexpression, where the
offset is a constant expression that is either added or subtracted from a register variable. if more
complicated indexing is desired, it must be written as an address calculation prior to use. examples
are:",6
6.4.3.vectors as operands,#vectors-as-operands,"6.4.3.vectors as operands vector operands are supported by a limited subset of instructions, which includemov,ld,st,atom,redandtex. vectors may also be passed as arguments to called functions. vector elements can be extracted from the vector with the suffixes.x,.y,.zand.w, as well as the typical color fields.r,.g,.band.a. a brace-enclosed list is used for pattern matching to pull apart vectors. vector loads and stores can be used to implement wide loads and stores, which may improve memory
performance. the registers in the load/store operations can be a vector, or a brace-enclosed list of
similarly typed scalars. here are examples: elements in a brace-enclosed vector, say {ra, rb, rc, rd}, correspond to extracted elements as follows:",6
6.4.4.labels and function names as operands,#labels-and-function-names-as-operands,"6.4.4.labels and function names as operands labels and function names can be used only inbra/brx.idxandcallinstructions
respectively. function names can be used inmovinstruction to get the address of the function
into a register, for use in an indirect call. beginning in ptx isa version 3.1, themovinstruction may be used to take the address of kernel
functions, to be passed to a system call that initiates a kernel launch from the gpu. this feature
is part of the support for cuda dynamic parallelism. see thecuda dynamic parallelism programming
guidefor details.",7
6.5.type conversion,#type-conversion,"6.5.type conversion all operands to all arithmetic, logic, and data movement instruction must be of the same type and
size, except for operations where changing the size and/or type is part of the definition of the
instruction. operands of different sizes or types must be converted prior to the operation.",6
6.5.1.scalar conversions,#scalar-conversions,"6.5.1.scalar conversions table 13shows what
precision and format the cvt instruction uses given operands of differing types. for example, if acvt.s32.u16instruction is given au16source operand ands32as a destination operand,
theu16is zero-extended tos32. conversions to floating-point that are beyond the range of floating-point numbers are represented
with the maximum floating-point value (ieee 754 inf forf32andf64, and ~131,000 forf16).",6
6.5.2.rounding modifiers,#rounding-modifiers,"6.5.2.rounding modifiers conversion instructions may specify a rounding modifier. in ptx, there are four integer rounding
modifiers and four floating-point rounding
modifiers.table 14andtable 15summarize the rounding modifiers.",7
6.6.operand costs,#operand-costs,"6.6.operand costs operands from different state spaces affect the speed of an operation. registers are fastest, while
global memory is slowest. much of the delay to memory can be hidden in a number of ways. the first
is to have multiple threads of execution so that the hardware can issue a memory operation and then
switch to other execution. another way to hide latency is to issue the load instructions as early as
possible, as execution is not blocked until the desired result is used in a subsequent (in time)
instruction. the register in a store operation is available much more
quickly.table 16gives estimates of the
costs of using different kinds of memory.",5
7.abstracting the abi,#abstracting-abi,"7.abstracting the abi rather than expose details of a particular calling convention, stack layout, and application binary
interface (abi), ptx provides a slightly higher-level abstraction and supports multiple abi
implementations. in this section, we describe the features of ptx needed to achieve this hiding of
the abi. these include syntax for function definitions, function calls, parameter passing, support
for variadic functions (varargs), and memory allocated on the stack (alloca). refer toptx writers guide to interoperabilityfor details on generating ptx compliant with
application binary interface (abi) for the cudaarchitecture.",7
7.1.function declarations and definitions,#function-declarations-and-definitions,"7.1.function declarations and definitions in ptx, functions are declared and defined using the.funcdirective. a functiondeclarationspecifies an optional list of return parameters, the function name, and an optional list of input
parameters; together these specify the functions interface, or prototype. a functiondefinitionspecifies both the interface and the body of the function. a function must be declared or defined
prior to being called. the simplest function has no parameters or return values, and is represented in ptx as follows: here, execution of thecallinstruction transfers control tofoo, implicitly saving the
return address. execution of theretinstruction withinfootransfers control to the
instruction following the call. scalar and vector base-type input and return parameters may be represented simply as register
variables. at the call, arguments may be register variables or constants, and return values may be
placed directly into register variables. the arguments and return variables at the call must have
type and size that match the callees corresponding formal parameters. example when using the abi,.regstate space parameters must be at least 32-bits in size. subword scalar
objects in the source language should be promoted to 32-bit registers in ptx, or use.paramstate space byte arrays described next. objects such as c structures and unions are flattened into registers or byte arrays in ptx and are
represented using.paramspace memory. for example, consider the following c structure, passed
by value to a function: in ptx, this structure will be flattened into a byte array. since memory accesses are required to be
aligned to a multiple of the access size, the structure in this example will be a 12 byte array with
8 byte alignment so that accesses to the.f64field are aligned. the.paramstate space is
used to pass the structure by value: example in this example, note that.paramspace variables are used in two ways. first, a.paramvariableyis used in function definition bar to represent a formal parameter. second, a.paramvariablepyis declared in the body of the calling function and used to set up the
structure being passed to bar. the following is a conceptual way to think about the.paramstate space use in device functions. for a caller, for a callee, the following restrictions apply to parameter passing. for a caller, for a callee, note that the choice of.regor.paramstate space for parameter passing has no impact on
whether the parameter is ultimately passed in physical registers or on the stack. the mapping of
parameters to physical registers and stack locations depends on the abi definition and the order,
size, and alignment of parameters.",7
7.1.1.changes from ptx isa version 1.x,#changes-from-ptx-isa-version-1-x,"7.1.1.changes from ptx isa version 1.x in ptx isa version 1.x, formal parameters were restricted to .reg state space, and there was no
support for array parameters. objects such as c structures were flattened and passed or returned
using multiple registers. ptx isa version 1.x supports multiple return values for this purpose. beginning with ptx isa version 2.0, formal parameters may be in either.regor.paramstate
space, and.paramspace parameters support arrays. for targetssm_20or higher, ptx
restricts functions to a single return value, and a.parambyte array should be used to return
objects that do not fit into a register. ptx continues to support multiple return registers forsm_1xtargets. ptx isa versions prior to 3.0 permitted variables in.regand.localstate spaces to be
defined at module scope. when compiling to use the abi, ptx isa version 3.0 and later disallows
module-scoped.regand.localvariables and restricts their use to within function
scope. when compiling without use of the abi, module-scoped.regand.localvariables are
supported as before. when compiling legacy ptx code (isa versions prior to 3.0) containing
module-scoped.regor.localvariables, the compiler silently disables use of the abi.",7
7.2.variadic functions,#variadic-functions,"7.2.variadic functions ptx version 6.0 supports passing unsized array parameter to a function which can be used to
implement variadic functions. refer tokernel and function directives: .funcfor details",6
7.3.alloca,#alloca,"7.3.alloca ptx providesallocainstruction for allocating storage at runtime on the per-thread local memory
stack. the allocated stack memory can be accessed withld.localandst.localinstructions
using the pointer returned byalloca. in order to facilitate deallocation of memory allocated withalloca, ptx provides two additional
instructions:stacksavewhich allows reading the value of stack pointer in a local variable, andstackrestorewhich can restore the stack pointer with the saved value. alloca,stacksave, andstackrestoreinstructions are described instack manipulation
instructions.",7
8.memory consistency model,#memory-consistency-model,"8.memory consistency model in multi-threaded executions, the side-effects of memory operations performed by each thread become
visible to other threads in a partial and non-identical order. this means that any two operations
may appear to happen in no order, or in different orders, to different threads. the axioms
introduced by the memory consistency model specify exactly which contradictions are forbidden
between the orders observed by different threads. in the absence of any constraint, each read operation returns the value committed by some write
operation to the same memory location, including the initial write to that memory location. the
memory consistency model effectively constrains the set of such candidate writes from which a read
operation can return a value.",5
8.1.scope and applicability of the model,#scope-and-applicability,"8.1.scope and applicability of the model the constraints specified under this model apply to ptx programs with any ptx isa version number,
running onsm_70or later architectures. the memory consistency model does not apply to texture (includingld.global.nc) and surface
accesses.",7
8.1.1.limitations on atomicity at system scope,#limitations-system-scope-atomicity,"8.1.1.limitations on atomicity at system scope when communicating with the host cpu, certain strong operations with system scope may not be
performed atomically on some systems. for more details on atomicity guarantees to host memory, see
thecuda atomicity requirements.",5
8.2.memory operations,#memory-operations,"8.2.memory operations the fundamental storage unit in the ptx memory model is a byte, consisting of 8 bits. each state
space available to a ptx program is a sequence of contiguous bytes in memory. every byte in a ptx
state space has a unique address relative to all threads that have access to the same state space. each ptx memory instruction specifies an address operand and a data type. the address operand
contains a virtual address that gets converted to a physical address during memory access. the
physical address and the size of the data type together define a physical memory location, which is
the range of bytes starting from the physical address and extending up to the size of the data type
in bytes. the memory consistency model specification uses the terms address or memory address to indicate
a virtual address, and the term memory location to indicate a physical memory location. each ptx memory instruction also specifies the operation  either a read, a write or an atomic
read-modify-write  to be performed on all the bytes in the corresponding memory location.",7
8.2.1.overlap,#overlap,"8.2.1.overlap two memory locations are said to overlap when the starting address of one location is within the
range of bytes constituting the other location. two memory operations are said to overlap when they
specify the same virtual address and the corresponding memory locations overlap. the overlap is said
to be complete when both memory locations are identical, and it is said to be partial otherwise.",5
8.2.2.aliases,#aliases,8.2.2.aliases two distinct virtual addresses are said to be aliases if they map to the same memory location.,6
8.2.3.multimem addresses,#multimem-addresses,"8.2.3.multimem addresses a multimem address is a virtual address which points to multiple distinct memory locations across
devices. onlymultimem.* operations are valid on multimem addresses. that is, the behavior of accessing
a multimem address in any other memory operation is undefined.",5
8.2.4.memory operations on vector data types,#memory-operations-on-vector-data-types,"8.2.4.memory operations on vector data types the memory consistency model relates operations executed on memory locations with scalar data types,
which have a maximum size and alignment of 64 bits. memory operations with a vector data type are
modelled as a set of equivalent memory operations with a scalar data type, executed in an
unspecified order on the elements in the vector.",5
8.2.5.memory operations on packed data types,#memory-operations-on-packed-data-types,"8.2.5.memory operations on packed data types a packed data type consists of two values of the same scalar data type, as described inpacked data
types. these values are accessed in adjacent memory locations. a
memory operation on a packed data type is modelled as a pair of equivalent memory operations on the
scalar data type, executed in an unspecified order on each element of the packed data.",5
8.2.6.initialization,#initialization,"8.2.6.initialization each byte in memory is initialized by a hypothetical writew0executed before starting any thread
in the program. if the byte is included in a program variable, and that variable has an initial
value, thenw0writes the corresponding initial value for that byte; elsew0is assumed to have
written an unknown but constant value to the byte.",6
8.3.state spaces,#memory-consistency-state-spaces,"8.3.state spaces the relations defined in the memory consistency model are independent of state spaces. in
particular, causality order closes over all memory operations across all the state spaces. but the
side-effect of a memory operation in one state space can be observed directly only by operations
that also have access to the same state space. this further constrains the synchronizing effect of a
memory operation in addition to scope. for example, the synchronizing effect of the ptx instructionld.relaxed.shared.sysis identical to that ofld.relaxed.shared.cluster, since no thread
outside the same cluster can execute an operation that accesses the same memory location.",5
8.4.operation types,#operation-types,"8.4.operation types for simplicity, the rest of the document refers to the following operation types, instead of
mentioning specific instructions that give rise to them.",6
8.4.1.mmio operation,#mmio-operation,"8.4.1.mmio operation anmmiooperation is a memory operation with.mmioqualifier specified. it is usually performed
on a memory location which is mapped to the control registers of peer i/o devices. it can also be
used for communication between threads but has poor performance relative to non-mmiooperations. the semantic meaning ofmmiooperations cannot be defined precisely as it is defined by the
underlying i/o device. for formal specification of semantics ofmmiooperation from memory
consistency model perspective, it is equivalent to the semantics of astrongoperation. but it
follows a few implementation-specific properties, if it meets thecuda atomicity requirementsat
the specified scope:",5
8.5.scope,#scope,"8.5.scope eachstrongoperation must specify ascope, which is the set of threads that may interact
directly with that operation and establish any of the relations described in the memory consistency
model. there are four scopes: note that the warp is not ascope; the cta is the smallest collection of threads that qualifies as
ascopein the memory consistency model.",5
8.6.proxies,#proxies,"8.6.proxies amemory proxy, or aproxyis an abstract label applied to a method of memory access. when two
memory operations use distinct methods of memory access, they are said to be differentproxies. memory operations as defined inoperation typesusegenericmethod of memory access, i.e. ageneric proxy. other operations such as textures and surfaces all
use distinct methods of memory access, also distinct from thegenericmethod. aproxy fenceis required to synchronize memory operations across differentproxies. although
virtual aliases use thegenericmethod of memory access, since using distinct virtual addresses
behaves as if using differentproxies, they require aproxy fenceto establish memory ordering.",5
8.7.morally strong operations,#morally-strong-operations,"8.7.morally strong operations two operations are said to bemorally strongrelative to each other if they satisfy all of the
following conditions: most (but not all) of the axioms in the memory consistency model depend on relations betweenmorally strongoperations.",5
8.7.1.conflict and data-races,#conflict-and-data-races,8.7.1.conflict and data-races twooverlappingmemory operations are said toconflictwhen at least one of them is awrite. twoconflictingmemory operations are said to be in adata-raceif they are not related incausality orderand they are notmorally strong.,5
8.7.2.limitations on mixed-size data-races,#mixed-size-limitations,"8.7.2.limitations on mixed-size data-races adata-racebetween operations thatoverlapcompletely is called auniform-size data-race,
while adata-racebetween operations thatoverlappartially is called amixed-size data-race. the axioms in the memory consistency model do not apply if a ptx program contains one or moremixed-size data-races. but these axioms are sufficient to describe the behavior of a ptx program
with onlyuniform-size data-races. atomicity of mixed-size rmw operations in any program with or withoutmixed-size data-races, the following property holds for every pair
ofoverlapping atomicoperations a1 and a2 such that each specifies ascopethat includes the
other: either theread-modify-writeoperation specified by a1 is performed completely before a2 is
initiated, or vice versa. this property holds irrespective of whether the two operations a1 and a2
overlap partially or completely.",5
8.8.release and acquire patterns,#release-acquire-patterns,"8.8.release and acquire patterns some sequences of instructions give rise to patterns that participate in memory synchronization as
described later. thereleasepattern makes prior operations from the current thread1visible to some operations from other threads. theacquirepattern makes some operations from
other threads visible to later operations from the current thread. areleasepattern on a location m consists of one of the following: anymemory synchronizationestablished by areleasepattern only affects operations occurring inprogram orderbefore the first instruction in that pattern. anacquirepattern on a location m consists of one of the following: anymemory synchronizationestablished by anacquirepattern only affects operations occurring
inprogram orderafter the last instruction in that pattern. 1for bothreleaseandacquirepatterns, this effect is further extended to operations in
other threads through the transitive nature ofcausality order.",5
8.9.ordering of memory operations,#ordering-memory-operations,"8.9.ordering of memory operations the sequence of operations performed by each thread is captured asprogram orderwhilememory
synchronizationacross threads is captured ascausality order. the visibility of the side-effects
of memory operations to other memory operations is captured ascommunication order. the memory
consistency model defines contradictions that are disallowed between communication order on the one
hand, andcausality orderandprogram orderon the other.",5
8.9.1.program order,#program-order,"8.9.1.program order theprogram orderrelates all operations performed by a thread to the order in which a sequential
processor will execute instructions in the corresponding ptx source. it is a transitive relation
that forms a total order over the operations performed by the thread, but does not relate operations
from different threads.",7
8.9.2.observation order,#observation-order,"8.9.2.observation order observation orderrelates a write w to a read r through an optional sequence of atomic
read-modify-write operations. a write w precedes a read r inobservation orderif:",6
8.9.3.fence-sc order,#fence-sc-order,"8.9.3.fence-sc order thefence-scorder is an acyclic partial order, determined at runtime, that relates every pair ofmorally strong fence.scoperations.",9
8.9.4.memory synchronization,#memory-synchronization,"8.9.4.memory synchronization synchronizing operations performed by different threads synchronize with each other at runtime as
described here. the effect of such synchronization is to establishcausality orderacross threads. api synchronization asynchronizesrelation can also be established by certain cuda apis. in addition to establishing asynchronizesrelation, the cuda api synchronization mechanisms above
also participate inproxy-preserved base causality order.",5
8.9.5.causality order,#causality-order,"8.9.5.causality order causality ordercaptures how memory operations become visible across threads through synchronizing
operations. the axiom causality uses this order to constrain the set of write operations from
which a read operation may read a value. relations in thecausality orderprimarily consist of relations inbase causality order1, which is a transitive order, determined at runtime. base causality order an operation x precedes an operation y inbase causality orderif: proxy-preserved base causality order a memory operation x precedes a memory operation y inproxy-preserved base causality orderif x
precedes y inbase causality order, and: causality order causality ordercombinesbase causality orderwith some non-transitive relations as follows: an operation x precedes an operation y incausality orderif: 1the transitivity ofbase causality orderaccounts for the cumulativity of synchronizing
operations.",5
8.9.6.coherence order,#coherence-order,"8.9.6.coherence order there exists a partial transitive order that relatesoverlappingwrite operations, determined at
runtime, called thecoherence order1. twooverlappingwrite operations are related incoherence orderif they aremorally strongor if they are related incausality order. twooverlappingwrites are unrelated incoherence orderif they are in adata-race, which gives
rise to the partial nature ofcoherence order. 1coherence ordercannot be observed directly since it consists entirely of write
operations. it may be observed indirectly by its use in constraining the set of candidate
writes that a read operation may read from.",6
8.9.7.communication order,#communication-order,"8.9.7.communication order thecommunication orderis a non-transitive order, determined at runtime, that relates write
operations to otheroverlappingmemory operations. communication ordercaptures the visibility of memory operations  when a memory operation x1
precedes a memory operation x2 incommunication order, x1 is said to be visible to x2.",5
8.10.axioms,#axioms,8.10.axioms,9
8.10.1.coherence,#coherence-axiom,"8.10.1.coherence if a write w precedes anoverlappingwrite w incausality order, then w must precede w incoherence order.",9
8.10.2.fence-sc,#fence-sc-axiom,"8.10.2.fence-sc fence-scorder cannot contradictcausality order. for a pair ofmorally strongfence.scoperations f1 and f2, if f1 precedes f2 incausality order, then f1 must precede f2 infence-scorder.",9
8.10.3.atomicity,#atomicity-axiom,"8.10.3.atomicity single-copy atomicity conflictingmorally strongoperations are performed withsingle-copy atomicity. when a read r
and a write w aremorally strong, then the following two communications cannot both exist in the
same execution, for the set of bytes accessed by both r and w: atomicity of read-modify-write (rmw) operations when anatomicoperation a and a write woverlapand aremorally strong, then the following
two communications cannot both exist in the same execution, for the set of bytes accessed by both a
and w: litmus test 1: atomicity is guaranteed when the operations aremorally strong. litmus test 2: atomicity is not guaranteed if the operations are notmorally strong.",5
8.10.4.no thin air,#no-thin-air-axiom,"8.10.4.no thin air values may not appear out of thin air: an execution cannot speculatively produce a value in such a
way that the speculation becomes self-satisfying through chains of instruction dependencies and
inter-thread communication. this matches both programmer intuition and hardware reality, but is
necessary to state explicitly when performing formal analysis. litmus test: load buffering the litmus test known as lb (load buffering) checks such forbidden values that may arise out of
thin air. two threads t1 and t2 each read from a first variable and copy the observed result into a
second variable, with the first and second variable exchanged between the threads. if each variable
is initially zero, the final result shall also be zero. if a1 reads from b2 and a2 reads from b1,
then values passing through the memory operations in this example form a cycle:
a1->b1->a2->b2->a1. only the values x == 0 and y == 0 are allowed to satisfy this cycle. if any of
the memory operations in this example were to speculatively associate a different value with the
corresponding memory location, then such a speculation would become self-fulfilling, and hence
forbidden.",5
8.10.5.sequential consistency per location,#sc-per-loc-axiom,"8.10.5.sequential consistency per location within any set ofoverlappingmemory operations that are pairwisemorally strong,communication
ordercannot contradictprogram order, i.e., a concatenation ofprogram orderbetweenoverlappingoperations andmorally strongrelations incommunication ordercannot result in a
cycle. this ensures that each program slice ofoverlappingpairwise morallystrong operationsis
strictlysequentially-consistent. litmus test: corr the litmus test corr (coherent read-read), demonstrates one consequence of this guarantee. a
thread t1 executes a write w1 on a location x, and a thread t2 executes two (or an infinite sequence
of) reads r1 and r2 on the same location x. no other writes are executed on x, except the one
modelling the initial value. the operations w1, r1 and r2 are pairwisemorally strong. if r1 reads
from w1, then the subsequent read r2 must also observe the same value. if r2 observed the initial
value of x instead, then this would form a sequence ofmorally-strongrelations r2->w1->r1 incommunication orderthat contradicts theprogram orderr1->r2 in thread t2. hence r2 cannot read
the initial value of x in such an execution.",5
8.10.6.causality,#causality-axiom,"8.10.6.causality relations incommunication ordercannot contradictcausality order. this constrains the set of
candidate write operations that a read operation may read from: litmus test: message passing the litmus test known as mp (message passing) represents the essence of typical synchronization
algorithms. a vast majority of useful programs can be reduced to sequenced applications of this
pattern. thread t1 first writes to a data variable and then to a flag variable while a second thread t2 first
reads from the flag variable and then from the data variable. the operations on the flag aremorally strongand the memory operations in each thread are separated by afence, and thesefencesaremorally strong. if r1 observes w2, then the release pattern f1; w2synchronizeswith theacquire patternr1;
f2. this establishes thecausality orderw1 -> f1 -> w2 -> r1 -> f2 -> r2. then axiomcausalityguarantees that r2 cannot read from any write that precedes w1 incoherence order. in the absence
of any other writes in this example, r2 must read from w1. litmus test: cowr virtual aliases require an aliasproxy fencealong the synchronization path. litmus test: store buffering the litmus test known as sb (store buffering) demonstrates thesequential consistencyenforced
by thefence.sc. a thread t1 writes to a first variable, and then reads the value of a second
variable, while a second thread t2 writes to the second variable and then reads the value of the
first variable. the memory operations in each thread are separated byfence.sc instructions,
and thesefencesaremorally strong. in any execution, either f1 precedes f2 infence-scorder, or vice versa. if f1 precedes f2 infence-scorder, then f1synchronizeswith f2. this establishes thecausality orderin w1 -> f1
-> f2 -> r2. axiomcausalityensures that r2 cannot read from any write that precedes w1 incoherence order. in the absence of any other write to that variable, r2 must read from
w1. similarly, in the case where f2 precedes f1 infence-scorder, r1 must read from w2. if eachfence.scin this example were replaced by afence.acq_relinstruction, then this outcome is
not guaranteed. there may be an execution where the write from each thread remains unobserved from
the other thread, i.e., an execution is possible, where both r1 and r2 return the initial value 0
for variables y and x respectively.",5
9.instruction set,#instruction-set,9.instruction set,9
9.1.format and semantics of instruction descriptions,#format-and-semantics-of-instruction-descriptions,"9.1.format and semantics of instruction descriptions this section describes each ptx instruction. in addition to the name and the format of the
instruction, the semantics are described, followed by some examples that attempt to show several
possible instantiations of the instruction.",7
9.2.ptx instructions,#ptx-instructions,"9.2.ptx instructions ptx instructions generally have from zero to four operands, plus an optional guard predicate
appearing after an@symbol to the left of theopcode: for instructions that create a result value, thedoperand is the destination operand, whilea,b, andcare source operands. thesetpinstruction writes two destination registers. we use a|symbol to separate
multiple destination registers. for some instructions the destination operand is optional. abit bucketoperand denoted with an
underscore (_) may be used in place of a destination register.",7
9.3.predicated execution,#predicated-execution,"9.3.predicated execution in ptx, predicate registers are virtual and have.predas the type specifier. so, predicate
registers can be declared as all instructions have an optionalguard predicatewhich controls conditional execution of the
instruction. the syntax to specify conditional execution is to prefix an instruction with@{!}p,
wherepis a predicate variable, optionally negated. instructions without a guard predicate are
executed unconditionally. predicates are most commonly set as the result of a comparison performed by thesetpinstruction. as an example, consider the high-level code this can be written in ptx as to get a conditional branch or conditional function call, use a predicate to control the execution
of the branch or call instructions. to implement the above example as a true conditional branch, the
following ptx instruction sequence might be used:",7
9.3.1.comparisons,#comparisons,9.3.1.comparisons,9
9.3.2.manipulating predicates,#manipulating-predicates,"9.3.2.manipulating predicates predicate values may be computed and manipulated using the following instructions:and,or,xor,not, andmov. there is no direct conversion between predicates and integer values, and no direct way to load or
store predicate register values. however,setpcan be used to generate a predicate from an
integer, and the predicate-based select (selp) instruction can be used to generate an integer
value based on the value of a predicate; for example:",6
9.4.type information for instructions and operands,#type-information-for-instructions-and-operands,"9.4.type information for instructions and operands typed instructions must have a type-size modifier. for example, theaddinstruction requires
type and size information to properly perform the addition operation (signed, unsigned, float,
different sizes), and this information must be specified as a suffix to the opcode. example some instructions require multiple type-size modifiers, most notably the data conversion instructioncvt. it requires separate type-size modifiers for the result and source, and these are placed in
the same order as the operands. for example: in general, an operands type must agree with the corresponding instruction-type modifier. the rules
for operand and instruction type conformance are as follows: table 23summarizes these type
checking rules. example",6
9.4.1.operand size exceeding instruction-type size,#operand-size-exceeding-instruction-type-size,"9.4.1.operand size exceeding instruction-type size for convenience,ld,st, andcvtinstructions permit source and destination data
operands to be wider than the instruction-type size, so that narrow values may be loaded, stored,
and converted using regular-width registers. for example, 8-bit or 16-bit values may be held
directly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and
sizes. the operand type checking rules are relaxed for bit-size and integer (signed and unsigned)
instruction types; floating-point instruction types still require that the operand type-size matches
exactly, unless the operand is of bit-size type. when a source operand has a size that exceeds the instruction-type size, the source data is
truncated (chopped) to the appropriate number of bits specified by the instruction type-size. table 24summarizes the relaxed type-checking rules for source operands. note that some combinations may
still be invalid for a particular instruction; for example, thecvtinstruction does not support.bxinstruction types, so those rows are invalid forcvt. when a destination operand has a size that exceeds the instruction-type size, the destination data
is zero- or sign-extended to the size of the destination register. if the corresponding instruction
type is signed integer, the data is sign-extended; otherwise, the data is zero-extended. table 25summarizes the relaxed type-checking rules for destination operands.",6
9.5.divergence of threads in control constructs,#divergence-of-threads-in-control-constructs,"9.5.divergence of threads in control constructs threads in a cta execute together, at least in appearance, until they come to a conditional control
construct such as a conditional branch, conditional function call, or conditional return. if threads
execute down different control flow paths, the threads are calleddivergent. if all of the threads
act in unison and follow a single control flow path, the threads are calleduniform. both
situations occur often in programs. a cta with divergent threads may have lower performance than a cta with uniformly executing threads,
so it is important to have divergent threads re-converge as soon as possible. all control constructs
are assumed to be divergent points unless the control-flow instruction is marked as uniform, using
the.unisuffix. for divergent control flow, the optimizing code generator automatically
determines points of re-convergence. therefore, a compiler or code author targeting ptx can ignore
the issue of divergent threads, but has the opportunity to improve performance by marking branch
points as uniform when the compiler or author can guarantee that the branch point is non-divergent.",5
9.6.semantics,#semantics,"9.6.semantics the goal of the semantic description of an instruction is to describe the results in all cases in as
simple language as possible. the semantics are described using c, until c is not expressive enough.",6
9.6.1.machine-specific semantics of 16-bit code,#machine-specific-semantics-of-16-bit-code,"9.6.1.machine-specific semantics of 16-bit code a ptx program may execute on a gpu with either a 16-bit or a 32-bit data path. when executing on a
32-bit data path, 16-bit registers in ptx are mapped to 32-bit physical registers, and 16-bit
computations arepromotedto 32-bit computations. this can lead to computational differences
between code run on a 16-bit machine versus the same code run on a 32-bit machine, since the
promoted computation may have bits in the high-order half-word of registers that are not present in
16-bit physical registers. these extra precision bits can become visible at the application level,
for example, by a right-shift instruction. at the ptx language level, one solution would be to define semantics for 16-bit code that is
consistent with execution on a 16-bit data path. this approach introduces a performance penalty for
16-bit code executing on a 32-bit data path, since the translated code would require many additional
masking instructions to suppress extra precision bits in the high-order half-word of 32-bit
registers. rather than introduce a performance penalty for 16-bit code running on 32-bit gpus, the semantics of
16-bit instructions in ptx is machine-specific. a compiler or programmer may chose to enforce
portable, machine-independent 16-bit semantics by adding explicit conversions to 16-bit values at
appropriate points in the program to guarantee portability of the code. however, for many
performance-critical applications, this is not desirable, and for many applications the difference
in execution is preferable to limiting performance.",7
9.7.instructions,#instructions,"9.7.instructions all ptx instructions may be predicated. in the following descriptions, the optional guard predicate
is omitted from the syntax.",7
9.7.1.integer arithmetic instructions,#integer-arithmetic-instructions,"9.7.1.integer arithmetic instructions integer arithmetic instructions operate on the integer types in register and constant immediate
forms. the integer arithmetic instructions are:",6
9.7.2.extended-precision integer arithmetic instructions,#extended-precision-integer-arithmetic-instructions,"9.7.2.extended-precision integer arithmetic instructions instructionsadd.cc,addc,sub.cc,subc,mad.ccandmadcreference an
implicitly specified condition code register (cc) having a single carry flag bit (cc.cf)
holding carry-in/carry-out or borrow-in/borrow-out. these instructions support extended-precision
integer addition, subtraction, and multiplication. no other instructions access the condition code,
and there is no support for setting, clearing, or testing the condition code. the condition code
register is not preserved across calls and is mainly intended for use in straight-line code
sequences for computing extended-precision integer addition, subtraction, and multiplication. the extended-precision arithmetic instructions are:",6
9.7.3.floating-point instructions,#floating-point-instructions,"9.7.3.floating-point instructions floating-point instructions operate on.f32and.f64register operands and constant
immediate values. the floating-point instructions are: instructions that support rounding modifiers are ieee-754 compliant. double-precision instructions
support subnormal inputs and results. single-precision instructions support subnormal inputs and
results by default forsm_20and subsequent targets, and flush subnormal inputs and results to
sign-preserving zero forsm_1xtargets. the optional.ftzmodifier on single-precision
instructions provides backward compatibility withsm_1xtargets by flushing subnormal inputs and
results to sign-preserving zero regardless of the target architecture. single-precisionadd,sub,mul, andmadsupport saturation of results to the range
[0.0, 1.0], withnans being flushed to positive zero.nanpayloads are supported for
double-precision instructions (except forrcp.approx.ftz.f64andrsqrt.approx.ftz.f64, which
maps inputnans to a canonicalnan). single-precision instructions return an unspecifiednan. note that future implementations may supportnanpayloads for single-precision
instructions, so ptx programs should not rely on the specific single-precisionnans being
generated. table 26summarizes
floating-point instructions in ptx.",6
9.7.4.half precision floating-point instructions,#half-precision-floating-point-instructions,"9.7.4.half precision floating-point instructions half precision floating-point instructions operate on.f16and.f16x2register operands. the
half precision floating-point instructions are: half-precisionadd,sub,mul, andfmasupport saturation of results to the range
[0.0, 1.0], withnans being flushed to positive zero. half-precision instructions return an
unspecifiednan.",6
9.7.5.comparison and selection instructions,#comparison-and-selection-instructions,"9.7.5.comparison and selection instructions the comparison select instructions are: as with single-precision floating-point instructions, theset,setp, andslctinstructions support subnormal numbers forsm_20and higher targets and flush single-precision
subnormal inputs to sign-preserving zero forsm_1xtargets. the optional.ftzmodifier
provides backward compatibility withsm_1xtargets by flushing subnormal inputs and results to
sign-preserving zero regardless of the target architecture.",6
9.7.6.half precision comparison instructions,#comparison--instructions,9.7.6.half precision comparison instructions the comparison instructions are:,9
9.7.7.logic and shift instructions,#logic-and-shift-instructions,"9.7.7.logic and shift instructions the logic and shift instructions are fundamentally untyped, performing bit-wise operations on
operands of any type, provided the operands are of the same size. this permits bit-wise operations
on floating point values without having to define a union to access the bits. instructionsand,or,xor, andnotalso operate on predicates. the logical shift instructions are:",6
9.7.8.data movement and conversion instructions,#data-movement-and-conversion-instructions,"9.7.8.data movement and conversion instructions these instructions copy data from place to place, and from state space to state space, possibly
converting it from one format to another.mov,ld,ldu, andstoperate on both
scalar and vector types. theisspacepinstruction is provided to query whether a generic address
falls within a particular state space window. thecvtainstruction converts addresses betweengenericandconst,global,local, orsharedstate spaces. instructionsld,st,suld, andsustsupport optional cache operations. the data movement and conversion instructions are:",6
9.7.9.texture instructions,#texture-instructions,"9.7.9.texture instructions this section describes ptx instructions for accessing textures and samplers. ptx supports the
following operations on texture and sampler descriptors:",7
9.7.10.surface instructions,#surface-instructions,"9.7.10.surface instructions this section describes ptx instructions for accessing surfaces. ptx supports the following
operations on surface descriptors: these instructions provide access to surface memory.",7
9.7.11.control flow instructions,#control-flow-instructions,9.7.11.control flow instructions the following ptx instructions and syntax are for controlling execution in a ptx program:,7
9.7.12.parallel synchronization and communication instructions,#parallel-synchronization-and-communication-instructions,9.7.12.parallel synchronization and communication instructions these instructions are:,9
9.7.13.warp level matrix multiply-accumulate instructions,#warp-level-matrix-instructions,9.7.13.warp level matrix multiply-accumulate instructions the matrix multiply and accumulate operation has the following form: wheredandcare called accumulators and may refer to the same matrix. ptx provides two ways to perform matrix multiply-and-accumulate computation:,1
9.7.14.asynchronous warpgroup level matrix multiply-accumulate instructions,#asynchronous-warpgroup-level-matrix-instructions,"9.7.14.asynchronous warpgroup level matrix multiply-accumulate instructions the warpgroup level matrix multiply and accumulate operation has either of the following forms,
where matrixdis called accumulator: thewgmmainstructions perform warpgroup level matrix multiply-and-accumulate operation by
having all threads in a warpgroup collectively perform the following actions:",1
9.7.15.stack manipulation instructions,#stack-manipulation-instructions,"9.7.15.stack manipulation instructions the stack manipulation instructions can be used to dynamically allocate and deallocate memory on the
stack frame of the current function. the stack manipulation instrucitons are:",6
9.7.16.video instructions,#video-instructions,"9.7.16.video instructions all video instructions operate on 32-bit register operands. however, the video instructions may be
classified as either scalar or simd based on whether their core operation applies to one or multiple
values. the video instructions are:",6
9.7.17.miscellaneous instructions,#miscellaneous-instructions,9.7.17.miscellaneous instructions the miscellaneous instructions are:,9
10.special registers,#special-registers,"10.special registers ptx includes a number of predefined, read-only variables, which are
visible as special registers and accessed throughmovorcvtinstructions. the special registers are:",7
10.1.special registers: %tid,#special-registers-tid,"10.1.special registers: %tid %tid thread identifier within a cta. syntax (predefined) description a predefined, read-only, per-thread special register initialized with the thread identifier within
the cta. the%tidspecial register contains a 1d, 2d, or 3d vector to match the cta shape; the%tidvalue in unused dimensions is0. the fourth element is unused and always returns
zero. the number of threads in each dimension are specified by the predefined special register%ntid. every thread in the cta has a unique%tid. %tidcomponent values range from0through%ntid-1in each cta dimension. %tid.y==%tid.z==0in 1d ctas.%tid.z==0in 2d ctas. it is guaranteed that: ptx isa notes introduced in ptx isa version 1.0 with type.v4.u16. redefined as type.v4.u32in ptx isa version 2.0. for compatibility with legacy ptx code, 16-bitmovandcvtinstructions may be used to read the lower 16-bits of each component of%tid. target isa notes supported on all target architectures. examples",7
10.2.special registers: %ntid,#special-registers-ntid,"10.2.special registers: %ntid %ntid number of thread ids per cta. syntax (predefined) description a predefined, read-only special register initialized with the number of thread ids in each cta
dimension. the%ntidspecial register contains a 3d cta shape vector that holds the cta
dimensions. cta dimensions are non-zero; the fourth element is unused and always returns zero. the
total number of threads in a cta is(%ntid.x*%ntid.y*%ntid.z). maximum values of %ntid.{x,y,z} are as follows: ptx isa notes introduced in ptx isa version 1.0 with type.v4.u16. redefined as type.v4.u32in ptx isa version 2.0. for compatibility with legacy ptx code, 16-bitmovandcvtinstructions may be used to read the lower 16-bits of each component of%ntid. target isa notes supported on all target architectures. examples",7
10.3.special registers: %laneid,#special-registers-laneid,"10.3.special registers: %laneid %laneid lane identifier. syntax (predefined) description a predefined, read-only special register that returns the threads lane within the warp. the lane
identifier ranges from zero towarp_sz-1. ptx isa notes introduced in ptx isa version 1.3. target isa notes supported on all target architectures. examples",7
10.4.special registers: %warpid,#special-registers-warpid,"10.4.special registers: %warpid %warpid warp identifier. syntax (predefined) description a predefined, read-only special register that returns the threads warp identifier. the warp
identifier provides a unique warp number within a cta but not across ctas within a grid. the warp
identifier will be the same for all threads within a single warp. note that%warpidis volatile and returns the location of a thread at the moment when read, but
its value may change during execution, e.g., due to rescheduling of threads following
preemption. for this reason,%ctaidand%tidshould be used to compute a virtual warp index
if such a value is needed in kernel code;%warpidis intended mainly to enable profiling and
diagnostic code to sample and log information such as work place mapping and load distribution. ptx isa notes introduced in ptx isa version 1.3. target isa notes supported on all target architectures. examples",7
10.5.special registers: %nwarpid,#special-registers-nwarpid,"10.5.special registers: %nwarpid %nwarpid number of warp identifiers. syntax (predefined) description a predefined, read-only special register that returns the maximum number of warp identifiers. ptx isa notes introduced in ptx isa version 2.0. target isa notes %nwarpidrequiressm_20or higher. examples",7
10.6.special registers: %ctaid,#special-registers-ctaid,"10.6.special registers: %ctaid %ctaid cta identifier within a grid. syntax (predefined) description a predefined, read-only special register initialized with the cta identifier within the cta
grid. the%ctaidspecial register contains a 1d, 2d, or 3d vector, depending on the shape and
rank of the cta grid. the fourth element is unused and always returns zero. it is guaranteed that: ptx isa notes introduced in ptx isa version 1.0 with type.v4.u16. redefined as type.v4.u32in ptx isa version 2.0. for compatibility with legacy ptx code, 16-bitmovandcvtinstructions may be used to read the lower 16-bits of each component of%ctaid. target isa notes supported on all target architectures. examples",7
10.7.special registers: %nctaid,#special-registers-nctaid,"10.7.special registers: %nctaid %nctaid number of cta ids per grid. syntax (predefined) description a predefined, read-only special register initialized with the number of ctas in each grid
dimension. the%nctaidspecial register contains a 3d grid shape vector, with each element
having a value of at least1. the fourth element is unused and always returns zero. maximum values of %nctaid.{x,y,z} are as follows: ptx isa notes introduced in ptx isa version 1.0 with type.v4.u16. redefined as type.v4.u32in ptx isa version 2.0. for compatibility with legacy ptx code, 16-bitmovandcvtinstructions may be used to read the lower 16-bits of each component of%nctaid. target isa notes supported on all target architectures. examples",7
10.8.special registers: %smid,#special-registers-smid,"10.8.special registers: %smid %smid sm identifier. syntax (predefined) description a predefined, read-only special register that returns the processor (sm) identifier on which a
particular thread is executing. the sm identifier ranges from0to%nsmid-1. the sm
identifier numbering is not guaranteed to be contiguous. notes note that%smidis volatile and returns the location of a thread at the moment when read, but
its value may change during execution, e.g. due to rescheduling of threads following
preemption.%smidis intended mainly to enable profiling and diagnostic code to sample and log
information such as work place mapping and load distribution. ptx isa notes introduced in ptx isa version 1.3. target isa notes supported on all target architectures. examples",7
10.9.special registers: %nsmid,#special-registers-nsmid,"10.9.special registers: %nsmid %nsmid number of sm identifiers. syntax (predefined) description a predefined, read-only special register that returns the maximum number of sm identifiers. the sm
identifier numbering is not guaranteed to be contiguous, so%nsmidmay be larger than the
physical number of sms in the device. ptx isa notes introduced in ptx isa version 2.0. target isa notes %nsmidrequiressm_20or higher. examples",7
10.10.special registers: %gridid,#special-registers-gridid,"10.10.special registers: %gridid %gridid grid identifier. syntax (predefined) description a predefined, read-only special register initialized with the per-grid temporal grid identifier. the%grididis used by debuggers to distinguish ctas and clusters within concurrent (small) grids. during execution, repeated launches of programs may occur, where each launch starts a
grid-of-ctas. this variable provides the temporal grid launch number for this context. forsm_1xtargets,%grididis limited to the range [0..216-1]. forsm_20,%grididis limited to the range [0..232-1].sm_30supports the entire 64-bit range. ptx isa notes introduced in ptx isa version 1.0 as type.u16. redefined as type.u32in ptx isa version 1.3. redefined as type.u64in ptx isa version 3.0. for compatibility with legacy ptx code, 16-bit and 32-bitmovandcvtinstructions may be
used to read the lower 16-bits or 32-bits of each component of%gridid. target isa notes supported on all target architectures. examples",7
10.11.special registers: %is_explicit_cluster,#special-registers-is-explicit-cluster,"10.11.special registers: %is_explicit_cluster %is_explicit_cluster checks if user has explicitly specified cluster launch. syntax (predefined) description a predefined, read-only special register initialized with the predicate value of whether the cluster
launch is explicitly specified by user. ptx isa notes introduced in ptx isa version 7.8. target isa notes requiressm_90or higher. examples",7
10.12.special registers: %clusterid,#special-registers-clusterid,"10.12.special registers: %clusterid %clusterid cluster identifier within a grid. syntax (predefined) description a predefined, read-only special register initialized with the cluster identifier in a grid in each
dimension. each cluster in a grid has a unique identifier. the%clusteridspecial register contains a 1d, 2d, or 3d vector, depending upon the shape and
rank of the cluster. the fourth element is unused and always returns zero. it is guaranteed that: ptx isa notes introduced in ptx isa version 7.8. target isa notes requiressm_90or higher. examples",7
10.13.special registers: %nclusterid,#special-registers-nclusterid,"10.13.special registers: %nclusterid %nclusterid number of cluster identifiers per grid. syntax (predefined) description a predefined, read-only special register initialized with the number of clusters in each grid
dimension. the%nclusteridspecial register contains a 3d grid shape vector that holds the grid dimensions
in terms of clusters. the fourth element is unused and always returns zero. refer to thecuda programming guidefor details on the maximum values of%nclusterid.{x,y,z}. ptx isa notes introduced in ptx isa version 7.8. target isa notes requiressm_90or higher. examples",7
10.14.special registers: %cluster_ctaid,#special-registers-cluster-ctaid,"10.14.special registers: %cluster_ctaid %cluster_ctaid cta identifier within a cluster. syntax (predefined) description a predefined, read-only special register initialized with the cta identifier in a cluster in each
dimension. each cta in a cluster has a unique cta identifier. the%cluster_ctaidspecial register contains a 1d, 2d, or 3d vector, depending upon the shape of
the cluster. the fourth element is unused and always returns zero. it is guaranteed that: ptx isa notes introduced in ptx isa version 7.8. target isa notes requiressm_90or higher. examples",7
10.15.special registers: %cluster_nctaid,#special-registers-cluster-nctaid,"10.15.special registers: %cluster_nctaid %cluster_nctaid number of cta identifiers per cluster. syntax (predefined) description a predefined, read-only special register initialized with the number of ctas in a cluster in each
dimension. the%cluster_nctaidspecial register contains a 3d grid shape vector that holds the cluster
dimensions in terms of ctas. the fourth element is unused and always returns zero. refer to thecuda programming guidefor details on the maximum values of%cluster_nctaid.{x,y,z}. ptx isa notes introduced in ptx isa version 7.8. target isa notes requiressm_90or higher. examples",7
10.16.special registers: %cluster_ctarank,#special-registers-cluster-ctarank,"10.16.special registers: %cluster_ctarank %cluster_ctarank cta identifier in a cluster across all dimensions. syntax (predefined) description a predefined, read-only special register initialized with the cta rank within a cluster across all
dimensions. it is guaranteed that: ptx isa notes introduced in ptx isa version 7.8. target isa notes requiressm_90or higher. examples",7
10.17.special registers: %cluster_nctarank,#special-registers-cluster-nctarank,"10.17.special registers: %cluster_nctarank %cluster_nctarank number of cta identifiers in a cluster across all dimensions. syntax (predefined) description a predefined, read-only special register initialized with the nunber of ctas within a cluster across
all dimensions. ptx isa notes introduced in ptx isa version 7.8. target isa notes requiressm_90or higher. examples",7
10.18.special registers: %lanemask_eq,#special-registers-lanemask-eq,"10.18.special registers: %lanemask_eq %lanemask_eq 32-bit mask with bit set in position equal to the threads lane number in the warp. syntax (predefined) description a predefined, read-only special register initialized with a 32-bit mask with a bit set in the
position equal to the threads lane number in the warp. ptx isa notes introduced in ptx isa version 2.0. target isa notes %lanemask_eqrequiressm_20or higher. examples",7
10.19.special registers: %lanemask_le,#special-registers-lanemask-le,"10.19.special registers: %lanemask_le %lanemask_le 32-bit mask with bits set in positions less than or equal to the threads lane number in the warp. syntax (predefined) description a predefined, read-only special register initialized with a 32-bit mask with bits set in positions
less than or equal to the threads lane number in the warp. ptx isa notes introduced in ptx isa version 2.0. target isa notes %lanemask_lerequiressm_20or higher. examples",7
10.20.special registers: %lanemask_lt,#special-registers-lanemask-lt,"10.20.special registers: %lanemask_lt %lanemask_lt 32-bit mask with bits set in positions less than the threads lane number in the warp. syntax (predefined) description a predefined, read-only special register initialized with a 32-bit mask with bits set in positions
less than the threads lane number in the warp. ptx isa notes introduced in ptx isa version 2.0. target isa notes %lanemask_ltrequiressm_20or higher. examples",7
10.21.special registers: %lanemask_ge,#special-registers-lanemask-ge,"10.21.special registers: %lanemask_ge %lanemask_ge 32-bit mask with bits set in positions greater than or equal to the threads lane number in the warp. syntax (predefined) description a predefined, read-only special register initialized with a 32-bit mask with bits set in positions
greater than or equal to the threads lane number in the warp. ptx isa notes introduced in ptx isa version 2.0. target isa notes %lanemask_gerequiressm_20or higher. examples",7
10.22.special registers: %lanemask_gt,#special-registers-lanemask-gt,"10.22.special registers: %lanemask_gt %lanemask_gt 32-bit mask with bits set in positions greater than the threads lane number in the warp. syntax (predefined) description a predefined, read-only special register initialized with a 32-bit mask with bits set in positions
greater than the threads lane number in the warp. ptx isa notes introduced in ptx isa version 2.0. target isa notes %lanemask_gtrequiressm_20or higher. examples",7
"10.23.special registers: %clock, %clock_hi",#special-registers-clock,"10.23.special registers: %clock, %clock_hi %clock, %clock_hi syntax (predefined) description special register%clockand%clock_hiare unsigned 32-bit read-only cycle counters that wrap
silently. ptx isa notes %clockintroduced in ptx isa version 1.0. %clock_hiintroduced in ptx isa version 5.0. target isa notes %clocksupported on all target architectures. %clock_hirequiressm_20or higher. examples",7
10.24.special registers: %clock64,#special-registers-clock64,"10.24.special registers: %clock64 %clock64 a predefined, read-only 64-bit unsigned cycle counter. syntax (predefined) description special register%clock64is an unsigned 64-bit read-only cycle counter that wraps silently. notes the lower 32-bits of%clock64are identical to%clock. the upper 32-bits of%clock64are identical to%clock_hi. ptx isa notes introduced in ptx isa version 2.0. target isa notes %clock64requiressm_20or higher. examples",7
10.25.special registers: %pm0..%pm7,#special-registers-pm0-pm7,"10.25.special registers: %pm0..%pm7 %pm0..%pm7 performance monitoring counters. syntax (predefined) description special registers%pm0..%pm7are unsigned 32-bit read-only performance monitor counters. their
behavior is currently undefined. ptx isa notes %pm0..%pm3introduced in ptx isa version 1.3. %pm4..%pm7introduced in ptx isa version 3.0. target isa notes %pm0..%pm3supported on all target architectures. %pm4..%pm7requiresm_20or higher. examples",7
10.26.special registers: %pm0_64..%pm7_64,#special-registers-pm0_64-pm7_64,"10.26.special registers: %pm0_64..%pm7_64 %pm0_64..%pm7_64 64 bit performance monitoring counters. syntax (predefined) description special registers%pm0_64..%pm7_64are unsigned 64-bit read-only performance monitor
counters. their behavior is currently undefined. notes the lower 32bits of%pm0_64..%pm7_64are identical to%pm0..%pm7. ptx isa notes %pm0_64..%pm7_64introduced in ptx isa version 4.0. target isa notes %pm0_64..%pm7_64requiresm_50or higher. examples",7
10.27.special registers: %envreg<32>,#special-registers-envreg-32,"10.27.special registers: %envreg<32> %envreg<32> driver-defined read-only registers. syntax (predefined) description a set of 32 pre-defined read-only registers used to capture execution environment of ptx program
outside of ptx virtual machine. these registers are initialized by the driver prior to kernel launch
and can contain cta-wide or grid-wide values. precise semantics of these registers is defined in the driver documentation. ptx isa notes introduced in ptx isa version 2.1. target isa notes supported on all target architectures. examples",7
"10.28.special registers: %globaltimer, %globaltimer_lo, %globaltimer_hi",#special-registers-globaltimer,"10.28.special registers: %globaltimer, %globaltimer_lo, %globaltimer_hi %globaltimer, %globaltimer_lo, %globaltimer_hi syntax (predefined) description special registers intended for use by nvidia tools. the behavior is target-specific and may change
or be removed in future gpus. when jit-compiled to other targets, the value of these registers is
unspecified. ptx isa notes introduced in ptx isa version 3.1. target isa notes requires targetsm_30or higher. examples",7
"10.29.special registers: %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_<2>",#special-registers-reserved-smem,"10.29.special registers: %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_<2> %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_<2> syntax (predefined) description these are predefined, read-only special registers containing information about the shared memory
region which is reserved for the nvidia system software use. this region of shared memory is not
available to users, and accessing this region from user code results in undefined behavior. refer tocuda programming guidefor details. ptx isa notes introduced in ptx isa version 7.6. target isa notes requiresm_80or higher. examples",7
10.30.special registers: %total_smem_size,#special-registers-total-smem-size,"10.30.special registers: %total_smem_size %total_smem_size total size of shared memory used by a cta of a kernel. syntax (predefined) description a predefined, read-only special register initialized with total size of shared memory allocated
(statically and dynamically, excluding the shared memory reserved for the nvidia system software
use) for the cta of a kernel at launch time. size is returned in multiples of shared memory allocation unit size supported by target
architecture. allocation unit values are as follows: ptx isa notes introduced in ptx isa version 4.1. target isa notes requiressm_20or higher. examples",7
10.31.special registers: %aggr_smem_size,#special-registers-aggr-smem-size,"10.31.special registers: %aggr_smem_size %aggr_smem_size total size of shared memory used by a cta of a kernel. syntax (predefined) description a predefined, read-only special register initialized with total aggregated size of shared memory
consisting of the size of user shared memory allocated (statically and dynamically) at launch time
and the size of shared memory region which is reserved for the nvidia system software use. ptx isa notes introduced in ptx isa version 8.1. target isa notes requiressm_90or higher. examples",7
10.32.special registers: %dynamic_smem_size,#special-registers-dynamic-smem-size,"10.32.special registers: %dynamic_smem_size %dynamic_smem_size size of shared memory allocated dynamically at kernel launch. syntax (predefined) description size of shared memory allocated dynamically at kernel launch. a predefined, read-only special register initialized with size of shared memory allocated dynamically for the cta of a kernel at launch time. ptx isa notes introduced in ptx isa version 4.1. target isa notes requiressm_20or higher. examples",7
10.33.special registers: %current_graph_exec,#special-registers-current-graph-exec,"10.33.special registers: %current_graph_exec %current_graph_exec an identifier for currently executing cuda device graph. syntax (predefined) description a predefined, read-only special register initialized with the identifier referring to the cuda
device graph being currently executed. this register is 0 if the executing kernel is not part of a
cuda device graph. refer to thecuda programming guidefor more details on cuda device graphs. ptx isa notes introduced in ptx isa version 8.0. target isa notes requiressm_50or higher. examples",7
11.directives,#directives,11.directives,9
11.1.ptx module directives,#ptx-module-directives,"11.1.ptx module directives the following directives declare the ptx isa version of the code in the module, the target
architecture for which the code was generated, and the size of addresses within the ptx module.",7
11.1.1.ptx module directives: .version,#ptx-module-directives-version,"11.1.1.ptx module directives: .version .version ptx isa version number. syntax description specifies the ptx language version number. themajornumber is incremented when there are incompatible changes to the ptx language, such as
changes to the syntax or semantics. the version major number is used by the ptx compiler to ensure
correct execution of legacy ptx code. theminornumber is incremented when new features are added to ptx. semantics indicates that this module must be compiled with tools that support an equal or greater version
number. each ptx module must begin with a.versiondirective, and no other.versiondirective is
allowed anywhere else within the module. ptx isa notes introduced in ptx isa version 1.0. target isa notes supported on all target architectures. examples",7
11.1.2.ptx module directives: .target,#ptx-module-directives-target,"11.1.2.ptx module directives: .target .target architecture and platform target. syntax description specifies the set of features in the target architecture for which the current ptx code was
generated. in general, generations of sm architectures follow anonion layermodel, where each
generation adds new features and retains all features of previous generations. the onion layer model
allows the ptx code generated for a given target to be run on later generation devices. target architectures with suffix a, such assm_90a, include architecture-accelerated
features that are supported on the specified architecture only, hence such targets do not follow the
onion layer model. therefore, ptx code generated for such targets cannot be run on later generation
devices. architecture-accelerated features can only be used with targets that support these
features. semantics each ptx module must begin with a.versiondirective, immediately followed by a.targetdirective containing a target architecture and optional platform options. a.targetdirective
specifies a single target architecture, but subsequent.targetdirectives can be used to change
the set of target features allowed during parsing. a program with multiple.targetdirectives
will compile and run only on devices that support all features of the highest-numbered architecture
listed in the program. ptx features are checked against the specified target architecture, and an error is generated if an
unsupported feature is used. the following table summarizes the features in ptx that vary according
to target architecture. the texturing mode is specified for an entire module and cannot be changed within the module. the.targetdebug option declares that the ptx file contains dwarf debug information, and
subsequent compilation of ptx will retain information needed for source-level debugging. if the
debug option is declared, an error message is generated if no dwarf information is found in the
file. the debug option requires ptx isa version 3.0 or later. map_f64_to_f32indicates that all double-precision instructions map to single-precision
regardless of the target architecture. this enables high-level language compilers to compile
programs containing type double to target device that do not support double-precision
operations. note that.f64storage remains as 64-bits, with only half being used by instructions
converted from.f64to.f32. notes targets of the formcompute_xxare also accepted as synonyms forsm_xxtargets. ptx isa notes introduced in ptx isa version 1.0. target stringssm_10andsm_11introduced in ptx isa version 1.0. target stringssm_12andsm_13introduced in ptx isa version 1.2. texturing mode introduced in ptx isa version 1.5. target stringsm_20introduced in ptx isa version 2.0. target stringsm_30introduced in ptx isa version 3.0. platform optiondebugintroduced in ptx isa version 3.0. target stringsm_35introduced in ptx isa version 3.1. target stringssm_32andsm_50introduced in ptx isa version 4.0. target stringssm_37andsm_52introduced in ptx isa version 4.1. target stringsm_53introduced in ptx isa version 4.2. target stringsm_60,sm_61,sm_62introduced in ptx isa version 5.0. target stringsm_70introduced in ptx isa version 6.0. target stringsm_72introduced in ptx isa version 6.1. target stringsm_75introduced in ptx isa version 6.3. target stringsm_80introduced in ptx isa version 7.0. target stringsm_86introduced in ptx isa version 7.1. target stringsm_87introduced in ptx isa version 7.4. target stringsm_89introduced in ptx isa version 7.8. target stringsm_90introduced in ptx isa version 7.8. target stringsm_90aintroduced in ptx isa version 8.0. target isa notes the.targetdirective is supported on all target architectures. examples",7
11.1.3.ptx module directives: .address_size,#ptx-module-directives-address-size,"11.1.3.ptx module directives: .address_size .address_size address size used throughout ptx module. syntax description specifies the address size assumed throughout the module by the ptx code and the binary dwarf
information in ptx. redefinition of this directive within a module is not allowed. in the presence of separate
compilation all modules must specify (or default to) the same address size. the.address_sizedirective is optional, but it must immediately follow the.targetdirective if present within a module. semantics if the.address_sizedirective is omitted, the address size defaults to 32. ptx isa notes introduced in ptx isa version 2.3. target isa notes supported on all target architectures. examples",7
11.2.specifying kernel entry points and functions,#specifying-kernel-entry-points-and-functions,11.2.specifying kernel entry points and functions the following directives specify kernel entry points and functions.,6
11.2.1.kernel and function directives: .entry,#kernel-and-function-directives-entry,"11.2.1.kernel and function directives: .entry .entry kernel entry point and body, with optional parameters. syntax description defines a kernel entry point name, parameters, and body for the kernel function. parameters are passed via.paramspace memory and are listed within an optional parenthesized
parameter list. parameters may be referenced by name within the kernel body and loaded into
registers usingld.param{::entry}instructions. in addition to normal parameters, opaque.texref,.samplerref, and.surfrefvariables
may be passed as parameters. these parameters can only be referenced by name within texture and
surface load, store, and query instructions and cannot be accessed viald.paraminstructions. the shape and size of the cta executing the kernel are available in special registers. semantics specify the entry point for a kernel program. at kernel launch, the kernel dimensions and properties are established and made available via
special registers, e.g.,%ntid,%nctaid, etc. ptx isa notes for ptx isa version 1.4 and later, parameter variables are declared in the kernel parameter
list. for ptx isa versions 1.0 through 1.3, parameter variables are declared in the kernel body. the maximum memory size supported by ptx for normal (non-opaque type) parameters is 32764
bytes. depending upon the ptx isa version, the parameter size limit varies. the following table
shows the allowed parameter size for a ptx isa version: the cuda and opencl drivers support the following limits for parameter memory: target isa notes supported on all target architectures. examples",7
11.2.2.kernel and function directives: .func,#kernel-and-function-directives-func,"11.2.2.kernel and function directives: .func .func function definition. syntax description defines a function, including input and return parameters and optional function body. an optional.noreturndirective indicates that the function does not return to the caller
function..noreturndirective cannot be specified on functions which have return parameters. see
the description of.noreturndirective inperformance-tuning directives: .noreturn. an optional.attributedirective specifies additional information associated with the
function. see the description ofvariable and function attribute directive: .attributefor allowed attributes. a.funcdefinition with no body provides a function prototype. the parameter lists define locally-scoped variables in the function body. parameters must be base
types in either the register or parameter state space. parameters in register state space may be
referenced directly within instructions in the function body. parameters in.paramspace are
accessed usingld.param{::func}andst.param{::func}instructions in the body. parameter
passing is call-by-value. the last parameter in the parameter list may be a.paramarray of type.b8with no size
specified. it is used to pass an arbitrary number of parameters to the function packed into a single
array object. when calling a function with such an unsized last argument, the last argument may be omitted from
thecallinstruction if no parameter is passed through it. accesses to this array parameter must
be within the bounds of the array. the result of an access is undefined if no array was passed, or
if the access was outside the bounds of the actual array being passed. semantics the ptx syntax hides all details of the underlying calling convention and abi. the implementation of parameter passing is left to the optimizing translator, which may use a
combination of registers and stack locations to pass parameters. release notes for ptx isa version 1.x code, parameters must be in the register state space, there is no stack, and
recursion is illegal. ptx isa versions 2.0 and later with targetsm_20or higher allow parameters in the.paramstate space, implements an abi with stack, and supports recursion. ptx isa versions 2.0 and later with targetsm_20or higher support at most one return value. ptx isa notes introduced in ptx isa version 1.0. support for unsized array parameter introduced in ptx isa version 6.0. support for.noreturndirective introduced in ptx isa version 6.4. support for.attributedirective introduced in ptx isa version 8.0. target isa notes functions without unsized array parameter supported on all target architectures. unsized array parameter requiressm_30or higher. .noreturndirective requiressm_30or higher. .attributedirective requiressm_90or higher. examples",6
11.2.3.kernel and function directives: .alias,#kernel-and-function-directives-alias,"11.2.3.kernel and function directives: .alias .alias define an alias to existing function symbol. syntax description .aliasis a module scope directive that defines identifierfaliasto be an alias to function
specified byfaliasee. bothfaliasandfaliaseeare non-entry function symbols. identifierfaliasis a function declaration without body. identifierfaliaseeis a function symbol which must be defined in the same module as.aliasdeclaration. functionfaliaseecannot have.weaklinkage. prototype offaliasandfaliaseemust match. program can use eitherfaliasorfaliseeidentifiers to reference function defined withfaliasee. ptx isa notes .aliasdirective introduced in ptx isa 6.3. target isa notes .aliasdirective requiressm_30or higher. examples",7
11.3.control flow directives,#control-flow-directives,11.3.control flow directives ptx provides directives for specifying potential targets forbrx.idxandcallinstructions. see the descriptions ofbrx.idxandcallfor more information.,7
11.3.1.control flow directives: .branchtargets,#control-flow-directives-branchtargets,"11.3.1.control flow directives: .branchtargets .branchtargets declare a list of potential branch targets. syntax description declares a list of potential branch targets for a subsequentbrx.idx, and associates the list
with the label at the start of the line. all control flow labels in the list must occur within the same function as the declaration. the list of labels may use the compact, shorthand syntax for enumerating a range of labels having a
common prefix, similar to the syntax described inparameterized variable names. ptx isa notes introduced in ptx isa version 2.1. target isa notes requiressm_20or higher. examples",7
11.3.2.control flow directives: .calltargets,#control-flow-directives-calltargets,"11.3.2.control flow directives: .calltargets .calltargets declare a list of potential call targets. syntax description declares a list of potential call targets for a subsequent indirect call, and associates the list
with the label at the start of the line. all functions named in the list must be declared prior to the.calltargetsdirective, and all
functions must have the same type signature. ptx isa notes introduced in ptx isa version 2.1. target isa notes requiressm_20or higher. examples",7
11.3.3.control flow directives: .callprototype,#control-flow-directives-callprototype,"11.3.3.control flow directives: .callprototype .callprototype declare a prototype for use in an indirect call. syntax description defines a prototype with no specific function name, and associates the prototype with a label. the
prototype may then be used in indirect call instructions where there is incomplete knowledge of the
possible call targets. parameters may have either base types in the register or parameter state spaces, or array types in
parameter state space. the sink symbol'_'may be used to avoid dummy parameter names. an optional.noreturndirective indicates that the function does not return to the caller
function..noreturndirective cannot be specified on functions which have return parameters. see
the description of .noreturn directive inperformance-tuning directives: .noreturn. ptx isa notes introduced in ptx isa version 2.1. support for.noreturndirective introduced in ptx isa version 6.4. target isa notes requiressm_20or higher. .noreturndirective requiressm_30or higher. examples",7
11.4.performance-tuning directives,#performance-tuning-directives,"11.4.performance-tuning directives to provide a mechanism for low-level performance tuning, ptx supports the following directives,
which pass information to the backend optimizing compiler. the.maxnregdirective specifies the maximum number of registers to be allocated to a single
thread; the.maxntiddirective specifies the maximum number of threads in a thread block (cta);
the.reqntiddirective specifies the required number of threads in a thread block (cta); and the.minnctapersmdirective specifies a minimum number of thread blocks to be scheduled on a single
multiprocessor (sm). these can be used, for example, to throttle the resource requirements (e.g.,
registers) to increase total thread count and provide a greater opportunity to hide memory
latency. the.minnctapersmdirective can be used together with either the.maxntidor.reqntiddirective to trade-off registers-per-thread against multiprocessor utilization without
needed to directly specify a maximum number of registers. this may achieve better performance when
compiling ptx for multiple devices having different numbers of registers per sm. currently, the.maxnreg,.maxntid,.reqntid, and.minnctapersmdirectives may be
applied per-entry and must appear between an.entrydirective and its body. the directives take
precedence over any module-level constraints passed to the optimizing backend. a warning message is
generated if the directives constraints are inconsistent or cannot be met for the specified target
device. a general.pragmadirective is supported for passing information to the ptx backend. the
directive passes a list of strings to the backend, and the strings have no semantics within the ptx
virtual machine model. the interpretation of.pragmavalues is determined by the backend
implementation and is beyond the scope of the ptx isa. note that.pragmadirectives may appear
at module (file) scope, at entry-scope, or as statements within a kernel or device function body.",7
11.4.1.performance-tuning directives: .maxnreg,#performance-tuning-directives-maxnreg,"11.4.1.performance-tuning directives: .maxnreg .maxnreg maximum number of registers that can be allocated per thread. syntax description declare the maximum number of registers per thread in a cta. semantics the compiler guarantees that this limit will not be exceeded. the actual number of registers used
may be less; for example, the backend may be able to compile to fewer registers, or the maximum
number of registers may be further constrained by.maxntidand.maxctapersm. ptx isa notes introduced in ptx isa version 1.3. target isa notes supported on all target architectures. examples",7
11.4.2.performance-tuning directives: .maxntid,#performance-tuning-directives-maxntid,"11.4.2.performance-tuning directives: .maxntid .maxntid maximum number of threads in the thread block (cta). syntax description declare the maximum number of threads in the thread block (cta). this maximum is specified by giving
the maximum extent of each dimension of the 1d, 2d, or 3d cta. the maximum number of threads is the
product of the maximum extent in each dimension. semantics the maximum number of threads in the thread block, computed as the product of the maximum extent
specified for each dimension, is guaranteed not to be exceeded in any invocation of the kernel in
which this directive appears. exceeding the maximum number of threads results in a runtime error or
kernel launch failure. note that this directive guarantees that thetotalnumber of threads does not exceed the maximum,
but does not guarantee that the limit in any particular dimension is not exceeded. ptx isa notes introduced in ptx isa version 1.3. target isa notes supported on all target architectures. examples",7
11.4.3.performance-tuning directives: .reqntid,#performance-tuning-directives-reqntid,"11.4.3.performance-tuning directives: .reqntid .reqntid number of threads in the thread block (cta). syntax description declare the number of threads in the thread block (cta) by specifying the extent of each dimension
of the 1d, 2d, or 3d cta. the total number of threads is the product of the number of threads in
each dimension. semantics the size of each cta dimension specified in any invocation of the kernel is required to be equal to
that specified in this directive. specifying a different cta dimension at launch will result in a
runtime error or kernel launch failure. notes the.reqntiddirective cannot be used in conjunction with the.maxntiddirective. ptx isa notes introduced in ptx isa version 2.1. target isa notes supported on all target architectures. examples",7
11.4.4.performance-tuning directives: .minnctapersm,#performance-tuning-directives-minnctapersm,"11.4.4.performance-tuning directives: .minnctapersm .minnctapersm minimum number of ctas per sm. syntax description declare the minimum number of ctas from the kernels grid to be mapped to a single multiprocessor
(sm). notes optimizations based on.minnctapersmneed either.maxntidor.reqntidto be specified as
well. if the total number of threads on a single sm resulting from.minnctapersmand.maxntid/.reqntidexceed maximum number of threads supported by an sm then directive.minnctapersmwill be ignored. in ptx isa version 2.1 or higher, a warning is generated if.minnctapersmis specified without
specifying either.maxntidor.reqntid. ptx isa notes introduced in ptx isa version 2.0 as a replacement for.maxnctapersm. target isa notes supported on all target architectures. examples",7
11.4.5.performance-tuning directives: .maxnctapersm (deprecated),#performance-tuning-directives-maxnctapersm,"11.4.5.performance-tuning directives: .maxnctapersm (deprecated) .maxnctapersm maximum number of ctas per sm. syntax description declare the maximum number of ctas from the kernels grid that may be mapped to a single
multiprocessor (sm). notes optimizations based on .maxnctapersm generally need.maxntidto be specified as well. the
optimizing backend compiler uses.maxntidand.maxnctapersmto compute an upper-bound on
per-thread register usage so that the specified number of ctas can be mapped to a single
multiprocessor. however, if the number of registers used by the backend is sufficiently lower than
this bound, additional ctas may be mapped to a single multiprocessor. for this reason,.maxnctapersmhas been renamed to .minnctapersm in ptx isa version 2.0. ptx isa notes introduced in ptx isa version 1.3. deprecated in ptx isa version 2.0. target isa notes supported on all target architectures. examples",7
11.4.6.performance-tuning directives: .noreturn,#performance-tuning-directives-noreturn,"11.4.6.performance-tuning directives: .noreturn .noreturn indicate that the function does not return to its caller function. syntax description indicate that the function does not return to its caller function. semantics an optional.noreturndirective indicates that the function does not return to caller
function..noreturndirective can only be specified on device functions and must appear between
a.funcdirective and its body. the directive cannot be specified on functions which have return parameters. if a function with.noreturndirective returns to the caller function at runtime, then the
behavior is undefined. ptx isa notes introduced in ptx isa version 6.4. target isa notes requiressm_30or higher. examples",7
11.4.7.performance-tuning directives: .pragma,#performance-tuning-directives-pragma,"11.4.7.performance-tuning directives: .pragma .pragma pass directives to ptx backend compiler. syntax description pass module-scoped, entry-scoped, or statement-level directives to the ptx backend compiler. the.pragmadirective may occur at module-scope, at entry-scope, or at statement-level. semantics the interpretation of.pragmadirective strings is implementation-specific and has no impact on
ptx semantics. seedescriptions of .pragma stringsfor
descriptions of the pragma strings defined inptxas. ptx isa notes introduced in ptx isa version 2.0. target isa notes supported on all target architectures. examples",7
11.5.debugging directives,#debugging-directives,"11.5.debugging directives dwarf-format debug information is passed through ptx modules using the following directives: the.sectiondirective was introduced in ptx isa version 2.0 and replaces the@@dwarfsyntax. the@@dwarfsyntax was deprecated in ptx isa version 2.0 but is supported for legacy ptx
isa version 1.x code. beginning with ptx isa version 3.0, ptx files containing dwarf debug information should include the.targetdebugplatform option. this forward declaration directs ptx compilation to retain
mappings for source-level debugging.",7
11.5.1.debugging directives: @@dwarf,#debugging-directives-atatdwarf,"11.5.1.debugging directives: @@dwarf @@dwarf dwarf-format information. syntax ptx isa notes introduced in ptx isa version 1.2. deprecated as of ptx isa version 2.0, replaced by.sectiondirective. target isa notes supported on all target architectures. examples",7
11.5.2.debugging directives: .section,#debugging-directives-section,"11.5.2.debugging directives: .section .section ptx section definition. syntax ptx isa notes introduced in ptx isa version 2.0, replaces@@dwarfsyntax. label+imm expression introduced in ptx isa version 3.2. support for.b16integers in dwarf-lines introduced in ptx isa version 6.0. support for defininglabelinside the dwarf section is introduced in ptx isa version 7.2. label1-label2expression introduced in ptx isa version 7.5. negative numbers in dwarf lines introduced in ptx isa version 7.5. target isa notes supported on all target architectures. examples",7
11.5.3.debugging directives: .file,#debugging-directives-file,"11.5.3.debugging directives: .file .file source file name. syntax description associates a source filename with an integer index..locdirectives reference source files by
index. .filedirective allows optionally specifying an unsigned number representing time of last
modification and an unsigned integer representing size in bytes of source file.timestampandfile_sizevalue can be 0 to indicate this information is not available. timestampvalue is in format of c and c++ data typetime_t. file_sizeis an unsigned 64-bit integer. the.filedirective is allowed only in the outermost scope, i.e., at the same level as kernel
and device function declarations. semantics if timestamp and file size are not specified, they default to 0. ptx isa notes introduced in ptx isa version 1.0. timestamp and file size introduced in ptx isa version 3.2. target isa notes supported on all target architectures. examples",7
11.5.4.debugging directives: .loc,#debugging-directives-loc,"11.5.4.debugging directives: .loc .loc source file location. syntax description declares the source file location (source file, line number, and column position) to be associated
with lexically subsequent ptx instructions..locrefers tofile_indexwhich is defined by a.filedirective. to indicate ptx instructions that are generated from a function that got inlined, additional
attribute.inlined_atcan be specified as part of the.locdirective..inlined_atattribute specifies source location at which the specified function is inlined.file_index2,line_number2, andcolumn_position2specify the location at which function is inlined. source
location specified as part of.inlined_atdirective must lexically precede as source location in.locdirective. thefunction_nameattribute specifies an offset in the dwarf section named.debug_str. offset is specified aslabelexpression orlabel+immediateexpression
wherelabelis defined in.debug_strsection. dwarf section.debug_strcontains ascii
null-terminated strings that specify the name of the function that is inlined. note that a ptx instruction may have a single associated source location, determined by the nearest
lexically preceding .loc directive, or no associated source location if there is no preceding .loc
directive. labels in ptx inherit the location of the closest lexically following instruction. a
label with no following ptx instruction has no associated source location. ptx isa notes introduced in ptx isa version 1.0. function_nameandinlined_atattributes are introduced in ptx isa version 7.2. target isa notes supported on all target architectures. examples",6
11.6.linking directives,#linking-directives,11.6.linking directives,9
11.6.1.linking directives: .extern,#linking-directives-extern,"11.6.1.linking directives: .extern .extern external symbol declaration. syntax description declares identifier to be defined external to the current module. the module defining such
identifier must define it as.weakor.visibleonly once in a single object file. extern
declaration of symbol may appear multiple times and references to that get resolved against the
single definition of that symbol. ptx isa notes introduced in ptx isa version 1.0. target isa notes supported on all target architectures. examples",7
11.6.2.linking directives: .visible,#linking-directives-visible,"11.6.2.linking directives: .visible .visible visible (externally) symbol declaration. syntax description declares identifier to be globally visible. unlike c, where identifiers are globally visible unless
declared static, ptx identifiers are visible only within the current module unless declared.visibleoutside the current. ptx isa notes introduced in ptx isa version 1.0. target isa notes supported on all target architectures. examples",7
11.6.3.linking directives: .weak,#linking-directives-weak,"11.6.3.linking directives: .weak .weak visible (externally) symbol declaration. syntax description declares identifier to be globally visible butweak. weak symbols are similar to globally visible
symbols, except during linking, weak symbols are only chosen after globally visible symbols during
symbol resolution. unlike globally visible symbols, multiple object files may declare the same weak
symbol, and references to a symbol get resolved against a weak symbol only if no global symbols have
the same name. ptx isa notes introduced in ptx isa version 3.1. target isa notes supported on all target architectures. examples",7
11.6.4.linking directives: .common,#linking-directives-common,"11.6.4.linking directives: .common .common visible (externally) symbol declaration. syntax description declares identifier to be globally visible but common. common symbols are similar to globally visible symbols. however multiple object files may declare
the same common symbol and they may have different types and sizes and references to a symbol get
resolved against a common symbol with the largest size. only one object file can initialize a common symbol and that must have the largest size among all
other definitions of that common symbol from different object files. .commonlinking directive can be used only on variables with.globalstorage. it cannot be
used on function symbols or on symbols with opaque type. ptx isa notes introduced in ptx isa version 5.0. target isa notes .commondirective requiressm_20or higher. examples",7
11.7.cluster dimension directives,#cluster-dimension-directives,"11.7.cluster dimension directives the following directives specify information about clusters: the.reqnctaperclusterdirective specifies the number of ctas in the cluster. the.explicitclusterdirective specifies that the kernel should be launched with explicit cluster
details. the.maxclusterrankdirective specifies the maximum number of ctas in the cluster. the cluster dimension directives can be applied only on kernel functions.",6
11.7.1.cluster dimension directives: .reqnctapercluster,#cluster-dimension-directives-reqnctapercluster,"11.7.1.cluster dimension directives: .reqnctapercluster .reqnctapercluster declare the number of ctas in the cluster. syntax description set the number of thread blocks (ctas) in the cluster by specifying the extent of each dimension of
the 1d, 2d, or 3d cluster. the total number of ctas is the product of the number of ctas in each
dimension. for kernels with.reqnctaperclusterdirective specified, runtime will use the
specified values for configuring the launch if the same are not specified at launch time. semantics if cluster dimension is explicitly specified at launch time, it should be equal to the values
specified in this directive. specifying a different cluster dimension at launch will result in a
runtime error or kernel launch failure. ptx isa notes introduced in ptx isa version 7.8. target isa notes requiressm_90or higher. examples",7
11.7.2.cluster dimension directives: .explicitcluster,#cluster-dimension-directives-explicitcluster,"11.7.2.cluster dimension directives: .explicitcluster .explicitcluster declare that kernel must be launched with cluster dimensions explicitly specified. syntax description declares that this kernel should be launched with cluster dimension explicitly specified. semantics kernels with.explicitclusterdirective must be launched with cluster dimension explicitly
specified (either at launch time or via.reqnctapercluster), otherwise program will fail with
runtime error or kernel launch failure. ptx isa notes introduced in ptx isa version 7.8. target isa notes requiressm_90or higher. examples",7
11.7.3.cluster dimension directives: .maxclusterrank,#cluster-dimension-directives-maxclusterrank,"11.7.3.cluster dimension directives: .maxclusterrank .maxclusterrank declare the maximum number of ctas that can be part of the cluster. syntax description declare the maximum number of thread blocks (ctas) allowed to be part of the cluster. semantics product of the number of ctas in each cluster dimension specified in any invocation of the kernel is
required to be less or equal to that specified in this directive. otherwise invocation will result
in a runtime error or kernel launch failure. the.maxclusterrankdirective cannot be used in conjunction with the.reqnctaperclusterdirective. ptx isa notes introduced in ptx isa version 7.8. target isa notes requiressm_90or higher. examples",7
12.release notes,#release-notes,"12.release notes this section describes the history of change in the ptx isa and implementation. the first section
describes isa and implementation changes in the current release of ptx isa version 8.5, and the
remaining sections provide a record of changes in previous releases of ptx isa versions back to ptx
isa version 2.0. table 32shows the ptx release history.",7
12.1.changes in ptx isa version 8.5,#changes-in-ptx-isa-version-8-5,12.1.changes in ptx isa version 8.5 new features ptx isa version 8.5 introduces the following new features: semantic changes and clarifications,7
12.2.changes in ptx isa version 8.4,#changes-in-ptx-isa-version-8-4,12.2.changes in ptx isa version 8.4 new features ptx isa version 8.4 introduces the following new features: semantic changes and clarifications none.,7
12.3.changes in ptx isa version 8.3,#changes-in-ptx-isa-version-8-3,12.3.changes in ptx isa version 8.3 new features ptx isa version 8.3 introduces the following new features: semantic changes and clarifications none.,7
12.4.changes in ptx isa version 8.2,#changes-in-ptx-isa-version-8-2,"12.4.changes in ptx isa version 8.2 new features ptx isa version 8.2 introduces the following new features: semantic changes and clarifications the.multicast::clusterqualifier oncp.async.bulkandcp.async.bulk.tensorinstructions
is optimized for target architecturesm_90aand may have substantially reduced performance on
other targets and hence.multicast::clusteris advised to be used withsm_90a.",7
12.5.changes in ptx isa version 8.1,#changes-in-ptx-isa-version-8-1,12.5.changes in ptx isa version 8.1 new features ptx isa version 8.1 introduces the following new features: semantic changes and clarifications none.,7
12.6.changes in ptx isa version 8.0,#changes-in-ptx-isa-version-8-0,12.6.changes in ptx isa version 8.0 new features ptx isa version 8.0 introduces the following new features: semantic changes and clarifications none.,7
12.7.changes in ptx isa version 7.8,#changes-in-ptx-isa-version-7-8,12.7.changes in ptx isa version 7.8 new features ptx isa version 7.8 introduces the following new features: semantic changes and clarifications none.,7
12.8.changes in ptx isa version 7.7,#changes-in-ptx-isa-version-7-7,12.8.changes in ptx isa version 7.7 new features ptx isa version 7.7 introduces the following new features: semantic changes and clarifications none.,7
12.9.changes in ptx isa version 7.6,#changes-in-ptx-isa-version-7-6,12.9.changes in ptx isa version 7.6 new features ptx isa version 7.6 introduces the following new features: semantic changes and clarifications none.,7
12.10.changes in ptx isa version 7.5,#changes-in-ptx-isa-version-7-5,12.10.changes in ptx isa version 7.5 new features ptx isa version 7.5 introduces the following new features: semantic changes and clarifications none.,7
12.11.changes in ptx isa version 7.4,#changes-in-ptx-isa-version-7-4,12.11.changes in ptx isa version 7.4 new features ptx isa version 7.4 introduces the following new features: semantic changes and clarifications none.,7
12.12.changes in ptx isa version 7.3,#changes-in-ptx-isa-version-7-3,"12.12.changes in ptx isa version 7.3 new features ptx isa version 7.3 introduces the following new features: semantic changes and clarifications the unimplemented version ofallocafrom the older ptx isa specification has been replaced with
new stack manipulation instructions in ptx isa version 7.3.",7
12.13.changes in ptx isa version 7.2,#changes-in-ptx-isa-version-7-2,12.13.changes in ptx isa version 7.2 new features ptx isa version 7.2 introduces the following new features: semantic changes and clarifications none.,7
12.14.changes in ptx isa version 7.1,#changes-in-ptx-isa-version-7-1,12.14.changes in ptx isa version 7.1 new features ptx isa version 7.1 introduces the following new features: semantic changes and clarifications none.,7
12.15.changes in ptx isa version 7.0,#changes-in-ptx-isa-version-7-0,12.15.changes in ptx isa version 7.0 new features ptx isa version 7.0 introduces the following new features: semantic changes and clarifications none.,7
12.16.changes in ptx isa version 6.5,#changes-in-ptx-isa-version-6-5,12.16.changes in ptx isa version 6.5 new features ptx isa version 6.5 introduces the following new features: removed features ptx isa version 6.5 removes the following features: semantic changes and clarifications none.,7
12.17.changes in ptx isa version 6.4,#changes-in-ptx-isa-version-6-4,12.17.changes in ptx isa version 6.4 new features ptx isa version 6.4 introduces the following new features: deprecated features ptx isa version 6.4 deprecates the following features: removed features ptx isa version 6.4 removes the following features: semantic changes and clarifications,7
12.18.changes in ptx isa version 6.3,#changes-in-ptx-isa-version-6-3,12.18.changes in ptx isa version 6.3 new features ptx isa version 6.3 introduces the following new features: semantic changes and clarifications,7
12.19.changes in ptx isa version 6.2,#changes-in-ptx-isa-version-6-2,12.19.changes in ptx isa version 6.2 new features ptx isa version 6.2 introduces the following new features: deprecated features ptx isa version 6.2 deprecates the following features: semantic changes and clarifications,7
12.20.changes in ptx isa version 6.1,#changes-in-ptx-isa-version-6-1,12.20.changes in ptx isa version 6.1 new features ptx isa version 6.1 introduces the following new features: semantic changes and clarifications none.,7
12.21.changes in ptx isa version 6.0,#changes-in-ptx-isa-version-6-0,12.21.changes in ptx isa version 6.0 new features ptx isa version 6.0 introduces the following new features: semantic changes and clarifications,7
12.22.changes in ptx isa version 5.0,#changes-in-ptx-isa-version-5-0,"12.22.changes in ptx isa version 5.0 new features ptx isa version 5.0 introduces the following new features: semantic changes and clarifications semantics of cache modifiers onldandstinstructions were clarified to reflect cache
operations are treated as performance hint only and do not change memory consistency behavior of the
program. semantics ofvolatileoperations onldandstinstructions were clarified to reflect howvolatileoperations are handled by optimizing compiler.",7
12.23.changes in ptx isa version 4.3,#changes-in-ptx-isa-version-4-3,12.23.changes in ptx isa version 4.3 new features ptx isa version 4.3 introduces the following new features: semantic changes and clarifications none.,7
12.24.changes in ptx isa version 4.2,#changes-in-ptx-isa-version-4-2,"12.24.changes in ptx isa version 4.2 new features ptx isa version 4.2 introduces the following new features: semantic changes and clarifications semantics for parameter passing under abi were updated to indicateld.paramandst.paraminstructions used for argument passing cannot be predicated. semantics of{atom/red}.add.f32were updated to indicate subnormal inputs and results are
flushed to sign-preserving zero for atomic operations on global memory; whereas atomic operations on
shared memory preserve subnormal inputs and results and dont flush them to zero.",7
12.25.changes in ptx isa version 4.1,#changes-in-ptx-isa-version-4-1,12.25.changes in ptx isa version 4.1 new features ptx isa version 4.1 introduces the following new features: semantic changes and clarifications none.,7
12.26.changes in ptx isa version 4.0,#changes-in-ptx-isa-version-4-0,"12.26.changes in ptx isa version 4.0 new features ptx isa version 4.0 introduces the following new features: semantic changes and clarifications thevoteinstruction semantics were updated to clearly indicate that an inactive thread in a
warp contributes a 0 for its entry when participating invote.ballot.b32.",7
12.27.changes in ptx isa version 3.2,#changes-in-ptx-isa-version-3-2,"12.27.changes in ptx isa version 3.2 new features ptx isa version 3.2 introduces the following new features: semantic changes and clarifications thevavrg2andvavrg4instruction semantics were updated to indicate that instruction adds 1
only if va[i] + vb[i] is non-negative, and that the addition result is shifted by 1 (rather than
being divided by 2).",7
12.28.changes in ptx isa version 3.1,#changes-in-ptx-isa-version-3-1,"12.28.changes in ptx isa version 3.1 new features ptx isa version 3.1 introduces the following new features: semantic changes and clarifications ptx 3.1 redefines the default addressing for global variables in initializers, from generic
addresses to offsets in the global state space. legacy ptx code is treated as having an implicitgeneric()operator for each global variable used in an initializer. ptx 3.1 code should either
include explicitgeneric()operators in initializers, usecvta.globalto form generic
addresses at runtime, or load from the non-generic address usingld.global. instructionmad.f32requires a rounding modifier forsm_20and higher targets. however for
ptx isa version 3.0 and earlier, ptxas does not enforce this requirement andmad.f32silently
defaults tomad.rn.f32. for ptx isa version 3.1, ptxas generates a warning and defaults tomad.rn.f32, and in subsequent releases ptxas will enforce the requirement for ptx isa version
3.2 and later.",7
12.29.changes in ptx isa version 3.0,#changes-in-ptx-isa-version-3-0,"12.29.changes in ptx isa version 3.0 new features ptx isa version 3.0 introduces the following new features: semantic changes and clarifications special register%grididhas been extended from 32-bits to 64-bits. ptx isa version 3.0 deprecates module-scoped.regand.localvariables when compiling to the
application binary interface (abi). when compiling without use of the abi, module-scoped.regand.localvariables are supported as before. when compiling legacy ptx code (isa versions prior
to 3.0) containing module-scoped.regor.localvariables, the compiler silently disables
use of the abi. theshflinstruction semantics were updated to clearly indicate that value of source operandais unpredictable for inactive and predicated-off threads within the warp. ptx modules no longer allow duplicate.versiondirectives. this feature was unimplemented, so
there is no semantic change. unimplemented instructionssuld.pandsust.p.{u32,s32,f32}have been removed.",7
12.30.changes in ptx isa version 2.3,#changes-in-ptx-isa-version-2-3,"12.30.changes in ptx isa version 2.3 new features ptx 2.3 adds support for texture arrays. the texture array feature supports access to an array of 1d
or 2d textures, where an integer indexes into the array of textures, and then one or two
single-precision floating point coordinates are used to address within the selected 1d or 2d
texture. ptx 2.3 adds a new directive,.address_size, for specifying the size of addresses. variables in.constand.globalstate spaces are initialized to zero by default. semantic changes and clarifications the semantics of the.maxntiddirective have been updated to match the current
implementation. specifically,.maxntidonly guarantees that the total number of threads in a
thread block does not exceed the maximum. previously, the semantics indicated that the maximum was
enforced separately in each dimension, which is not the case. bit field extract and insert instructions bfe and bfi now indicate that thelenandposoperands are restricted to the value range0..255. unimplemented instructions{atom,red}.{min,max}.f32have been removed.",7
12.31.changes in ptx isa version 2.2,#changes-in-ptx-isa-version-2-2,"12.31.changes in ptx isa version 2.2 new features ptx 2.2 adds a new directive for specifying kernel parameter attributes; specifically, there is a
new directives for specifying that a kernel parameter is a pointer, for specifying to which state
space the parameter points, and for optionally specifying the alignment of the memory to which the
parameter points. ptx 2.2 adds a new field namedforce_unnormalized_coordsto the.samplerrefopaque
type. this field is used in the independent texturing mode to override thenormalized_coordsfield in the texture header. this field is needed to support languages such as opencl, which
represent the property of normalized/unnormalized coordinates in the sampler header rather than in
the texture header. ptx 2.2 deprecates explicit constant banks and supports a large, flat address space for the.conststate space. legacy ptx that uses explicit constant banks is still supported. ptx 2.2 adds a newtld4instruction for loading a component (r,g,b, ora) from
the four texels compising the bilinear interpolation footprint of a given texture location. this
instruction may be used to compute higher-precision bilerp results in software, or for performing
higher-bandwidth texture loads. semantic changes and clarifications none.",7
12.32.changes in ptx isa version 2.1,#changes-in-ptx-isa-version-2-1,"12.32.changes in ptx isa version 2.1 new features the underlying, stack-based abi is supported in ptx isa version 2.1 forsm_2xtargets. support for indirect calls has been implemented forsm_2xtargets. new directives,.branchtargetsand.calltargets, have been added for specifying potential
targets for indirect branches and indirect function calls. a.callprototypedirective has been
added for declaring the type signatures for indirect function calls. the names of.globaland.constvariables can now be specified in variable initializers to
represent their addresses. a set of thirty-two driver-specific execution environment special registers has been added. these
are named%envreg0..%envreg31. textures and surfaces have new fields for channel data type and channel order, and thetxqandsuqinstructions support queries for these fields. directive.minnctapersmhas replaced the.maxnctapersmdirective. directive.reqntidhas been added to allow specification of exact cta dimensions. a new instruction,rcp.approx.ftz.f64, has been added to compute a fast, gross approximate
reciprocal. semantic changes and clarifications a warning is emitted if.minnctapersmis specified without also specifying.maxntid.",7
12.33.changes in ptx isa version 2.0,#changes-in-ptx-isa-version-2-0,"12.33.changes in ptx isa version 2.0 new features floating point extensions this section describes the floating-point changes in ptx isa version 2.0 forsm_20targets. the
goal is to achieve ieee 754 compliance wherever possible, while maximizing backward compatibility
with legacy ptx isa version 1.x code andsm_1xtargets. the changes from ptx isa version 1.x are as follows: new instructions aload uniforminstruction,ldu, has been added. surface instructions support additional.clampmodifiers,.clampand.zero. instructionsustnow supports formatted surface stores. acount leading zerosinstruction,clz, has been added. afind leading non-sign bit instruction,bfind, has been added. abit reversalinstruction,brev, has been added. bit field extract and insert instructions,bfeandbfi, have been added. apopulation countinstruction,popc, has been added. avote ballotinstruction,vote.ballot.b32, has been added. instructions{atom,red}.add.f32have been implemented. instructions{atom,red}.shared have been extended to handle 64-bit data types forsm_20targets. a system-level membar instruction,membar.sys, has been added. thebarinstruction has been extended as follows: scalar video instructions (includesprmt) have been added. instructionisspacepfor querying whether a generic address falls within a specified state space
window has been added. instructioncvtafor converting global, local, and shared addresses to generic address and
vice-versa has been added. other new features instructionsld,ldu,st,prefetch,prefetchu,isspacep,cvta,atom,
andrednow support generic addressing. new special registers%nwarpid,%nsmid,%clock64,%lanemask_{eq,le,lt,ge,gt}have
been added. cache operations have been added to instructionsld,st,suld, andsust, e.g., forprefetchingto specified level of memory hierarchy. instructionsprefetchandprefetchuhave also been added. the.maxnctapersmdirective was deprecated and replaced with.minnctapersmto better match
its behavior and usage. a new directive,.section, has been added to replace the@@dwarfsyntax for passing
dwarf-format debugging information through ptx. a new directive,.pragmanounroll, has been added to allow users to disable loop unrolling. semantic changes and clarifications the errata incvt.ftzfor ptx isa versions 1.4 and earlier, where single-precision subnormal
inputs and results were not flushed to zero if either source or destination type size was 64-bits,
has been fixed. in ptx isa version 1.5 and later,cvt.ftz(andcvtfor.targetsm_1x,
where.ftzis implied) instructions flush single-precision subnormal inputs and results to
sign-preserving zero for all combinations of floating-point instruction types. to maintain
compatibility with legacy ptx code, if .version is 1.4 or earlier, single-precision subnormal inputs
and results are flushed to sign-preserving zero only when neither source nor destination type size
is 64-bits. components of special registers%tid,%ntid,%ctaid, and%nctaidhave been extended
from 16-bits to 32-bits. these registers now have type.v4.u32. the number of samplers available in independent texturing mode was incorrectly listed as thirty-two
in ptx isa version 1.5; the correct number is sixteen.",7
14.descriptions of .pragma strings,#descriptions-pragma-strings,14.descriptions of .pragma strings this section describes the.pragmastrings defined by ptxas.,7
14.1.pragma strings: nounroll,#pragma-strings-nounroll,"14.1.pragma strings: nounroll nounroll disable loop unrolling in optimizing the backend compiler. syntax description the""nounroll""pragmais a directive to disable loop unrolling in the optimizing backend
compiler. the""nounroll""pragmais allowed at module, entry-function, and statement levels, with the
following meanings: note that in order to have the desired effect at statement level, the""nounroll""directive must
appear before any instruction statements in the loop header basic block for the desired loop. the
loop header block is defined as the block that dominates all blocks in the loop body and is the
target of the loop backedge. statement-level""nounroll""directives appearing outside of loop
header blocks are silently ignored. ptx isa notes introduced in ptx isa version 2.0. target isa notes requiressm_20or higher. ignored forsm_1xtargets. examples",7
14.2.pragma strings: used_bytes_mask,#pragma-strings-used-bytes-mask,"14.2.pragma strings: used_bytes_mask used_bytes_mask mask for indicating used bytes in data of ld operation. syntax description the""used_bytes_mask""pragmais a directive that specifies used bytes in a load
operation based on the mask provided. ""used_bytes_mask""pragmaneeds to be specified prior to a load instruction for which
information about bytes used from the load operation is needed.
pragma is ignored if instruction following it is not a load instruction. for a load instruction without this pragma, all bytes from the load operation are assumed
to be used. operandmaskis a 32-bit integer with set bits indicating the used bytes in data of
load operation. semantics ptx isa notes introduced in ptx isa version 8.3. target isa notes requiressm_50or higher. examples",7
15.notices,#notices,15.notices,9
15.1.notice,#notice,"15.1.notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation (nvidia) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material (defined below), code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer (terms of sale). nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion and/or use of nvidia products in such equipment or applications and therefore such inclusion and/or use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions and/or requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the nvidia product in any manner that is contrary to this document or (ii) customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding third-party products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents (together and separately, materials) are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product.",4
15.2.opencl,#opencl,15.2.opencl opencl is a trademark of apple inc. used under license to the khronos group inc.,4
15.3.trademarks,#trademarks,15.3.trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.,4
1.maxwell compatibility,#maxwell-compatibility,1.maxwell compatibility,9
1.1.about this document,#about-this-document,"1.1.about this document this application note, maxwell compatibility guide for cuda applications, is intended to help developers ensure that their nvidiacudaapplications will run on gpus based on the nvidiamaxwell architecture. this document provides guidance to developers who are already familiar with programming in cuda c++ and want to make sure that their software applications are compatible with maxwell.",0
1.2.application compatibility on maxwell,#application-compatibility-on-maxwell,"1.2.application compatibility on maxwell the nvidia cuda c++ compiler,nvcc, can be used to generate both architecture-specificcubinfiles and forward-compatibleptxversions of each kernel. each cubin file targets a specific compute-capability version and is forward-compatibleonly with gpu architectures of the same major version number. for example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (kepler) devices but arenotsupported on compute-capability 5.x (maxwell) devices. for this reason, to ensure forward compatibility with gpu architectures introduced after the application has been released, it is recommended that all applications include ptx versions of their kernels. applications that already include ptx versions of their kernels should work as-is on maxwell-based gpus. applications that only support specific gpu architectures via cubin files, however, will need to be updated to provide maxwell-compatible ptx or cubins.",0
1.3.verifying maxwell compatibility for existing applications,#verifying-maxwell-compatibility-for-existing-applications,1.3.verifying maxwell compatibility for existing applications the first step is to check that maxwell-compatible device code (at least ptx) is compiled in to the application. the following sections show how to accomplish this for applications built with different cuda toolkit versions.,0
1.3.1.applications using cuda toolkit 5.5 or earlier,#applications-using-cuda-toolkit-5-5-or-earlier,"1.3.1.applications using cuda toolkit 5.5 or earlier cuda applications built using cuda toolkit versions 2.1 through 5.5 are compatible with maxwell as long as they are built to include ptx versions of their kernels. to test that ptx jit is working for your application, you can do the following: when starting a cuda application for the first time with the above environment flag, the cuda driver will jit-compile the ptx for each cuda kernel that is used into native cubin code. if you set the environment variable above and then launch your program and it works properly, then you have successfully verified maxwell compatibility.",0
1.3.2.applications using cuda toolkit 6.0 or later,#applications-using-cuda-toolkit-6-0-or-later,1.3.2.applications using cuda toolkit 6.0 or later cuda applications built using cuda toolkit 6.0 or later1are compatible with maxwell as long as they are built to include kernels in either maxwell-native cubin format (seebuilding applications with maxwell support) or ptx format (seeapplications using cuda toolkit 5.5 or earlier) or both.,0
1.4.building applications with maxwell support,#building-applications-with-maxwell-support,"1.4.building applications with maxwell support when a cuda application launches a kernel, the cuda runtime determines the compute capability of each gpu in the system and uses this information to automatically find the best matching cubin or ptx version of the kernel that is available. if a cubin file supporting the architecture of the target gpu is available, it is used; otherwise, the cuda runtime will load the ptx and jit-compile that ptx to the gpus native cubin format before launching it. if neither is available, then the kernel launch will fail. the method used to build your application with either native cubin or at least ptx support for maxwell depend on the version of the cuda toolkit used. the main advantages of providing native cubins are as follows:",0
1.4.1.applications using cuda toolkit 5.5 or earlier,#id2,"1.4.1.applications using cuda toolkit 5.5 or earlier the compilers included in cuda toolkit 5.5 or earlier generate cubin files native to earlier nvidia architectures such as fermi and kepler, but theycannotgenerate cubin files native to the maxwell architecture. to allow support for maxwell and future architectures when using version 5.5 or earlier of the cuda toolkit, the compiler must generate a ptx version of each kernel. below are compiler settings that could be used to buildmykernel.cuto run on fermi or kepler devices natively and on maxwell devices via ptx jit. windows mac/linux alternatively, you may be familiar with the simplifiednvcccommand-line option-arch=sm_xx, which is a shorthand equivalent to the following more explicit-gencode=command-line options used above.-arch=sm_xxexpands to the following: however, while the-arch=sm_xxcommand-line option does result in inclusion of a ptx back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple-arch=options on the samenvcccommand line, which is why the examples above use-gencode=explicitly.",0
1.4.2.applications using cuda toolkit 6.0 or later,#id3,"1.4.2.applications using cuda toolkit 6.0 or later with version 6.0 of the cuda toolkit,nvcccan generate cubin files native to the first-generation maxwell architecture (compute capability 5.0); cuda toolkit 6.5 and later further add native support for second-generation maxwell devices (compute capability 5.2). when using cuda toolkit 6.x or later, to ensure thatnvccwill generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate-gencode=parameters on thenvcccommand line as shown in the examples below. windows mac/linux",0
1.introduction,#introduction,1.introduction cusparse release notes:cuda-toolkit-release-notes cusparse github samples:cudalibrarysamples nvidia developer forum:gpu-accelerated libraries provide feedback:math-libs-feedback@nvidia.com recent cusparse/cusparselt blog posts and gtc presentations: the library routines provide the following functionalities:,0
1.1.library organization and features,#library-organization-and-features,1.1.library organization and features the cusparse library is organized in two set of apis:,8
1.2.static library support,#static-library-support,"1.2.static library support starting with cuda 6.5, the cusparse library is also delivered in astaticform aslibcusparse_static.aon linux. for example, to compile a small application using cusparse against thedynamic library, the following command can be used: whereas to compile against thestatic library, the following command has to be used: it is also possible to use the native host c++ compiler. depending on the host operating system, some additional libraries likepthreadordlmight be needed on the linking line. the following command on linux is suggested: note that in the latter case, the librarycudais not needed. the cuda runtime will try to open explicitly thecudalibrary if needed. in the case of a system which does not have the cuda driver installed, this allows the application to gracefully manage this issue and potentially run if a cpu-only path is available.",0
1.3.library dependencies,#library-dependencies,"1.3.library dependencies starting with cuda 12.0, cusparse will depend onnvjitlinklibrary for jit (just-in-time) lto (link-time-optimization) capabilities; refer to thecusparsespmmopapis for more information. if the user links to thedynamic library, the environment variables for loading the libraries at run-time (such asld_library_pathon linux andpathon windows) must include the path wherelibnvjitlink.sois located. if it is in the same directory as cusparse, the user doesnt need to take any action. if linking to thestatic library, the user needs to link with-lnvjitlinkand set the environment variables for loading the libraries at compile-timelibrary_path/pathaccordingly.",0
2.using the cusparse api,#using-the-cusparse-api,2.using the cusparse api this chapter describes how to use the cusparse library api. it is not a reference for the cusparse api data types and functions; that is provided in subsequent chapters.,8
2.1.apis usage notes,#apis-usage-notes,"2.1.apis usage notes the cusparse library allows developers to access the computational resources of the nvidia graphics processing unit (gpu). it is the responsibility of the developer to allocate memory and to copy data between gpu memory and cpu memory using standard cuda runtime api routines, such ascudamalloc(),cudafree(),cudamemcpy(), andcudamemcpyasync(). the cusparse library functions are executedasynchronouslywith respect to the host and may return control to the application on the host before the result is ready. developers can use thecudadevicesynchronize()function to ensure that the execution of a particular cusparse library routine has completed. a developer can also use thecudamemcpy()routine to copy data from the device to the host and vice versa, using thecudamemcpydevicetohostandcudamemcpyhosttodeviceparameters, respectively. in this case there is no need to add a call tocudadevicesynchronize()because the call tocudamemcpy()with the above parameters is blocking and completes only when the results are ready on the host.",5
2.2.deprecated apis,#deprecated-apis,"2.2.deprecated apis the cusparse library documentation explicitly indicates the set of apis/enumerators/data structures that are deprecated. the library policy for deprecated apis is the following: correctness bugs are still addressed even for deprecated apis, while performance issues are not always ensured. in addition to the documentation, deprecated apis generate acompile-timewarning for most platforms when used. deprecation warnings can be disabled by defining the macrodisable_cusparse_deprecatedbefore includingcusparse.hor by passing the flag-ddisable_cusparse_deprecatedto the compiler.",8
2.3.thread safety,#thread-safety,"2.3.thread safety the library is thread safe and its functions can be called from multiple host threads, even with the same handle. when multiple threads share the same handle, extreme care needs to be taken when the handle configuration is changed because that change will affect potentially subsequent cusparse calls in all threads. it is even more true for the destruction of the handle. so it is not recommended that multiple thread share the same cusparse handle.",5
2.4.result reproducibility,#result-reproducibility,"2.4.result reproducibility the design of cusparse prioritizes performance over bit-wise reproducibility. operations using transpose or conjugate-transposecusparseoperation_thave no reproducibility guarantees. for the remaining operations,
performing the same api call twice with the exact same arguments,
on the same machine, with the same executable will produce bit-wise identical results.
this bit-wise reproducibility can be disrupted by changes to:
hardware, cuda drivers, cusparse version, memory alignment of the data, or algorithm selection.",5
2.5.nan and inf propagation,#nan-and-inf-propagation,"2.5.nan and inf propagation floating-point numbers have special values for nan (not-a-number) and inf (infinity).
functions in cusparse make no guarantees about the propagation of nan and inf. the cusparse algorithms evaluate assuming all finite floating-point values.
nan and inf appear in the output only if the algorithms happen to generate or propagate them.
because the algorithms are subject to change based on toolkit version and runtime considerations,
so too are the propagation behaviours of nan and inf. nan propagation is different in cusparse than in
typical dense numerical linear algebra, such as cublas.
the dot product between vectors[0,1,0]and[1,1,nan]is nan when using typical dense numerical algorithms,
but will be 1.0 with typical sparse numerical algorithms.",1
2.6.parallelism with streams,#parallelism-with-streams,"2.6.parallelism with streams if the application performs several small independent computations, or if it makes data transfers in parallel with the computation, cuda streams can be used to overlap these tasks. the application can conceptually associate a stream with each task. to achieve the overlap of computation between the tasks, the developer should create cuda streams using the functioncudastreamcreate()and set the stream to be used by each individual cusparse library routine by callingcusparsesetstream()just before calling the actual cusparse routine. then, computations performed in separate streams would be overlapped automatically on the gpu, when possible. this approach is especially useful when the computation performed by a single task is relatively small and is not enough to fill the gpu with work, or when there is a data transfer that can be performed in parallel with the computation. when streams are used, we recommend using the new cusparse api with scalar parameters and results passed by reference in the device memory to achieve maximum computational overlap. although a developer can create many streams, in practice it is not possible to have more than 16 concurrent kernels executing at the same time.",5
2.7.compatibility and versioning,#compatibility-and-versioning,"2.7.compatibility and versioning the cusparse apis are intended to be backward compatible at the source level with future releases (unless stated otherwise in the release notes of a specific future release). in other words, if a program uses cusparse, it should continue to compile and work correctly with newer versions of cusparse without source code changes. cusparse is not guaranteed to be backward compatible at the binary level. using different versions of thecusparse.hheader file and shared library is not supported. using different versions of cusparse and the cuda runtime is not supported. the library uses the standardversion semanticconvention for identify different releases. theversiontakes the form of four fields joined by periods:major.minor.patch.build theseversion fieldsare incremented based on the following rules: *different cuda toolkit releases ensure distinct library versions even if there are no changes at library level.",0
2.8.optimization notes,#optimization-notes,"2.8.optimization notes most of the cusparse routines can be optimized by exploitingcuda graphs captureandhardware memory compressionfeatures. more in details, a single cusparse call or a sequence of calls can be captured by acuda graphand executed in a second moment. this minimizes kernels launch overhead and allows the cuda runtime to optimize the whole workflow. a full example of cuda graphs capture applied to a cusparse routine can be found incusparse library samples - cuda graph. secondly, the data types and functionalities involved in cusparse are suitable forhardware memory compressionavailable in ampere gpu devices (compute capability 8.0) or above. the feature allows memory compression for data with enough zero bytes without no loss of information. the device memory must be allocation with thecuda driver apis. a full example of hardware memory compression applied to a cusparse routine can be found incusparse library samples - memory compression.",5
3.cusparse storage formats,#cusparse-storage-formats,"3.cusparse storage formats the cusparse library supports dense and sparse vector, and dense and sparse matrix formats.",1
3.1.index base,#index-base,3.1.index base the library supports zero- and one-based indexing to ensure the compatibility with c/c++ and fortran languages respectively. the index base is selected through thecusparseindexbase_ttype.,6
3.2.vector formats,#vector-formats,3.2.vector formats this section describes dense and sparse vector formats.,1
3.2.1.dense vector format,#dense-vector-format,"3.2.1.dense vector format dense vectors are represented with a single data array that is stored linearly in memory, such as the following\(7 \times 1\)dense vector.",6
3.2.2.sparse vector format,#sparse-vector-format,"3.2.2.sparse vector format sparse vectors are represented with two arrays. for example, the dense vector in section 3.2.1 can be stored as a sparse vector with zero-based or one-based indexing.",1
3.3.matrix formats,#matrix-formats,3.3.matrix formats dense and several sparse formats for matrices are discussed in this section.,1
3.3.1.dense matrix format,#dense-matrix-format,3.3.1.dense matrix format a dense matrix can be stored in bothrow-majorandcolumn-majormemory layout (ordering) and it is represented by the following parameters. the following figure represents a\(5 \times 2\)dense matrix with both memory layouts the indices within the matrix represents the contiguous locations in memory. the leading dimension is useful to represent a sub-matrix within the original one,1
3.3.2.coordinate (coo),#coordinate-coo,"3.3.2.coordinate (coo) a sparse matrix stored incooformat is represented by the following parameters. the following example shows a\(5 \times 4\)matrix represented in coo format. given an entry in the coo format (zero-base), the corresponding position in the dense matrix is computed as:",1
3.3.3.compressed sparse row (csr),#compressed-sparse-row-csr,"3.3.3.compressed sparse row (csr) thecsrformat is similar to coo, where therow indicesare compressed and replaced by an array ofoffsets. a sparse matrix stored in csr format is represented by the following parameters. the following example shows a\(5 \times 4\)matrix represented in csr format. given an entry in the csr format (zero-base), the corresponding position in the dense matrix is computed as:",1
3.3.4.compressed sparse column (csc),#compressed-sparse-column-csc,"3.3.4.compressed sparse column (csc) thecscformat is similar to coo, where thecolumn indicesare compressed and replaced by an array ofoffsets. a sparse matrix stored in csc format is represented by the following parameters. the following example shows a\(5 \times 4\)matrix represented in csc format. given an entry in the csc format (zero-base), the corresponding position in the dense matrix is computed as:",1
3.3.5.sliced ellpack (sell),#sliced-ellpack-sell,"3.3.5.sliced ellpack (sell) thesliced ellpackformat is standardized and well-known as the state of the art.
this format allows to significantly improve the performance of all problems that involve low variability in the number of nonzero elements per row. a matrix in the sliced ellpack format is divided intoslicesof anexact number of rows(\(slicesize\)), defined by the user.
the maximum row length (i.e.,  the maximum non-zeros per row) is found for each slice, and every row in the slice is padded to the maximum row length.
the value-1is used for padding. a\(m \times n\)sparse matrix\(a\)is equivalent to asliced sparse matrix\(a_{s}\)with\(nslices = \left \lceil{\frac{m}{slicesize}}\right \rceil\)slice rows and\(n\)columns.
to improve memory coalescing and memory utilization, each slice is stored incolumn-majororder. a sparse matrix stored in sell format is represented by the following parameters. the following example shows a\(5 \times 4\)matrix represented in sell format.",1
3.3.6.block sparse row (bsr),#block-sparse-row-bsr,"3.3.6.block sparse row (bsr) the bsr format is similar to csr, where thecolumn indicesrepresent two-dimensional blocks instead of a single matrix entry. a matrix in the block sparse row format is organized into blocks of size\(blocksize\), defined by the user. a\(m \times n\)sparse matrix\(a\)is equivalent to ablock sparse matrix\(a_{b}\):\(mb \times nb\)with\(mb = \frac{m}{blocksize}\)block rowsand\(nb = \frac{n}{blocksize}\)block columns.
if\(m\)or\(n\)is not multiple of\(blocksize\), the user needs to pad the matrix with zeros. the bsr format stores the blocks in row-major ordering. however, the internal storage format of blocks can becolumn-major(cusparsedirection_t=cusparse_direction_column) orrow-major(cusparsedirection_t=cusparse_direction_row), independently of the base index. a sparse matrix stored in bsr format is represented by the following parameters. the following example shows a\(4 \times 7\)matrix represented in bsr format.",1
3.3.7.blocked ellpack (blocked-ell),#blocked-ellpack-blocked-ell,"3.3.7.blocked ellpack (blocked-ell) the blocked ellpack format is similar to the standard ellpack, where thecolumn indicesrepresent two-dimensional blocks instead of a single matrix entry. a matrix in the blocked ellpack format is organized into blocks of size\(blocksize\), defined by the user. the number of columns per row\(nellcols\)is also defined by the user (\(nellcols \le n\)). a\(m \times n\)sparse matrix\(a\)is equivalent to ablocked-ellmatrix\(a_{b}\):\(mb \times nb\)with\(mb = \left \lceil{\frac{m}{blocksize}}\right \rceil\)block rows, and\(nb = \left \lceil{\frac{nellcols}{blocksize}}\right \rceil\)block columns.
if\(m\)or\(n\)is not multiple of\(blocksize\), then the remaining elements are zero. a sparse matrix stored in blocked-ell format is represented by the following parameters. the following example shows a\(9 \times 9\)matrix represented in blocked-ell format.",1
3.3.8.extended bsr format (bsrx) [deprecated],#extended-bsr-format-bsrx-deprecated,"3.3.8.extended bsr format (bsrx) [deprecated] bsrx is the same as the bsr format, but the arraybsrrowptrais separated into two parts. the first nonzero block of each row is still specified by the arraybsrrowptra, which is the same as in bsr, but the position next to the last nonzero block of each row is specified by the arraybsrendptra. briefly, bsrx format is simply like a 4-vector variant of bsr format. matrixais represented in bsrx format by the following parameters. a simple conversion between bsr and bsrx can be done as follows. suppose the developer has a23block sparse matrix\(a_{b}\)represented as shown. assume it has this bsr format. thebsrrowptraof the bsrx format is simply the first two elements of thebsrrowptrabsr format. thebsrendptraof bsrx format is the last two elements of thebsrrowptraof bsr format. the advantage of the bsrx format is that the developer can specify a submatrix in the original bsr format by modifyingbsrrowptraandbsrendptrawhile keepingbsrcolindaandbsrvalaunchanged. for example, to create another block matrix\(\widetilde{a} = \begin{bmatrix}
o & o & o \\
o & a_{11} & o \\
\end{bmatrix}\)that is slightly different from\(a\), the developer can keepbsrcolindaandbsrvala, but reconstruct\(\widetilde{a}\)by properly setting ofbsrrowptraandbsrendptra. the following 4-vector characterizes\(\widetilde{a}\).",1
4.cusparse basic apis,#cusparse-basic-apis,4.cusparse basic apis,8
4.1.cusparse types reference,#cusparse-types-reference,4.1.cusparse types reference,8
4.1.1.cudadatatype_t,#cudadatatype-t,"4.1.1.cudadatatype_t the section describes the types shared by multiple cuda libraries and defined in the header filelibrary_types.h. thecudadatatypetype is an enumerator to specify the data precision. it is used when the data reference does not carry the type itself (e.g.void*). for example, it is used in the routinecusparsespmm(). important:the generic api routines allow all data types reported in the respective section of the documentation only on gpu architectures withnativesupport for them. if a specific gpu model does not providenativesupport for a given data type, the routine returnscusparse_status_arch_mismatcherror. unsupported data types and compute capability (cc): seehttps://developer.nvidia.com/cuda-gpus",0
4.1.2.cusparsestatus_t,#cusparsestatus-t,4.1.2.cusparsestatus_t this data type represents the status returned by the library functions and it can have the following values,8
4.1.3.cusparsehandle_t,#cusparsehandle-t,"4.1.3.cusparsehandle_t this is a pointer type to an opaque cusparse context, which the user must initialize by calling prior to callingcusparsecreate()any other library function. the handle created and returned bycusparsecreate()must be passed to every cusparse function.",8
4.1.4.cusparsepointermode_t,#cusparsepointermode-t,"4.1.4.cusparsepointermode_t this type indicates whether the scalar values are passed by reference on the host or device. it is important to point out that if several scalar values are passed by reference in the function call, all of them will conform to the same single pointer mode. the pointer mode can be set and retrieved usingcusparsesetpointermode()andcusparsegetpointermode()routines, respectively.",6
4.1.5.cusparseoperation_t,#cusparseoperation-t,"4.1.5.cusparseoperation_t this type indicates which operations is applied to the related input (e.g. sparse matrix, or vector).",1
4.1.6.cusparsediagtype_t,#cusparsediagtype-t,"4.1.6.cusparsediagtype_t this type indicates if the matrix diagonal entries are unity. the diagonal elements are always assumed to be present, but ifcusparse_diag_type_unitis passed to an api routine, then the routine assumes that all diagonal entries are unity and will not read or modify those entries. note that in this case the routine assumes the diagonal entries are equal to one, regardless of what those entries are actually set to in memory.",1
4.1.7.cusparsefillmode_t,#cusparsefillmode-t,4.1.7.cusparsefillmode_t this type indicates if the lower or upper part of a matrix is stored in sparse storage.,1
4.1.8.cusparseindexbase_t,#cusparseindexbase-t,4.1.8.cusparseindexbase_t this type indicates if the base of the matrix indices is zero or one.,1
4.1.9.cusparsedirection_t,#cusparsedirection-t,4.1.9.cusparsedirection_t this type indicates whether the elements of a dense matrix should be parsed by rows or by columns (assuming column-major storage in memory of the dense matrix) in function cusparse[s|d|c|z]nnz. besides storage format of blocks in bsr format is also controlled by this type.,1
4.2.cusparse management api,#cusparse-management-api,4.2.cusparse management api the cusparse functions for managing the library are described in this section.,8
4.2.1.cusparsecreate(),#cusparsecreate,4.2.1.cusparsecreate() this function initializes the cusparse library and creates a handle on the cusparse context. it must be called before any other cusparse api function is invoked. it allocates hardware resources necessary for accessing the gpu. seecusparsestatus_tfor the description of the return status,8
4.2.2.cusparsedestroy(),#cusparsedestroy,4.2.2.cusparsedestroy() this function releases cpu-side resources used by the cusparse library. the release of gpu-side resources may be deferred until the application shuts down. seecusparsestatus_tfor the description of the return status,5
4.2.3.cusparsegeterrorname(),#cusparsegeterrorname,"4.2.3.cusparsegeterrorname() the function returns the string representation of an error code enum name. if the error code is not recognized, unrecognized error code is returned.",8
4.2.4.cusparsegeterrorstring(),#cusparsegeterrorstring,"4.2.4.cusparsegeterrorstring() returns the description string for an error code. if the error code is not recognized, unrecognized error code is returned.",8
4.2.5.cusparsegetproperty(),#cusparsegetproperty,4.2.5.cusparsegetproperty() the function returns the value of the requested property. refer tolibrarypropertytypefor supported types. librarypropertytype(defined inlibrary_types.h): seecusparsestatus_tfor the description of the return status,8
4.2.6.cusparsegetversion(),#cusparsegetversion,4.2.6.cusparsegetversion() this function returns the version number of the cusparse library. seecusparsestatus_tfor the description of the return status,8
4.2.7.cusparsegetpointermode(),#cusparsegetpointermode,4.2.7.cusparsegetpointermode() this function obtains the pointer mode used by the cusparse library. please see the section on thecusparsepointermode_ttype for more details. seecusparsestatus_tfor the description of the return status,1
4.2.8.cusparsesetpointermode(),#cusparsesetpointermode,4.2.8.cusparsesetpointermode() this function sets the pointer mode used by the cusparse library. thedefaultis for the values to be passed by reference on the host. please see the section on thecublaspointermode_ttype for more details. seecusparsestatus_tfor the description of the return status,8
4.2.9.cusparsegetstream(),#cusparsegetstream,"4.2.9.cusparsegetstream() this function gets the cusparse library stream, which is being used to to execute all calls to the cusparse library functions. if the cusparse library stream is not set, all kernels use the default null stream. seecusparsestatus_tfor the description of the return status",8
4.2.10.cusparsesetstream(),#cusparsesetstream,4.2.10.cusparsesetstream() this function sets the stream to be used by the cusparse library to execute its routines. seecusparsestatus_tfor the description of the return status,1
4.3.cusparse logging api,#cusparse-logging-api,"4.3.cusparse logging api cusparse logging mechanism can be enabled by setting the following environment variables before launching the target application: cusparse_log_level=<level>- while level is one of the following levels: cusparse_log_mask=<mask>- while mask is a combination of the following masks: cusparse_log_file=<file_name>- while file name is a path to a logging file. file name may contain%i, that will be replaced with the process id. e.g<file_name>_%i.log. ifcusparse_log_fileis not defined, the log messages are printed tostdout. starting from cuda 12.3, it is also possible to dump sparse matrices (csr, csc, coo, sell, bsr) in binary files during the creation by setting the environment variablecusparse_store_input_matrix. later on, the binary files can be send tomath-libs-feedback@nvidia.comfor debugging and reproducibility purposes of a specific correctness/performance issue. another option is to use the experimental cusparse logging api. see:",1
4.3.1.cusparseloggersetcallback(),#cusparseloggersetcallback,4.3.1.cusparseloggersetcallback() experimental: the function sets the logging callback function. wherecusparseloggercallback_thas the following signature: seecusparsestatus_tfor the description of the return status,6
4.3.2.cusparseloggersetfile(),#cusparseloggersetfile,"4.3.2.cusparseloggersetfile() experimental: the function sets the logging output file. note: once registered using this function call, the provided file handle must not be closed unless the function is called again to switch to a different file handle. seecusparsestatus_tfor the description of the return status",6
4.3.3.cusparseloggeropenfile(),#cusparseloggeropenfile,4.3.3.cusparseloggeropenfile() experimental: the function opens a logging output file in the given path. seecusparsestatus_tfor the description of the return status,1
4.3.4.cusparseloggersetlevel(),#cusparseloggersetlevel,4.3.4.cusparseloggersetlevel() experimental: the function sets the value of the logging level. path. seecusparsestatus_tfor the description of the return status,1
4.3.5.cusparseloggersetmask(),#cusparseloggersetmask,4.3.5.cusparseloggersetmask() experimental: the function sets the value of the logging mask. seecusparsestatus_tfor the description of the return status,1
5.cusparse legacy apis,#cusparse-legacy-apis,5.cusparse legacy apis,0
5.1.naming conventions,#naming-conventions,"5.1.naming conventions the cusparse legacy functions are available for data typesfloat,double,cucomplex, andcudoublecomplex. the sparse level 2, and level 3 functions follow this naming convention: cusparse<t>[<matrixdataformat>]<operation>[<outputmatrixdataformat>] where <t> can bes,d,c,z, orx, corresponding to the data typesfloat,double,cucomplex,cudoublecomplex, and the generic type, respectively. the <matrixdataformat> can bedense,coo,csr, orcsc, corresponding to the dense, coordinate, compressed sparse row, and compressed sparse column formats, respectively.",1
5.2.cusparse legacy types reference,#cusparse-legacy-types-reference,5.2.cusparse legacy types reference,8
5.2.1.cusparseaction_t,#cusparseaction-t,5.2.1.cusparseaction_t this type indicates whether the operation is performed only on indices or on data and indices.,1
5.2.2.cusparsematdescr_t,#cusparsematdescr-t,5.2.2.cusparsematdescr_t this structure is used to describe the shape and properties of a matrix.,1
5.2.3.cusparsematrixtype_t,#cusparsematrixtype-t,"5.2.3.cusparsematrixtype_t this type indicates the type of matrix stored in sparse storage. notice that for symmetric, hermitian and triangular matrices only their lower or upper part is assumed to be stored. the whole idea of matrix type and fill mode is to keep minimum storage for symmetric/hermitian matrix, and also to take advantage of symmetric property on spmv (sparse matrix vector multiplication). to computey=a*xwhenais symmetric and only lower triangular part is stored, two steps are needed. first step is to computey=(l+d)*xand second step is to computey=l^t*x+y. given the fact that the transpose operationy=l^t*xis 10x slower than non-transpose versiony=l*x, the symmetric property does not show up any performance gain. it is better for the user to extend the symmetric matrix to a general matrix and applyy=a*xwith matrix typecusparse_matrix_type_general. in general, spmv, preconditioners (incomplete cholesky or incomplete lu) and triangular solver are combined together in iterative solvers, for example pcg and gmres. if the user always uses general matrix (instead of symmetric matrix), there is no need to support other than general matrix in preconditioners. therefore the new routines,[bsr|csr]sv2(triangular solver),[bsr|csr]ilu02(incomplete lu) and[bsr|csr]ic02(incomplete cholesky), only support matrix typecusparse_matrix_type_general.",1
5.2.4.cusparsecolorinfo_t [deprecated],#cusparsecolorinfo-t-deprecated,5.2.4.cusparsecolorinfo_t [deprecated] this is a pointer type to an opaque structure holding the information used incsrcolor().,8
5.2.5.cusparsesolvepolicy_t [deprecated],#cusparsesolvepolicy-t-deprecated,"5.2.5.cusparsesolvepolicy_t [deprecated] this type indicates whether level information is generated and used incsrsv2,csric02,csrilu02,bsrsv2,bsric02andbsrilu02.",6
5.2.6.bsric02info_t [deprecated],#bsric02info-t-deprecated,"5.2.6.bsric02info_t [deprecated] this is a pointer type to an opaque structure holding the information used inbsric02_buffersize(),bsric02_analysis(), andbsric02().",6
5.2.7.bsrilu02info_t [deprecated],#bsrilu02info-t-deprecated,"5.2.7.bsrilu02info_t [deprecated] this is a pointer type to an opaque structure holding the information used inbsrilu02_buffersize(),bsrilu02_analysis(), andbsrilu02().",6
5.2.8.bsrsm2info_t [deprecated],#bsrsm2info-t-deprecated,"5.2.8.bsrsm2info_t [deprecated] this is a pointer type to an opaque structure holding the information used inbsrsm2_buffersize(),bsrsm2_analysis(), andbsrsm2_solve().",6
5.2.9.bsrsv2info_t [deprecated],#bsrsv2info-t-deprecated,"5.2.9.bsrsv2info_t [deprecated] this is a pointer type to an opaque structure holding the information used inbsrsv2_buffersize(),bsrsv2_analysis(), andbsrsv2_solve().",6
5.2.10.csric02info_t [deprecated],#csric02info-t-deprecated,"5.2.10.csric02info_t [deprecated] this is a pointer type to an opaque structure holding the information used incsric02_buffersize(),csric02_analysis(), andcsric02().",6
5.2.11.csrilu02info_t [deprecated],#csrilu02info-t-deprecated,"5.2.11.csrilu02info_t [deprecated] this is a pointer type to an opaque structure holding the information used incsrilu02_buffersize(),csrilu02_analysis(), andcsrilu02().",6
5.3.cusparse helper function reference,#cusparse-helper-function-reference,5.3.cusparse helper function reference the cusparse helper functions are described in this section.,1
5.3.1.cusparsecreatecolorinfo() [deprecated],#cusparsecreatecolorinfo-deprecated,5.3.1.cusparsecreatecolorinfo() [deprecated] this function creates and initializes thecusparsecolorinfo_tstructure todefaultvalues. input seecusparsestatus_tfor the description of the return status,1
5.3.2.cusparsecreatematdescr(),#cusparsecreatematdescr,"5.3.2.cusparsecreatematdescr() this function initializes the matrix descriptor. it sets the fieldsmatrixtypeandindexbaseto thedefaultvaluescusparse_matrix_type_generalandcusparse_index_base_zero, respectively, while leaving other fields uninitialized. input seecusparsestatus_tfor the description of the return status",1
5.3.3.cusparsedestroycolorinfo() [deprecated],#cusparsedestroycolorinfo-deprecated,5.3.3.cusparsedestroycolorinfo() [deprecated] this function destroys and releases any memory required by the structure. input seecusparsestatus_tfor the description of the return status,6
5.3.4.cusparsedestroymatdescr(),#cusparsedestroymatdescr,5.3.4.cusparsedestroymatdescr() this function releases the memory allocated for the matrix descriptor. input seecusparsestatus_tfor the description of the return status,1
5.3.5.cusparsegetmatdiagtype(),#cusparsegetmatdiagtype,5.3.5.cusparsegetmatdiagtype() this function returns thediagtypefield of the matrix descriptordescra. input returned,1
5.3.6.cusparsegetmatfillmode(),#cusparsegetmatfillmode,5.3.6.cusparsegetmatfillmode() this function returns thefillmodefield of the matrix descriptordescra. input returned,1
5.3.7.cusparsegetmatindexbase(),#cusparsegetmatindexbase,5.3.7.cusparsegetmatindexbase() this function returns theindexbasefield of the matrix descriptordescra. input returned,1
5.3.8.cusparsegetmattype(),#cusparsegetmattype,5.3.8.cusparsegetmattype() this function returns thematrixtypefield of the matrix descriptordescra. input returned,1
5.3.9.cusparsesetmatdiagtype(),#cusparsesetmatdiagtype,5.3.9.cusparsesetmatdiagtype() this function sets thediagtypefield of the matrix descriptordescra. input output seecusparsestatus_tfor the description of the return status,1
5.3.10.cusparsesetmatfillmode(),#cusparsesetmatfillmode,5.3.10.cusparsesetmatfillmode() this function sets thefillmodefield of the matrix descriptordescra. input output seecusparsestatus_tfor the description of the return status,1
5.3.11.cusparsesetmatindexbase(),#cusparsesetmatindexbase,5.3.11.cusparsesetmatindexbase() this function sets theindexbasefield of the matrix descriptordescra. input output seecusparsestatus_tfor the description of the return status,1
5.3.12.cusparsesetmattype(),#cusparsesetmattype,5.3.12.cusparsesetmattype() this function sets thematrixtypefield of the matrix descriptordescra. input output seecusparsestatus_tfor the description of the return status,1
5.3.13.cusparsecreatecsric02info() [deprecated],#cusparsecreatecsric02info-deprecated,5.3.13.cusparsecreatecsric02info() [deprecated] this function creates and initializes the solve and analysis structure of incomplete cholesky todefaultvalues. input seecusparsestatus_tfor the description of the return status,1
5.3.14.cusparsedestroycsric02info() [deprecated],#cusparsedestroycsric02info-deprecated,5.3.14.cusparsedestroycsric02info() [deprecated] this function destroys and releases any memory required by the structure. input seecusparsestatus_tfor the description of the return status,6
5.3.15.cusparsecreatecsrilu02info() [deprecated],#cusparsecreatecsrilu02info-deprecated,5.3.15.cusparsecreatecsrilu02info() [deprecated] this function creates and initializes the solve and analysis structure of incomplete lu todefaultvalues. input seecusparsestatus_tfor the description of the return status,1
5.3.16.cusparsedestroycsrilu02info() [deprecated],#cusparsedestroycsrilu02info-deprecated,5.3.16.cusparsedestroycsrilu02info() [deprecated] this function destroys and releases any memory required by the structure. input seecusparsestatus_tfor the description of the return status,6
5.3.17.cusparsecreatebsrsv2info() [deprecated],#cusparsecreatebsrsv2info-deprecated,5.3.17.cusparsecreatebsrsv2info() [deprecated] this function creates and initializes the solve and analysis structure of bsrsv2 todefaultvalues. input seecusparsestatus_tfor the description of the return status,1
5.3.18.cusparsedestroybsrsv2info() [deprecated],#cusparsedestroybsrsv2info-deprecated,5.3.18.cusparsedestroybsrsv2info() [deprecated] this function destroys and releases any memory required by the structure. input seecusparsestatus_tfor the description of the return status,6
5.3.19.cusparsecreatebsrsm2info() [deprecated],#cusparsecreatebsrsm2info-deprecated,5.3.19.cusparsecreatebsrsm2info() [deprecated] this function creates and initializes the solve and analysis structure of bsrsm2 todefaultvalues. input seecusparsestatus_tfor the description of the return status,1
5.3.20.cusparsedestroybsrsm2info() [deprecated],#cusparsedestroybsrsm2info-deprecated,5.3.20.cusparsedestroybsrsm2info() [deprecated] this function destroys and releases any memory required by the structure. input seecusparsestatus_tfor the description of the return status,6
5.3.21.cusparsecreatebsric02info() [deprecated],#cusparsecreatebsric02info-deprecated,5.3.21.cusparsecreatebsric02info() [deprecated] this function creates and initializes the solve and analysis structure of block incomplete cholesky todefaultvalues. input seecusparsestatus_tfor the description of the return status,1
5.3.22.cusparsedestroybsric02info() [deprecated],#cusparsedestroybsric02info-deprecated,5.3.22.cusparsedestroybsric02info() [deprecated] this function destroys and releases any memory required by the structure. input seecusparsestatus_tfor the description of the return status,6
5.3.23.cusparsecreatebsrilu02info() [deprecated],#cusparsecreatebsrilu02info-deprecated,5.3.23.cusparsecreatebsrilu02info() [deprecated] this function creates and initializes the solve and analysis structure of block incomplete lu todefaultvalues. input seecusparsestatus_tfor the description of the return status,1
5.3.24.cusparsedestroybsrilu02info() [deprecated],#cusparsedestroybsrilu02info-deprecated,5.3.24.cusparsedestroybsrilu02info() [deprecated] this function destroys and releases any memory required by the structure. input seecusparsestatus_tfor the description of the return status,6
5.3.25.cusparsecreatepruneinfo() [deprecated],#cusparsecreatepruneinfo-deprecated,5.3.25.cusparsecreatepruneinfo() [deprecated] this function creates and initializes structure ofprunetodefaultvalues. input seecusparsestatus_tfor the description of the return status,1
5.3.26.cusparsedestroypruneinfo() [deprecated],#cusparsedestroypruneinfo-deprecated,5.3.26.cusparsedestroypruneinfo() [deprecated] this function destroys and releases any memory required by the structure. input seecusparsestatus_tfor the description of the return status,6
5.4.cusparse level 2 function reference,#cusparse-level-2-function-reference,5.4.cusparse level 2 function reference this chapter describes the sparse linear algebra functions that perform operations between sparse matrices and dense vectors.,1
5.4.1.cusparse<t>bsrmv(),#bsrmv,"5.4.1.cusparse<t>bsrmv() this function performs the matrix-vector operation where\(a\text{ is an }(mb \ast blockdim) \times (nb \ast blockdim)\)sparse matrix that is defined in bsr storage format by the three arraysbsrval,bsrrowptr, andbsrcolind);xandyare vectors;\(\alpha\text{ and }\beta\)are scalars; and \(\text{op}(a) = \begin{cases}
a & \text{if trans == cusparse_operation_non_transpose} \\
a^{t} & \text{if trans == cusparse_operation_transpose} \\
a^{h} & \text{if trans == cusparse_operation_conjugate_transpose} \\
\end{cases}\) bsrmv()has the following properties: several comments onbsrmv(): for example, suppose the user has a csr format and wants to trybsrmv(), the following code demonstrates how to usecsr2bsr()conversion andbsrmv()multiplication in single precision. input output seecusparsestatus_tfor the description of the return status.",1
5.4.2.cusparse<t>bsrxmv()[deprecated],#bsrxmv,"5.4.2.cusparse<t>bsrxmv()[deprecated] this function performs absrmvand a mask operation where\(a\text{ is an }(mb \ast blockdim) \times (nb \ast blockdim)\)sparse matrix that is defined in bsrx storage format by the four arraysbsrval,bsrrowptr,bsrendptr, andbsrcolind);xandyare vectors;\(\alpha\text{~and~}\beta\)are scalars; and \(\text{op}(a) = \begin{cases}
a & \text{if trans == cusparse_operation_non_transpose} \\
a^{t} & \text{if trans == cusparse_operation_transpose} \\
a^{h} & \text{if trans == cusparse_operation_conjugate_transpose} \\
\end{cases}\) the mask operation is defined by arraybsrmaskptrwhich contains updated block row indices of\(y\). if row\(i\)is not specified inbsrmaskptr, thenbsrxmv()does not touch row block\(i\)of\(a\)and\(y\). for example, consider the\(2 \times 3\)block matrix\(a\): and its one-based bsr format (three vector form) is suppose we want to do the followingbsrmvoperation on a matrix\(\overset{}{a}\)which is slightly different from\(a\). we dont need to create another bsr format for the new matrix\(\overset{}{a}\), all that we should do is to keepbsrvalandbsrcolindunchanged, but modifybsrrowptrand add an additional arraybsrendptrwhich points to the last nonzero elements per row of\(\overset{}{a}\)plus 1. for example, the followingbsrrowptrandbsrendptrcan represent matrix\(\overset{}{a}\): further we can use a mask operator (specified by arraybsrmaskptr) to update particular block row indices of\(y\)only because\(y_{1}\)is never changed. in this case,bsrmaskptr\(=\)[2] andsizeofmask=1. the mask operator is equivalent to the following operation: if a block row is not present in thebsrmaskptr, then no calculation is performed on that row, and the corresponding value inyis unmodified. the question mark ? is used to inidcate row blocks not inbsrmaskptr. in this case, first row block is not present inbsrmaskptr, sobsrrowptr[0]andbsrendptr[0]are not touched also. bsrxmv()has the following properties: a couple of comments onbsrxmv(): input seecusparsestatus_tfor the description of the return status.",1
5.4.3.cusparse<t>bsrsv2_buffersize()[deprecated],#bsrsv2_bufferSize,"5.4.3.cusparse<t>bsrsv2_buffersize()[deprecated] this function returns size of the buffer used inbsrsv2, a new sparse triangular linear systemop(a)*y=\(\alpha\)x. ais an(mb*blockdim)x(mb*blockdim)sparse matrix that is defined in bsr storage format by the three arraysbsrvala,bsrrowptra, andbsrcolinda);xandyare the right-hand-side and the solution vectors;\(\alpha\)is a scalar; and \(\text{op}(a) = \begin{cases}
a & \text{if trans == cusparse_operation_non_transpose} \\
a^{t} & \text{if trans == cusparse_operation_transpose} \\
a^{h} & \text{if trans == cusparse_operation_conjugate_transpose} \\
\end{cases}\) although there are six combinations in terms of parametertransand the upper (lower) triangular part ofa,bsrsv2_buffersize()returns the maximum size buffer among these combinations. the buffer size depends on the dimensionsmb,blockdim, and the number of nonzero blocks of the matrixnnzb. if the user changes the matrix, it is necessary to callbsrsv2_buffersize()again to have the correct buffer size; otherwise a segmentation fault may occur. input output seecusparsestatus_tfor the description of the return status.",1
5.4.4.cusparse<t>bsrsv2_analysis()[deprecated],#bsrsv2_analysis,"5.4.4.cusparse<t>bsrsv2_analysis()[deprecated] this function performs the analysis phase ofbsrsv2, a new sparse triangular linear systemop(a)*y=\(\alpha\)x. ais an(mb*blockdim)x(mb*blockdim)sparse matrix that is defined in bsr storage format by the three arraysbsrvala,bsrrowptra, andbsrcolinda);xandyare the right-hand side and the solution vectors;\(\alpha\)is a scalar; and \(\text{op}(a) = \begin{cases}
a & \text{if trans == cusparse_operation_non_transpose} \\
a^{t} & \text{if trans == cusparse_operation_transpose} \\
a^{h} & \text{if trans == cusparse_operation_conjugate_transpose} \\
\end{cases}\) the block of bsr format is of sizeblockdim*blockdim, stored as column-major or row-major as determined by parameterdira, which is eithercusparse_direction_columnorcusparse_direction_row. the matrix type must becusparse_matrix_type_general, and the fill mode and diagonal type are ignored. it is expected that this function will be executed only once for a given matrix and a particular operation type. this function requires a buffer size returned bybsrsv2_buffersize(). the address ofpbuffermust be multiple of 128 bytes. if it is not,cusparse_status_invalid_valueis returned. functionbsrsv2_analysis()reports a structural zero and computes level information, which stored in the opaque structureinfo. the level information can extract more parallelism for a triangular solver. howeverbsrsv2_solve()can be done without level information. to disable level information, the user needs to specify the policy of the triangular solver ascusparse_solve_policy_no_level. functionbsrsv2_analysis()always reports the first structural zero, even when parameterpolicyiscusparse_solve_policy_no_level. no structural zero is reported ifcusparse_diag_type_unitis specified, even if blocka(j,j)is missing for somej. the user needs to callcusparsexbsrsv2_zeropivot()to know where the structural zero is. it is the users choice whether to callbsrsv2_solve()ifbsrsv2_analysis()reports a structural zero. in this case, the user can still callbsrsv2_solve(), which will return a numerical zero at the same position as a structural zero. however the resultxis meaningless. input output seecusparsestatus_tfor the description of the return status.",1
5.4.5.cusparse<t>bsrsv2_solve()[deprecated],#bsrsv2_solve,"5.4.5.cusparse<t>bsrsv2_solve()[deprecated] this function performs the solve phase ofbsrsv2, a new sparse triangular linear systemop(a)*y=\(\alpha\)x. ais an(mb*blockdim)x(mb*blockdim)sparse matrix that is defined in bsr storage format by the three arraysbsrvala,bsrrowptra, andbsrcolinda);xandyare the right-hand-side and the solution vectors;\(\alpha\)is a scalar; and \(\text{op}(a) = \begin{cases}
a & \text{if trans == cusparse_operation_non_transpose} \\
a^{t} & \text{if trans == cusparse_operation_transpose} \\
a^{h} & \text{if trans == cusparse_operation_conjugate_transpose} \\
\end{cases}\) the block in bsr format is of sizeblockdim*blockdim, stored as column-major or row-major as determined by parameterdira, which is eithercusparse_direction_columnorcusparse_direction_row. the matrix type must becusparse_matrix_type_general, and the fill mode and diagonal type are ignored. functionbsrsv02_solve()can support an arbitraryblockdim. this function may be executed multiple times for a given matrix and a particular operation type. this function requires a buffer size returned bybsrsv2_buffersize(). the address ofpbuffermust be multiple of 128 bytes. if it is not,cusparse_status_invalid_valueis returned. althoughbsrsv2_solve()can be done without level information, the user still needs to be aware of consistency. ifbsrsv2_analysis()is called with policycusparse_solve_policy_use_level,bsrsv2_solve()can be run with or without levels. on the other hand, ifbsrsv2_analysis()is called withcusparse_solve_policy_no_level,bsrsv2_solve()can only acceptcusparse_solve_policy_no_level; otherwise,cusparse_status_invalid_valueis returned. the level information may not improve the performance, but may spend extra time doing analysis. for example, a tridiagonal matrix has no parallelism. in this case,cusparse_solve_policy_no_levelperforms better thancusparse_solve_policy_use_level. if the user has an iterative solver, the best approach is to dobsrsv2_analysis()withcusparse_solve_policy_use_levelonce. then dobsrsv2_solve()withcusparse_solve_policy_no_levelin the first run, and withcusparse_solve_policy_use_levelin the second run, and pick the fastest one to perform the remaining iterations. functionbsrsv02_solve()has the same behavior ascsrsv02_solve(). that is,bsr2csr(bsrsv02(a))=csrsv02(bsr2csr(a)). the numerical zero ofcsrsv02_solve()means there exists some zeroa(j,j). the numerical zero ofbsrsv02_solve()means there exists some blocka(j,j)that is not invertible. functionbsrsv2_solve()reports the first numerical zero, including a structural zero. no numerical zero is reported ifcusparse_diag_type_unitis specified, even ifa(j,j)is not invertible for somej. the user needs to callcusparsexbsrsv2_zeropivot()to know where the numerical zero is. the function supports the following properties ifpbuffer!=null for example, suppose l is a lower triangular matrix with unit diagonal, then the following code solvesl*y=xby level information. input output seecusparsestatus_tfor the description of the return status.",1
5.4.6.cusparsexbsrsv2_zeropivot() [deprecated],#cusparsexbsrsv2-zeropivot-deprecated,"5.4.6.cusparsexbsrsv2_zeropivot() [deprecated] if the returned error code iscusparse_status_zero_pivot,position=jmeansa(j,j)is either structural zero or numerical zero (singular block). otherwiseposition=-1. thepositioncan be 0-based or 1-based, the same as the matrix. functioncusparsexbsrsv2_zeropivot()is a blocking call. it callscudadevicesynchronize()to make sure all previous kernels are done. thepositioncan be in the host memory or device memory. the user can set the proper mode withcusparsesetpointermode(). input output seecusparsestatus_tfor the description of the return status",1
5.4.7.cusparse<t>gemvi(),#cusparse-t-gemvi,"5.4.7.cusparse<t>gemvi() this function performs the matrix-vector operation ais anmndense matrix and a sparse vectorxthat is defined in a sparse storage format by the two arraysxval,xindof lengthnnz, andyis a dense vector;\(\alpha\)and\(\beta\)are scalars; and \(\text{op}(a) = \begin{cases}
a & \text{if trans == cusparse_operation_non_transpose} \\
a^{t} & \text{if trans == cusparse_operation_transpose} \\
\end{cases}\) to simplify the implementation, we have not (yet) optimized the transpose multiple case. we recommend the following for users interested in this case. the functioncusparse<t>gemvi_buffersize()returns the size of buffer used incusparse<t>gemvi(). input output seecusparsestatus_tfor the description of the return status.",1
5.5.cusparse level 3 function reference,#cusparse-level-3-function-reference,5.5.cusparse level 3 function reference this chapter describes sparse linear algebra functions that perform operations between sparse and (usually tall) dense matrices.,1
5.5.1.cusparse<t>bsrmm(),#bsrmm,"5.5.1.cusparse<t>bsrmm() this function performs one of the following matrix-matrix operations: ais anmbkbsparse matrix that is defined in bsr storage format by the three arraysbsrvala,bsrrowptra, andbsrcolinda;bandcare dense matrices;\(\alpha\text{~and~}\beta\)are scalars; and \(\text{op}(a) = \begin{cases}
a & \text{if transa == cusparse_operation_non_transpose} \\
a^{t} & \text{if transa == cusparse_operation_transpose (not\ supported)} \\
a^{h} & \text{if transa == cusparse_operation_conjugate_transpose (not supported)} \\
\end{cases}\) and \(\text{op}(b) = \begin{cases}
b & \text{if transb == cusparse_operation_non_transpose} \\
b^{t} & \text{if transb == cusparse_operation_transpose} \\
b^{h} & \text{if transb == cusparse_operation_conjugate_transpose (not supported)} \\
\end{cases}\) the function has the following limitations: the motivation oftranspose(b)is to improve memory access of matrixb. the computational pattern ofa*transpose(b)with matrixbin column-major order is equivalent toa*bwith matrixbin row-major order. in practice, no operation in an iterative solver or eigenvalue solver usesa*transpose(b). however, we can performa*transpose(transpose(b))which is the same asa*b. for example, supposeaismb*kb,bisk*nandcism*n, the following code shows usage ofcusparsedbsrmm(). instead of usinga*b, our proposal is to transposebtobtby first callingcublas<t>geam(), and then to performa*transpose(bt). bsrmm()has the following properties: input output seecusparsestatus_tfor the description of the return status",1
5.5.2.cusparse<t>bsrsm2_buffersize()[deprecated],#bsrsm2_bufferSize,"5.5.2.cusparse<t>bsrsm2_buffersize()[deprecated] this function returns size of buffer used inbsrsm2(), a new sparse triangular linear systemop(a)*op(x)=\(\alpha\)op(b). ais an(mb*blockdim)x(mb*blockdim)sparse matrix that is defined in bsr storage format by the three arraysbsrvala,bsrrowptra, andbsrcolinda);bandxare the right-hand-side and the solution matrices;\(\alpha\)is a scalar; and \(\text{op}(a) == \text{cusparse_operation_non_transpose}\) although there are six combinations in terms of parametertransand the upper (and lower) triangular part ofa,bsrsm2_buffersize()returns the maximum size of the buffer among these combinations. the buffer size depends on dimensionmb,blockdimand the number of nonzeros of the matrix,nnzb. if the user changes the matrix, it is necessary to callbsrsm2_buffersize()again to get the correct buffer size, otherwise a segmentation fault may occur. input output seecusparsestatus_tfor the description of the return status",1
5.5.3.cusparse<t>bsrsm2_analysis()[deprecated],#bsrsm2_analysis,"5.5.3.cusparse<t>bsrsm2_analysis()[deprecated] this function performs the analysis phase ofbsrsm2(), a new sparse triangular linear systemop(a)*op(x)=\(\alpha\)op(b). ais an(mb*blockdim)x(mb*blockdim)sparse matrix that is defined in bsr storage format by the three arraysbsrvala,bsrrowptra, andbsrcolinda);bandxare the right-hand-side and the solution matrices;\(\alpha\)is a scalar; and \(\text{op}(a) == \text{cusparse_operation_non_transpose}\) and \(\text{op}(x) = \begin{cases}
x & \text{if transx == cusparse_operation_non_transpose} \\
x^{t} & \text{if transx == cusparse_operation_transpose} \\
x^{h} & \text{if transx == cusparse_operation_conjugate_transpose (not supported)} \\
\end{cases}\) andop(b)andop(x)are equal. the block of bsr format is of sizeblockdim*blockdim, stored in column-major or row-major as determined by parameterdira, which is eithercusparse_direction_roworcusparse_direction_column. the matrix type must becusparse_matrix_type_general, and the fill mode and diagonal type are ignored. it is expected that this function will be executed only once for a given matrix and a particular operation type. this function requires the buffer size returned bybsrsm2_buffersize(). the address ofpbuffermust be multiple of 128 bytes. if not,cusparse_status_invalid_valueis returned. functionbsrsm2_analysis()reports a structural zero and computes the level information stored in opaque structureinfo. the level information can extract more parallelism during a triangular solver. howeverbsrsm2_solve()can be done without level information. to disable level information, the user needs to specify the policy of the triangular solver ascusparse_solve_policy_no_level. functionbsrsm2_analysis()always reports the first structural zero, even if the parameterpolicyiscusparse_solve_policy_no_level. besides, no structural zero is reported ifcusparse_diag_type_unitis specified, even if blocka(j,j)is missing for somej. the user must callcusparsexbsrsm2_query_zero_pivot()to know where the structural zero is. ifbsrsm2_analysis()reports a structural zero, the solve will return a numerical zero in the same position as the structural zero but this resultxis meaningless. input output seecusparsestatus_tfor the description of the return status",1
5.5.4.cusparse<t>bsrsm2_solve()[deprecated],#bsrsm2_solve,"5.5.4.cusparse<t>bsrsm2_solve()[deprecated] this function performs the solve phase of the solution of a sparse triangular linear system: ais an(mb*blockdim)x(mb*blockdim)sparse matrix that is defined in bsr storage format by the three arraysbsrvala,bsrrowptra, andbsrcolinda);bandxare the right-hand-side and the solution matrices;\(\alpha\)is a scalar, and \(\text{op}(a) == \text{cusparse_operation_non_transpose}\) and \(\text{op}(x) = \begin{cases}
x & \text{if transx == cusparse_operation_non_transpose} \\
x^{t} & \text{if transx == cusparse_operation_transpose} \\
x^{h} & \text{not supported} \\
\end{cases}\) onlyop(a)=ais supported. op(b)andop(x)must be performed in the same way. in other words, ifop(b)=b,op(x)=x. the block of bsr format is of sizeblockdim*blockdim, stored as column-major or row-major as determined by parameterdira, which is eithercusparse_direction_roworcusparse_direction_column. the matrix type must becusparse_matrix_type_general, and the fill mode and diagonal type are ignored. functionbsrsm02_solve()can support an arbitraryblockdim. this function may be executed multiple times for a given matrix and a particular operation type. this function requires the buffer size returned bybsrsm2_buffersize(). the address ofpbuffermust be multiple of 128 bytes. if it is not,cusparse_status_invalid_valueis returned. althoughbsrsm2_solve()can be done without level information, the user still needs to be aware of consistency. ifbsrsm2_analysis()is called with policycusparse_solve_policy_use_level,bsrsm2_solve()can be run with or without levels. on the other hand, ifbsrsm2_analysis()is called withcusparse_solve_policy_no_level,bsrsm2_solve()can only acceptcusparse_solve_policy_no_level; otherwise,cusparse_status_invalid_valueis returned. functionbsrsm02_solve()has the same behavior asbsrsv02_solve(), reporting the first numerical zero, including a structural zero. the user must callcusparsexbsrsm2_query_zero_pivot()to know where the numerical zero is. the motivation oftranspose(x)is to improve the memory access of matrixx. the computational pattern oftranspose(x)with matrixxin column-major order is equivalent toxwith matrixxin row-major order. in-place is supported and requires thatbandxpoint to the same memory block, andldb=ldx. the function supports the following properties ifpbuffer!=null: input output seecusparsestatus_tfor the description of the return status.",1
5.5.5.cusparsexbsrsm2_zeropivot() [deprecated],#cusparsexbsrsm2-zeropivot-deprecated,"5.5.5.cusparsexbsrsm2_zeropivot() [deprecated] if the returned error code iscusparse_status_zero_pivot,position=jmeansa(j,j)is either a structural zero or a numerical zero (singular block). otherwiseposition=-1. thepositioncan be 0-base or 1-base, the same as the matrix. functioncusparsexbsrsm2_zeropivot()is a blocking call. it callscudadevicesynchronize()to make sure all previous kernels are done. thepositioncan be in the host memory or device memory. the user can set the proper mode withcusparsesetpointermode(). input output seecusparsestatus_tfor the description of the return status.",1
5.6.cusparse extra function reference,#cusparse-extra-function-reference,5.6.cusparse extra function reference this chapter describes the extra routines used to manipulate sparse matrices.,1
5.6.1.cusparse<t>csrgeam2(),#csrgeam2,"5.6.1.cusparse<t>csrgeam2() this function performs following matrix-matrix operation wherea,b, andcaremnsparse matrices (defined in csr storage format by the three arrayscsrvala|csrvalb|csrvalc,csrrowptra|csrrowptrb|csrrowptrc, andcsrcolinda|csrcolindb|csrcolindcrespectively), and\(\alpha\text{~and~}\beta\)are scalars. sinceaandbhave different sparsity patterns, cusparse adopts a two-step approach to complete sparse matrixc. in the first step, the user allocatescsrrowptrcofm+1elements and uses functioncusparsexcsrgeam2nnz()to determinecsrrowptrcand the total number of nonzero elements. in the second step, the user gathersnnzc(number of nonzero elements of matrixc) from either(nnzc=*nnztotaldevhostptr)or(nnzc=csrrowptrc(m)-csrrowptrc(0))and allocatescsrvalc,csrcolindcofnnzcelements respectively, then finally calls functioncusparse[s|d|c|z]csrgeam2()to complete matrixc. the general procedure is as follows: several comments oncsrgeam2(): input output seecusparsestatus_tfor the description of the return status",1
5.7.cusparse preconditioners reference,#cusparse-preconditioners-reference,5.7.cusparse preconditioners reference this chapter describes the routines that implement different preconditioners.,1
5.7.1.incomplete cholesky factorization: level 0 [deprecated],#incomplete-cholesky-factorization-level-0-deprecated,5.7.1.incomplete cholesky factorization: level 0 [deprecated] different algorithms for ic0 are discussed in this section.,1
5.7.2.incomplete lu factorization: level 0 [deprecated],#incomplete-lu-factorization-level-0-deprecated,5.7.2.incomplete lu factorization: level 0 [deprecated] different algorithms for ilu0 are discussed in this section.,6
5.7.3.tridiagonal solve,#tridiagonal-solve,5.7.3.tridiagonal solve different algorithms for tridiagonal solve are discussed in this section.,1
5.7.4.batched tridiagonal solve,#batched-tridiagonal-solve,5.7.4.batched tridiagonal solve different algorithms for batched tridiagonal solve are discussed in this section.,1
5.7.5.batched pentadiagonal solve,#batched-pentadiagonal-solve,5.7.5.batched pentadiagonal solve different algorithms for batched pentadiagonal solve are discussed in this section.,1
5.8.cusparse reorderings reference,#cusparse-reorderings-reference,5.8.cusparse reorderings reference this chapter describes the reordering routines used to manipulate sparse matrices.,1
5.8.1.cusparse<t>csrcolor()[deprecated],#csrcol,"5.8.1.cusparse<t>csrcolor()[deprecated] this function performs the coloring of the adjacency graph associated with the matrix a stored in csr format. the coloring is an assignment of colors (integer numbers) to nodes, such that neighboring nodes have distinct colors. an approximate coloring algorithm is used in this routine, and is stopped when a certain percentage of nodes has been colored. the rest of the nodes are assigned distinct colors (an increasing sequence of integers numbers, starting from the last integer used previously). the last two auxiliary routines can be used to extract the resulting number of colors, their assignment and the associated reordering. the reordering is such that nodes that have been assigned the same color are reordered to be next to each other. the matrix a passed to this routine, must be stored as a general matrix and have a symmetric sparsity pattern. if the matrix is nonsymmetric the user should pass a+a^t as a parameter to this routine. input output seecusparsestatus_tfor the description of the return status.",1
5.9.cusparse format conversion reference,#cusparse-format-conversion-reference,"5.9.cusparse format conversion reference this chapter describes the conversion routines between different sparse and dense storage formats. coosort,csrsort,cscsort, andcsru2csrare sorting routines without malloc inside, the following table estimates the buffer size.",1
5.9.1.cusparse<t>bsr2csr(),#bsr2csr,"5.9.1.cusparse<t>bsr2csr() this function converts a sparse matrix in bsr format that is defined by the three arraysbsrvala,bsrrowptra, andbsrcolinda) into a sparse matrix in csr format that is defined by arrayscsrvalc,csrrowptrc, andcsrcolindc. letm(=mb*blockdim)be the number of rows ofaandn(=nb*blockdim)be number of columns ofa, thenaandcarem*nsparse matrices. the bsr format ofacontainsnnzb(=bsrrowptra[mb]-bsrrowptra[0])nonzero blocks, whereas the sparse matrixacontainsnnz(=nnzb*blockdim*blockdim)elements. the user must allocate enough space for arrayscsrrowptrc,csrcolindc, andcsrvalc. the requirements are as follows: csrrowptrcofm+1elements csrvalcofnnzelements csrcolindcofnnzelements the general procedure is as follows: input output seecusparsestatus_tfor the description of the return status.",1
5.9.2.cusparse<t>gebsr2gebsc(),#gebsr2gebsc,"5.9.2.cusparse<t>gebsr2gebsc() this function can be seen as the same ascsr2csc()when each block of sizerowblockdim*colblockdimis regarded as a scalar. this sparsity pattern of the result matrix can also be seen as the transpose of the original sparse matrix, but the memory layout of a block does not change. the user must callgebsr2gebsc_buffersize()to determine the size of the buffer required bygebsr2gebsc(), allocate the buffer, and pass the buffer pointer togebsr2gebsc(). input output seecusparsestatus_tfor the description of the return status.",1
5.9.3.cusparse<t>gebsr2gebsr(),#gebsr2gebsr,"5.9.3.cusparse<t>gebsr2gebsr() this function converts a sparse matrix in general bsr format that is defined by the three arraysbsrvala,bsrrowptra, andbsrcolindainto a sparse matrix in another general bsr format that is defined by arraysbsrvalc,bsrrowptrc, andbsrcolindc. ifrowblockdima=1andcolblockdima=1,cusparse[s|d|c|z]gebsr2gebsr()is the same ascusparse[s|d|c|z]csr2gebsr(). ifrowblockdimc=1andcolblockdimc=1,cusparse[s|d|c|z]gebsr2gebsr()is the same ascusparse[s|d|c|z]gebsr2csr(). ais anm*nsparse matrix wherem(=mb*rowblockdim)is the number of rows ofa, andn(=nb*colblockdim)is the number of columns ofa. the general bsr format ofacontainsnnzb(=bsrrowptra[mb]-bsrrowptra[0])nonzero blocks. the matrixcis also general bsr format with a different block size,rowblockdimc*colblockdimc. ifmis not a multiple ofrowblockdimc, ornis not a multiple ofcolblockdimc, zeros are filled in. the number of block rows ofcismc(=(m+rowblockdimc-1)/rowblockdimc). the number of block rows ofcisnc(=(n+colblockdimc-1)/colblockdimc). the number of nonzero blocks ofcisnnzc. the implementation adopts a two-step approach to do the conversion. first, the user allocatesbsrrowptrcofmc+1elements and uses functioncusparsexgebsr2gebsrnnz()to determine the number of nonzero block columns per block row of matrixc. second, the user gathersnnzc(number of non-zero block columns of matrixc) from either(nnzc=*nnztotaldevhostptr)or(nnzc=bsrrowptrc[mc]-bsrrowptrc[0])and allocatesbsrvalcofnnzc*rowblockdimc*colblockdimcelements andbsrcolindcofnnzcintegers. finally the functioncusparse[s|d|c|z]gebsr2gebsr()is called to complete the conversion. the user must callgebsr2gebsr_buffersize()to know the size of the buffer required bygebsr2gebsr(), allocate the buffer, and pass the buffer pointer togebsr2gebsr(). the general procedure is as follows: input output seecusparsestatus_tfor the description of the return status.",1
5.9.4.cusparse<t>gebsr2csr(),#gebsr2csr,"5.9.4.cusparse<t>gebsr2csr() this function converts a sparse matrix in general bsr format that is defined by the three arraysbsrvala,bsrrowptra, andbsrcolindainto a sparse matrix in csr format that is defined by arrayscsrvalc,csrrowptrc, andcsrcolindc. letm(=mb*rowblockdim)be number of rows ofaandn(=nb*colblockdim)be number of columns ofa, thenaandcarem*nsparse matrices. the general bsr format ofacontainsnnzb(=bsrrowptra[mb]-bsrrowptra[0])non-zero blocks, whereas sparse matrixacontainsnnz(=nnzb*rowblockdim*colblockdim)elements. the user must allocate enough space for arrayscsrrowptrc,csrcolindc, andcsrvalc. the requirements are as follows: csrrowptrcofm+1elements csrvalcofnnzelements csrcolindcofnnzelements the general procedure is as follows: input output seecusparsestatus_tfor the description of the return status.",1
5.9.5.cusparse<t>csr2gebsr(),#csr2gebsr,"5.9.5.cusparse<t>csr2gebsr() this function converts a sparse matrixain csr format (that is defined by arrayscsrvala,csrrowptra, andcsrcolinda) into a sparse matrixcin general bsr format (that is defined by the three arraysbsrvalc,bsrrowptrc, andbsrcolindc). the matrix a is am*nsparse matrix and matrixcis a(mb*rowblockdim)*(nb*colblockdim)sparse matrix, wheremb(=(m+rowblockdim-1)/rowblockdim)is the number of block rows ofc, andnb(=(n+colblockdim-1)/colblockdim)is the number of block columns ofc. the block ofcis of sizerowblockdim*colblockdim. ifmis not multiple ofrowblockdimornis not multiple ofcolblockdim, zeros are filled in. the implementation adopts a two-step approach to do the conversion. first, the user allocatesbsrrowptrcofmb+1elements and uses functioncusparsexcsr2gebsrnnz()to determine the number of nonzero block columns per block row. second, the user gathersnnzb(number of nonzero block columns of matrixc) from either(nnzb=*nnztotaldevhostptr)or(nnzb=bsrrowptrc[mb]-bsrrowptrc[0])and allocatesbsrvalcofnnzb*rowblockdim*colblockdimelements andbsrcolindcofnnzbintegers. finally functioncusparse[s|d|c|z]csr2gebsr()is called to complete the conversion. the user must obtain the size of the buffer required bycsr2gebsr()by callingcsr2gebsr_buffersize(), allocate the buffer, and pass the buffer pointer tocsr2gebsr(). the general procedure is as follows: the routinecusparsexcsr2gebsrnnz()has the following properties: the routinecusparse<t>csr2gebsr()has the following properties: input output seecusparsestatus_tfor the description of the return status.",1
5.9.6.cusparse<t>coo2csr(),#coo2csr,5.9.6.cusparse<t>coo2csr() this function converts the array containing the uncompressed row indices (corresponding to coo format) into an array of compressed row pointers (corresponding to csr format). it can also be used to convert the array containing the uncompressed column indices (corresponding to coo format) into an array of column pointers (corresponding to csc format). input output seecusparsestatus_tfor the description of the return status.,1
5.9.7.cusparse<t>csr2coo(),#csr2coo,5.9.7.cusparse<t>csr2coo() this function converts the array containing the compressed row pointers (corresponding to csr format) into an array of uncompressed row indices (corresponding to coo format). it can also be used to convert the array containing the compressed column indices (corresponding to csc format) into an array of uncompressed column indices (corresponding to coo format). input output seecusparsestatus_tfor the description of the return status.,1
5.9.8.cusparsecsr2cscex2(),#cusparsecsr2cscex2,"5.9.8.cusparsecsr2cscex2() this function converts a sparse matrix in csr format (that is defined by the three arrayscsrval,csrrowptr, andcsrcolind) into a sparse matrix in csc format (that is defined by arrayscscval,cscrowind, andcsccolptr). the resulting matrix can also be seen as the transpose of the original sparse matrix. notice that this routine can also be used to convert a matrix in csc format into a matrix in csr format. the routine requires extra storage proportional to the number of nonzero valuesnnz. it provides in output always the same matrix. it is executed asynchronously with respect to the host, and it may return control to the application on the host before the result is ready. the functioncusparsecsr2cscex2_buffersize()returns the size of the workspace needed bycusparsecsr2cscex2(). user needs to allocate a buffer of this size and give that buffer tocusparsecsr2cscex2()as an argument. ifnnz==0, thencsrcolind,csrval,cscval, andcscrowindcould havenullvalue. in this case,csccolptris set toidxbasefor all values. ifm==0orn==0, the pointers are not checked and the routine returnscusparse_status_success. input cusparsecsr2cscex2()supports the following data types: cusparsecsr2cscex2()supports the following algorithms (cusparsecsr2cscalg_t): cusparsecsr2cscex2()has the following properties: cusparsecsr2cscex2()supports the followingoptimizations: seecusparsestatus_tfor the description of the return status.",1
5.9.9.cusparse<t>nnz(),#nnz,5.9.9.cusparse<t>nnz() this function computes the number of nonzero elements per row or column and the total number of nonzero elements in a dense matrix. input output seecusparsestatus_tfor the description of the return status.,1
5.9.10.cusparsecreateidentitypermutation() [deprecated],#cusparsecreateidentitypermutation-deprecated,"5.9.10.cusparsecreateidentitypermutation() [deprecated] this function creates an identity map. the output parameterprepresents such map byp=0:1:(n-1). this function is typically used withcoosort,csrsort,cscsort. input output seecusparsestatus_tfor the description of the return status.",1
5.9.11.cusparsexcoosort(),#cusparsexcoosort,"5.9.11.cusparsexcoosort() this function sorts coo format. the sorting is in-place. also the user can sort by row or sort by column. ais anmnsparse matrix that is defined in coo storage format by the three arrayscoovals,coorows, andcoocols. there is no assumption for the base index of the matrix.coosortuses stable sort on signed integer, so the value ofcoorowsorcoocolscan be negative. this functioncoosort()requires buffer size returned bycoosort_buffersizeext(). the address ofpbuffermust be multiple of 128 bytes. if not,cusparse_status_invalid_valueis returned. the parameterpis both input and output. if the user wants to compute sortedcooval,pmust be set as 0:1:(nnz-1) beforecoosort(), and aftercoosort(), new sorted value array satisfiescooval_sorted=cooval(p). remark: the dimensionmandnare not used. if the user does not know the value ofmorn, just passes a value positive. this usually happens if the user only reads a coo array first and needs to decide the dimensionmornlater. input output seecusparsestatus_tfor the description of the return status please visitcusparse library samples - cusparsexcoosortbyrowfor a code example.",1
5.9.12.cusparsexcsrsort(),#cusparsexcsrsort,"5.9.12.cusparsexcsrsort() this function sorts csr format. the stable sorting is in-place. the matrix type is regarded ascusparse_matrix_type_generalimplicitly. in other words, any symmetric property is ignored. this functioncsrsort()requires buffer size returned bycsrsort_buffersizeext(). the address ofpbuffermust be multiple of 128 bytes. if not,cusparse_status_invalid_valueis returned. the parameterpis both input and output. if the user wants to compute sortedcsrval,pmust be set as 0:1:(nnz-1) beforecsrsort(), and aftercsrsort(), new sorted value array satisfiescsrval_sorted=csrval(p). the general procedure is as follows: input output seecusparsestatus_tfor the description of the return status.",1
5.9.13.cusparsexcscsort(),#cusparsexcscsort,"5.9.13.cusparsexcscsort() this function sorts csc format. the stable sorting is in-place. the matrix type is regarded ascusparse_matrix_type_generalimplicitly. in other words, any symmetric property is ignored. this functioncscsort()requires buffer size returned bycscsort_buffersizeext(). the address ofpbuffermust be multiple of 128 bytes. if not,cusparse_status_invalid_valueis returned. the parameterpis both input and output. if the user wants to compute sortedcscval,pmust be set as 0:1:(nnz-1) beforecscsort(), and aftercscsort(), new sorted value array satisfiescscval_sorted=cscval(p). the general procedure is as follows: input output seecusparsestatus_tfor the description of the return status.",1
5.9.14.cusparsexcsru2csr() [deprecated],#cusparsexcsru2csr-deprecated,"5.9.14.cusparsexcsru2csr() [deprecated] this function transfers unsorted csr format to csr format, and vice versa. the operation is in-place. this function is a wrapper ofcsrsortandgthr. the usecase is the following scenario. if the user has a matrixaof csr format which is unsorted, and implements his own code (which can be cpu or gpu kernel) based on this special order (for example, diagonal first, then lower triangle, then upper triangle), and wants to convert it to csr format when calling cusparse library, and then convert it back when doing something else on his/her kernel. for example, suppose the user wants to solve a linear systemax=bby the following iterative scheme the code heavily uses spmv and triangular solve. assume that the user has an in-house design of spmv (sparse matrix-vector multiplication) based on special order ofa. however the user wants to use the cusparse library for triangular solver. then the following code can work. the requirements of step 2 and step 5 are the operation is calledcsru2csr, which means unsorted csr to sorted csr. also we provide the inverse operation, calledcsr2csru. in order to keep the permutation vector invisible, we need an opaque structure calledcsru2csrinfo. then two functions (cusparsecreatecsru2csrinfo,cusparsedestroycsru2csrinfo) are used to initialize and to destroy the opaque structure. cusparse[s|d|c|z]csru2csr_buffersizeextreturns the size of the buffer. the permutation vectorpis also allcated insidecsru2csrinfo. the lifetime of the permutation vector is the same as the lifetime ofcsru2csrinfo. cusparse[s|d|c|z]csru2csrperforms forward transformation from unsorted csr to sorted csr. first call uses csrsort to generate the permutation vectorp, and subsequent call usespto do transformation. cusparse[s|d|c|z]csr2csruperforms backward transformation from sorted csr to unsorted csr.pis used to get unsorted form back. the routinecusparse<t>csru2csr()has the following properties: the routinecusparse<t>csr2csru()has the following properties ifpbuffer!=null: the following tables describe parameters ofcsr2csru_buffersizeextandcsr2csru. input output seecusparsestatus_tfor the description of the return status.",1
5.9.15.cusparsexprunedense2csr() [deprecated],#cusparsexprunedense2csr-deprecated,"5.9.15.cusparsexprunedense2csr() [deprecated] this function prunes a dense matrix to a sparse matrix with csr format. given a dense matrixaand a non-negative valuethreshold, the function returns a sparse matrixc, defined by the implementation adopts a two-step approach to do the conversion. first, the user allocatescsrrowptrcofm+1elements and uses functionprunedense2csrnnz()to determine the number of nonzeros columns per row. second, the user gathersnnzc(number of nonzeros of matrixc) from either(nnzc=*nnztotaldevhostptr)or(nnzc=csrrowptrc[m]-csrrowptrc[0])and allocatescsrvalcofnnzcelements andcsrcolindcofnnzcintegers. finally functionprunedense2csr()is called to complete the conversion. the user must obtain the size of the buffer required byprunedense2csr()by callingprunedense2csr_buffersizeext(), allocate the buffer, and pass the buffer pointer toprunedense2csr(). examples of prunechapter provides a simple example ofprunedense2csr(). the routinecusparse<t>prunedense2csrnnz()has the following properties: the routinecusparse<t>dprunedense2csr()has the following properties: input output seecusparsestatus_tfor the description of the return status.",1
5.9.16.cusparsexprunecsr2csr()  [deprecated],#cusparsexprunecsr2csr-deprecated,"5.9.16.cusparsexprunecsr2csr()  [deprecated] this function prunes a sparse matrix to a sparse matrix with csr format. given a sparse matrixaand a non-negative valuethreshold, the function returns a sparse matrixc, defined by the implementation adopts a two-step approach to do the conversion. first, the user allocatescsrrowptrcofm+1elements and uses functionprunecsr2csrnnz()to determine the number of nonzeros columns per row. second, the user gathersnnzc(number of nonzeros of matrixc) from either(nnzc=*nnztotaldevhostptr)or(nnzc=csrrowptrc[m]-csrrowptrc[0])and allocatescsrvalcofnnzcelements andcsrcolindcofnnzcintegers. finally functionprunecsr2csr()is called to complete the conversion. the user must obtain the size of the buffer required byprunecsr2csr()by callingprunecsr2csr_buffersizeext(), allocate the buffer, and pass the buffer pointer toprunecsr2csr(). examples of prunechapter provides a simple example ofprunecsr2csr(). the routinecusparse<t>prunecsr2csrnnz()has the following properties: the routinecusparse<t>prunecsr2csr()has the following properties: input output seecusparsestatus_tfor the description of the return status.",1
5.9.17.cusparsexprunedense2csrpercentage()  [deprecated],#cusparsexprunedense2csrpercentage-deprecated,"5.9.17.cusparsexprunedense2csrpercentage()  [deprecated] this function prunes a dense matrix to a sparse matrix by percentage. given a dense matrixaand a non-negative valuepercentage, the function computes sparse matrixcby the following three steps: step 1: sort absolute value ofain ascending order. step 2: choose threshold by the parameterpercentage step 3: callprunedense2csr()by with the parameterthreshold. the implementation adopts a two-step approach to do the conversion. first, the user allocatescsrrowptrcofm+1elements and uses functionprunedense2csrnnzbypercentage()to determine the number of nonzeros columns per row. second, the user gathersnnzc(number of nonzeros of matrixc) from either(nnzc=*nnztotaldevhostptr)or(nnzc=csrrowptrc[m]-csrrowptrc[0])and allocatescsrvalcofnnzcelements andcsrcolindcofnnzcintegers. finally functionprunedense2csrbypercentage()is called to complete the conversion. the user must obtain the size of the buffer required byprunedense2csrbypercentage()by callingprunedense2csrbypercentage_buffersizeext(), allocate the buffer, and pass the buffer pointer toprunedense2csrbypercentage(). remark 1: the value ofpercentagemust be not greater than 100. otherwise,cusparse_status_invalid_valueis returned. remark 2: the zeros ofaare not ignored. all entries are sorted, including zeros. this is different fromprunecsr2csrbypercentage() examples of prunechapter provides a simple example ofprunedense2csrnnzbypercentage(). the routinecusparse<t>prunedense2csrnnzbypercentage()has the following properties: the routinecusparse<t>prunedense2csrbypercentage()has the following properties: input output seecusparsestatus_tfor the description of the return status.",1
5.9.18.cusparsexprunecsr2csrbypercentage() [deprecated],#cusparsexprunecsr2csrbypercentage-deprecated,"5.9.18.cusparsexprunecsr2csrbypercentage() [deprecated] this function prunes a sparse matrix to a sparse matrix by percentage. given a sparse matrixaand a non-negative valuepercentage, the function computes sparse matrixcby the following three steps: step 1: sort absolute value ofain ascending order. step 2: choose threshold by the parameterpercentage step 3: callprunecsr2csr()by with the parameterthreshold. the implementation adopts a two-step approach to do the conversion. first, the user allocatescsrrowptrcofm+1elements and uses functionprunecsr2csrnnzbypercentage()to determine the number of nonzeros columns per row. second, the user gathersnnzc(number of nonzeros of matrixc) from either(nnzc=*nnztotaldevhostptr)or(nnzc=csrrowptrc[m]-csrrowptrc[0])and allocatescsrvalcofnnzcelements andcsrcolindcofnnzcintegers. finally functionprunecsr2csrbypercentage()is called to complete the conversion. the user must obtain the size of the buffer required byprunecsr2csrbypercentage()by callingprunecsr2csrbypercentage_buffersizeext(), allocate the buffer, and pass the buffer pointer toprunecsr2csrbypercentage(). remark 1: the value ofpercentagemust be not greater than 100. otherwise,cusparse_status_invalid_valueis returned. examples of prunechapter provides a simple example ofprunecsr2csrbypercentage(). the routinecusparse<t>prunecsr2csrnnzbypercentage()has the following properties: the routinecusparse<t>prunecsr2csrbypercentage()has the following properties: input output seecusparsestatus_tfor the description of the return status.",1
5.9.19.cusparse<t>nnz_compress()[deprecated],#nnz_compress,"5.9.19.cusparse<t>nnz_compress()[deprecated] this function is the step one to convert from csr format to compressed csr format. given a sparse matrix a and a non-negative value threshold, the function returns nnzperrow(the number of nonzeros columns per row) and nnzc(the total number of nonzeros) of a sparse matrix c, defined by a key assumption for the cucomplex and cudoublecomplex case is that this tolerance is given as the real part. for example tol = 1e-8 + 0*i and we extract cureal, that is the x component of this struct. input output seecusparsestatus_tfor the description of the return status.",1
6.cusparse generic apis,#cusparse-generic-apis,"6.cusparse generic apis the cusparse generic apis allow computing the most common sparse linear algebra operations, such as sparse matrix-vector (spmv) and sparse matrix-matrix multiplication (spmm), in a flexible way. the new apis have the following capabilities and features:",1
6.1.generic types reference,#generic-types-reference,6.1.generic types reference the cusparse generic type references are described in this section.,8
6.1.1.cusparseformat_t,#cusparseformat-t,"6.1.1.cusparseformat_t this type indicates the format of the sparse matrix.
seecusparse storage formatfor their description.",1
6.1.2.cusparseorder_t,#cusparseorder-t,6.1.2.cusparseorder_t this type indicates the memory layout of a dense matrix.,1
6.1.3.cusparseindextype_t,#cusparseindextype-t,6.1.3.cusparseindextype_t this type indicates the index type for representing the sparse matrix indices.,1
6.2.dense vector apis,#dense-vector-apis,6.2.dense vector apis the cusparse helper functions for dense vector descriptor are described in this section. see thedense vector formatsection for the detailed description of the storage format.,1
6.2.1.cusparsecreatednvec(),#cusparsecreatednvec,6.2.1.cusparsecreatednvec() this function initializes the dense vector descriptordnvecdescr. cusparsecreatednvec()has the following constraints: seecusparsestatus_tfor the description of the return status.,1
6.2.2.cusparsedestroydnvec(),#cusparsedestroydnvec,6.2.2.cusparsedestroydnvec() this function releases the host memory allocated for the dense vector descriptordnvecdescr. seecusparsestatus_tfor the description of the return status.,1
6.2.3.cusparsednvecget(),#cusparsednvecget,6.2.3.cusparsednvecget() this function returns the fields of the dense vector descriptordnvecdescr. seecusparsestatus_tfor the description of the return status.,1
6.2.4.cusparsednvecgetvalues(),#cusparsednvecgetvalues,6.2.4.cusparsednvecgetvalues() this function returns thevaluesfield of the dense vector descriptordnvecdescr. seecusparsestatus_tfor the description of the return status.,1
6.2.5.cusparsednvecsetvalues(),#cusparsednvecsetvalues,6.2.5.cusparsednvecsetvalues() this function set thevaluesfield of the dense vector descriptordnvecdescr. cusparsednvecsetvalues()has the following constraints: seecusparsestatus_tfor the description of the return status.,1
6.3.sparse vector apis,#sparse-vector-apis,6.3.sparse vector apis the cusparse helper functions for sparse vector descriptor are described in this section. see thesparse vector formatsection for the detailed description of the storage format.,1
6.3.1.cusparsecreatespvec(),#cusparsecreatespvec,6.3.1.cusparsecreatespvec() this function initializes the sparse matrix descriptorspvecdescr. cusparsecreatespvec()has the following constraints: seecusparsestatus_tfor the description of the return status.,1
6.3.2.cusparsedestroyspvec(),#cusparsedestroyspvec,6.3.2.cusparsedestroyspvec() this function releases the host memory allocated for the sparse vector descriptorspvecdescr. seecusparsestatus_tfor the description of the return status.,1
6.3.3.cusparsespvecget(),#cusparsespvecget,6.3.3.cusparsespvecget() this function returns the fields of the sparse vector descriptorspvecdescr. seecusparsestatus_tfor the description of the return status.,1
6.3.4.cusparsespvecgetindexbase(),#cusparsespvecgetindexbase,6.3.4.cusparsespvecgetindexbase() this function returns theidxbasefield of the sparse vector descriptorspvecdescr. seecusparsestatus_tfor the description of the return status.,1
6.3.5.cusparsespvecgetvalues(),#cusparsespvecgetvalues,6.3.5.cusparsespvecgetvalues() this function returns thevaluesfield of the sparse vector descriptorspvecdescr. seecusparsestatus_tfor the description of the return status.,1
6.3.6.cusparsespvecsetvalues(),#cusparsespvecsetvalues,6.3.6.cusparsespvecsetvalues() this function set thevaluesfield of the sparse vector descriptorspvecdescr. cusparsednvecsetvalues()has the following constraints: seecusparsestatus_tfor the description of the return status.,1
6.4.dense matrix apis,#dense-matrix-apis,6.4.dense matrix apis the cusparse helper functions for dense matrix descriptor are described in this section. see thedense matrix formatsection for the detailed description of the storage format.,1
6.4.1.cusparsecreatednmat(),#cusparsecreatednmat,6.4.1.cusparsecreatednmat() the function initializes the dense matrix descriptordnmatdescr. cusparsecreatednmat()has the following constraints: seecusparsestatus_tfor the description of the return status.,1
6.4.2.cusparsedestroydnmat(),#cusparsedestroydnmat,6.4.2.cusparsedestroydnmat() this function releases the host memory allocated for the dense matrix descriptordnmatdescr. seecusparsestatus_tfor the description of the return status.,1
6.4.3.cusparsednmatget(),#cusparsednmatget,6.4.3.cusparsednmatget() this function returns the fields of the dense matrix descriptordnmatdescr. seecusparsestatus_tfor the description of the return status.,1
6.4.4.cusparsednmatgetvalues(),#cusparsednmatgetvalues,6.4.4.cusparsednmatgetvalues() this function returns thevaluesfield of the dense matrix descriptordnmatdescr. seecusparsestatus_tfor the description of the return status.,1
6.4.5.cusparsednmatsetvalues(),#cusparsednmatsetvalues,6.4.5.cusparsednmatsetvalues() this function sets thevaluesfield of the dense matrix descriptordnmatdescr. cusparsednmatsetvalues()has the following constraints: seecusparsestatus_tfor the description of the return status.,1
6.4.6.cusparsednmatgetstridedbatch(),#cusparsednmatgetstridedbatch,6.4.6.cusparsednmatgetstridedbatch() the function returns the number of batches and the batch stride of the dense matrix descriptordnmatdescr. seecusparsestatus_tfor the description of the return status.,1
6.4.7.cusparsednmatsetstridedbatch(),#cusparsednmatsetstridedbatch,6.4.7.cusparsednmatsetstridedbatch() the function sets the number of batches and the batch stride of the dense matrix descriptordnmatdescr. seecusparsestatus_tfor the description of the return status.,1
6.5.sparse matrix apis,#sparse-matrix-apis,"6.5.sparse matrix apis the cusparse helper functions for sparse matrix descriptor are described in this section. see thecoo,csr,csc,sell,bsr,blocked-ellsections for the detailed description of the storage formats.",1
6.5.1.coordinate (coo),#id1,6.5.1.coordinate (coo),9
6.5.2.compressed sparse row (csr),#id2,6.5.2.compressed sparse row (csr),1
6.5.3.compressed sparse column (csc),#id3,6.5.3.compressed sparse column (csc),1
6.5.4.blocked-ellpack (blocked-ell),#id4,6.5.4.blocked-ellpack (blocked-ell),9
6.5.5.sliced-ellpack (sell),#id5,6.5.5.sliced-ellpack (sell),9
6.5.6.block sparse row (bsr),#id6,6.5.6.block sparse row (bsr),1
6.5.7.all sparse formats,#all-sparse-formats,6.5.7.all sparse formats,9
6.6.generic api functions,#generic-api-functions,6.6.generic api functions,6
6.6.1.cusparseaxpby(),#cusparseaxpby,"6.6.1.cusparseaxpby() the function computes the sum of a sparse vectorvecxand a dense vectorvecy. in other words, cusparseaxpbysupports the following index type for representing the sparse vectorvecx: cusparseaxpbysupports the following data types: uniform-precision computation: mixed-precision computation: cusparseaxpby()has the following constraints: cusparseaxpby()has the following properties: cusparseaxpby()supports the followingoptimizations: seecusparsestatus_tfor the description of the return status. please visitcusparse library samples - cusparseaxpbyfor a code example.",1
6.6.2.cusparsegather(),#cusparsegather,"6.6.2.cusparsegather() the function gathers the elements of the dense vectorvecyinto the sparse vectorvecx in other words, cusparsegathersupports the following index type for representing the sparse vectorvecx: cusparsegathersupports the following data types: cusparsegather()has the following constraints: cusparsegather()has the following properties: cusparsegather()supports the followingoptimizations: seecusparsestatus_tfor the description of the return status. please visitcusparse library samples - cusparsegatherfor a code example.",1
6.6.3.cusparsescatter(),#cusparsescatter,"6.6.3.cusparsescatter() the function scatters the elements of the sparse vectorvecxinto the dense vectorvecy in other words, cusparsescattersupports the following index type for representing the sparse vectorvecx: cusparsescattersupports the following data types: cusparsescatter()has the following constraints: cusparsescatter()has the following properties: cusparsescatter()supports the followingoptimizations: seecusparsestatus_tfor the description of the return status. please visitcusparse library samples - cusparsescatterfor a code example.",1
6.6.4.cusparserot() [deprecated],#cusparserot-deprecated,"6.6.4.cusparserot() [deprecated] the function computes the givens rotation matrix to a sparsevecxand a dense vectorvecy in other words, cusparserotsupports the following index type for representing the sparse vectorvecx: cusparserotsupports the following data types: uniform-precision computation: mixed-precision computation: cusparserot()has the following constraints: cusparserot()has the following properties: cusparserot()supports the followingoptimizations: seecusparsestatus_tfor the description of the return status. please visitcusparse library samples - cusparserotfor a code example.",1
6.6.5.cusparsespvv(),#cusparsespvv,"6.6.5.cusparsespvv() the function computes the inner dot product of a sparse vectorvecxand a dense vectorvecy in other words, \(\text{op}(x) = \begin{cases}
x & \text{if op(x) == cusparse_operation_non_transpose} \\
\overline{x} & \text{if op(x) == cusparse_operation_conjugate_transpose} \\
\end{cases}\) the functioncusparsespvv_buffersize()returns the size of the workspace needed bycusparsespvv() cusparsespvvsupports the following index type for representing the sparse vectorvecx: the data types combinations currently supported forcusparsespvvare listed below: uniform-precision computation: mixed-precision computation: cusparsespvv()has the following constraints: cusparsespvv()has the following properties: cusparsespvv()supports the followingoptimizations: seecusparsestatus_tfor the description of the return status. please visitcusparse library samples - cusparsespvvfor a code example.",1
6.6.6.cusparsespmv(),#cusparsespmv,"6.6.6.cusparsespmv() this function performs the multiplication of a sparse matrixmataand a dense vectorvecx where also, for matrixa \(\text{op}(a) == \begin{cases}
a & \text{if op(a) == cusparse_operation_non_transpose} \\
a^{t} & \text{if op(a) == cusparse_operation_transpose} \\
a^{h} & \text{if op(a) ==cusparse_operation_conjugate_transpose} \\
\end{cases}\) the functioncusparsespmv_buffersize()returns the size of the workspace needed bycusparsespmv_preprocess()andcusparsespmv() the sparse matrix formats currently supported are listed below: cusparsespmvsupports the following index type for representing the sparse matrixmata: cusparsespmvsupports the following data types: uniform-precision computation: mixed-precision computation: mixed regular/complex computation: note:cuda_r_16f,cuda_r_16bf,cuda_c_16f, andcuda_c_16bfdata types always imply mixed-precision computation. cusparsespmv()supports the following algorithms: performance notes: cusparsespmv()has the following properties: cusparsespmv()supports the followingoptimizations: seecusparsestatus_tfor the description of the return status. please visitcusparse library samples - cusparsespmv csrandcusparsespmv coofor a code example.",1
6.6.7.cusparsespsv(),#cusparsespsv,"6.6.7.cusparsespsv() the function solves a system of linear equations whose coefficients are represented in a sparse triangular matrix: where also, for matrixa \(\text{op}(a) = \begin{cases}
a & \text{if op(a) == cusparse_operation_non_transpose} \\
a^{t} & \text{if op(a) == cusparse_operation_transpose} \\
a^{h} & \text{if op(a) == cusparse_operation_conjugate_transpose} \\
\end{cases}\) the functioncusparsespsv_buffersize()returns the size of the workspace needed bycusparsespsv_analysis()andcusparsespsv_solve().
the functioncusparsespsv_analysis()performs the analysis phase, whilecusparsespsv_solve()executes the solve phase for a sparse triangular linear system.
the opaque data structurespsvdescris used to share information among all functions.
the functioncusparsespsv_updatematrix()updatesspsvdescrwith new matrix values. the routine supports arbitrary sparsity for the input matrix, but only the upper or lower triangular part is taken into account in the computation. note:all parameters must be consistent acrosscusparsespsvapi calls and the matrix descriptions andexternalbuffermust not be modified betweencusparsespsv_analysis()andcusparsespsv_solve(). the functioncusparsespsv_updatematrix()can be used to update the values on the sparse matrix stored inside the opaque data structurespsvdescr the sparse matrix formats currently supported are listed below: thecusparsespsv()supports the following shapes and properties: the fill mode and diagonal type can be set bycusparsespmatsetattribute() cusparsespsv()supports the following index type for representing the sparse matrixmata: cusparsespsv()supports the following data types: uniform-precision computation: cusparsespsv()supports the following algorithms: cusparsespsv()has the following properties: cusparsespsv()supports the followingoptimizations: cusparsespsv_updatematrix()updates the sparse matrix after calling the analysis phase. this functions supports the following update strategies (updatepart): seecusparsestatus_tfor the description of the return status. please visitcusparse library samples - cusparsespsv csrandcusparse library samples - cusparsespsv coofor code examples.",1
6.6.8.cusparsespmm(),#cusparsespmm,"6.6.8.cusparsespmm() the function performs the multiplication of a sparse matrixmataand a dense matrixmatb. where the routine can be also used to perform the multiplication of a dense matrix and a sparse matrix by switching the dense matrices layout: where\(\mathbf{b}_{c}\),\(\mathbf{c}_{c}\)indicate column-major layout, while\(\mathbf{b}_{r}\),\(\mathbf{c}_{r}\)refer to row-major layout also, for matrixaandb \(\text{op}(a) = \begin{cases}
a & \text{if op(a) == cusparse_operation_non_transpose} \\
a^{t} & \text{if op(a) == cusparse_operation_transpose} \\
a^{h} & \text{if op(a) == cusparse_operation_conjugate_transpose} \\
\end{cases}\) \(\text{op}(b) = \begin{cases}
b & \text{if op(b) == cusparse_operation_non_transpose} \\
b^{t} & \text{if op(b) == cusparse_operation_transpose} \\
b^{h} & \text{if op(b) == cusparse_operation_conjugate_transpose} \\
\end{cases}\) when using the (conjugate) transpose of the sparse matrixa, this routine may produce slightly different results during different runs with the same input parameters. the functioncusparsespmm_buffersize()returns the size of the workspace needed bycusparsespmm() the functioncusparsespmm_preprocess()can be called beforecusparsespmmto speedup the actual computation. it is useful whencusparsespmmis called multiple times with the same sparsity pattern (mata). the values of the matrices (mata,matb,matc) can change arbitrarily. it provides performance advantages is used withcusparse_spmm_csr_alg1orcusparse_spmm_csr_alg3. for all other formats and algorithms have no effect. cusparsespmmsupports the following sparse matrix formats: cusparsespmmsupports the following index type for representing the sparse matrixmata: cusparsespmmsupports the following data types: uniform-precision computation: mixed-precision computation: note:cuda_r_16f,cuda_r_16bf,cuda_c_16f, andcuda_c_16bfdata types always imply mixed-precision computation. cusparsespmmsupports the following algorithms: performance notes: cusparsespmm()with all algorithms support the following batch modes except forcusparse_spmm_csr_alg3: the number of batches and their strides can be set by usingcusparsecoosetstridedbatch,cusparsecsrsetstridedbatch, andcusparsednmatsetstridedbatch. the maximum number of batches forcusparsespmm()is 65,535. cusparsespmm()has the following properties: cusparsespmm()supports the followingoptimizations: please visitcusparse library samples - cusparsespmm csrandcusparsespmm coofor a code example. for batched computation please visitcusparsespmm csr batchedandcusparsespmm coo batched. cusparsespmmsupports the following data types forcusparse_format_blocked_ellformat and the following gpu architectures for exploiting nvidia tensor cores: cusparsespmmsupports the following algorithms withcusparse_format_blocked_ellformat: performance notes: the function has the following limitations: please visitcusparse library samples - cusparsespmm blocked-ellfor a code example. seecusparsestatus_tfor the description of the return status.",1
6.6.9.cusparsespmmop(),#cusparsespmmop,"6.6.9.cusparsespmmop() note 1:nvrtc and nvjitlink are not currently available on arm64 android platforms. note 2:the routine does not support android and tegra platforms except judy (sm87). experimental: the function performs the multiplication of a sparse matrixmataand a dense matrixmatbwith custom operators. where also, for matrixaandb \(\text{op}(a) = \begin{cases}
a & \text{if op(a) == cusparse_operation_non_transpose} \\
a^{t} & \text{if op(a) == cusparse_operation_transpose} \\
\end{cases}\) \(\text{op}(b) = \begin{cases}
b & {\text{if op(}b\text{) == cusparse_operation_non_transpose}} \\
b^{t} & {\text{if op(}b\text{) == cusparse_operation_transpose}} \\
\end{cases}\) onlyopa==cusparse_operation_non_transposeis currently supported the functioncusparsespmmop_createplan()returns the size of the workspace and the compiled kernel needed bycusparsespmmop() the operators must have the following signature and return type <computetype>is one offloat,double,cucomplex,cudoublecomplex, orint, cusparsespmmopsupports the following sparse matrix formats: cusparsespmmopsupports the following index type for representing the sparse matrixmata: cusparsespmmopsupports the following data types: uniform-precision computation: mixed-precision computation: cusparsespmmopsupports the following algorithms: performance notes: cusparsespmmop()has the following properties: cusparsespmmop()supports the followingoptimizations: please visitcusparse library samples - cusparsespmmop seecusparsestatus_tfor the description of the return status.",1
6.6.10.cusparsespsm(),#cusparsespsm,"6.6.10.cusparsespsm() the function solves a system of linear equations whose coefficients are represented in a sparse triangular matrix: where also, for matrixa \(\text{op}(a) = \begin{cases}
a & \text{if op(a) == cusparse_operation_non_transpose} \\
a^{t} & \text{if op(a) == cusparse_operation_transpose} \\
a^{h} & \text{if op(a) == cusparse_operation_conjugate_transpose} \\
\end{cases}\) \(\text{op}(b) = \begin{cases}
b & \text{if op(b) == cusparse_operation_non_transpose} \\
b^{t} & \text{if op(b) == cusparse_operation_transpose} \\
& \text{ } \\
\end{cases}\) the functioncusparsespsm_buffersize()returns the size of the workspace needed bycusparsespsm_analysis()andcusparsespsm_solve().
the functioncusparsespsm_analysis()performs the analysis phase, whilecusparsespsm_solve()executes the solve phase for a sparse triangular linear system.
the opaque data structurespsmdescris used to share information among all functions.
the functioncusparsespsm_updatematrix()updatesspsmdescrwith new matrix values. the routine supports arbitrary sparsity for the input matrix, but only the upper or lower triangular part is taken into account in the computation. cusparsespsm_buffersize()requires a buffer size for the analysis phase which is proportional to number of non-zero entries of the sparse matrix theexternalbufferis stored intospsmdescrand used bycusparsespsm_solve(). for this reason, the device memory buffer must be deallocated only aftercusparsespsm_solve() note:all parameters must be consistent acrosscusparsespsmapi calls and the matrix descriptions andexternalbuffermust not be modified betweencusparsespsm_analysis()andcusparsespsm_solve() the sparse matrix formats currently supported are listed below: thecusparsespsm()supports the following shapes and properties: the fill mode and diagonal type can be set bycusparsespmatsetattribute() cusparsespsm()supports the following index type for representing the sparse matrixmata: cusparsespsm()supports the following data types: uniform-precision computation: cusparsespsm()supports the following algorithms: cusparsespsm()has the following properties: cusparsespsm()supports the followingoptimizations: cusparsespsm_updatematrix()updates the sparse matrix after calling the analysis phase. this functions supports the following update strategies (updatepart): seecusparsestatus_tfor the description of the return status. please visitcusparse library samples - cusparsespsm csrandcusparse library samples - cusparsespsm coofor code examples.",1
6.6.11.cusparsesddmm(),#cusparsesddmm,"6.6.11.cusparsesddmm() this function performs the multiplication ofmataandmatb, followed by an element-wise multiplication with the sparsity pattern ofmatc. formally, it performs the following operation: where also, for matrixaandb \(\text{op}(a) = \begin{cases}
a & \text{if op(a) == cusparse_operation_non_transpose} \\
a^{t} & \text{if op(a) == cusparse_operation_transpose} \\
\end{cases}\) \(\text{op}(b) = \begin{cases}
b & \text{if op(b) == cusparse_operation_non_transpose} \\
b^{t} & \text{if op(b) == cusparse_operation_transpose} \\
\end{cases}\) the functioncusparsesddmm_buffersize()returns the size of the workspace needed bycusparsesddmmorcusparsesddmm_preprocess. the functioncusparsesddmm_preprocess()can be called beforecusparsesddmmto speedup the actual computation. it is useful whencusparsesddmmis called multiple times with the same sparsity pattern (matc). the values of the dense matrices (mata,matb) can change arbitrarily. currently supported sparse matrix formats: cusparsesddmm()supports the following index type for representing the sparse matrixmata: the data types combinations currently supported forcusparsesddmmare listed below: uniform-precision computation: mixed-precision computation: cusparsesddmmforcusparse_format_bsralso supports the following mixed-precision computation: note:cuda_r_16f,cuda_r_16bfdata types always imply mixed-precision computation. cusparsesddmm()forcuspasre_format_bsrsupports block sizes of 2, 4, 8, 16, 32, 64 and 128. cusparsesddmm()supports the following algorithms: performance notes:cuspasesddmm()forcusparse_format_csrprovides the best performance whenmataandmatbsatisfy: cuspasesddmm()forcusparse_format_bsrprovides the best performance whenmataandmatbsatisfy: cusparsesddmm()supports the following batch modes: the number of batches and their strides can be set by usingcusparsecsrsetstridedbatchandcusparsednmatsetstridedbatch. the maximum number of batches forcusparsesddmm()is 65,535. cusparsesddmm()has the following properties: cusparsesddmm()supports the followingoptimizations: seecusparsestatus_tfor the description of the return status. please visitcusparse library samples - cusparsesddmmfor a code example. for batched computation please visitcusparsesddmm csr batched.",1
6.6.12.cusparsespgemm(),#cusparsespgemm,"6.6.12.cusparsespgemm() this function performs the multiplication of two sparse matricesmataandmatb. where\(\alpha,\)\(\beta\)are scalars, and\(\mathbf{c},\)\(\mathbf{c^{\prime}}\)have the same sparsity pattern. the functionscusparsespgemm_workestimation(),cusparsespgemm_estimatememory(), andcusparsespgemm_compute()are used for both determining the buffer size and performing the actual computation. currently, the function has the following limitations: the data types combinations currently supported forcusparsespgemmare listed below : uniform-precision computation: cusparsespgemmroutine runs for the following algorithms: cusparsespgemm()has the following properties: cusparsespgemm()supports the followingoptimizations: seecusparsestatus_tfor the description of the return status. please visitcusparse library samples - cusparsespgemmfor a code example forcusparse_spgemm_defaultandcusparse_spgemm_alg1, andcusparse library samples - memory-optimzed cusparsespgemmfor a code example forcusparse_spgemm_alg2andcusparse_spgemm_alg3.",1
6.6.13.cusparsespgemmreuse(),#cusparsespgemmreuse,"6.6.13.cusparsespgemmreuse() this function performs the multiplication of two sparse matricesmataandmatbwhere the structure of the output matrixmatccan be reused for multiple computations with different values. where\(\alpha\)and\(\beta\)are scalars. the functionscusparsespgemmreuse_workestimation(),cusparsespgemmreuse_nnz(), andcusparsespgemmreuse_copy()are used for determining the buffer size and performing the actual computation. note:cusparsespgemmreuse()output csr matrix (matc) is sorted by column indices. memory requirement:cusparsespgemmreuserequires to keep in memory all intermediate products to reuse the structure of the output matrix. on the other hand, the number of intermediate products is orders of magnitude higher than the number of non-zero entries in general. in order to minimize the memory requirements, the routine uses multiple buffers that can be deallocated after they are no more needed. if the number of intermediate product exceeds2^31-1, the routine will returnscusparse_status_insufficient_resourcesstatus. currently, the function has the following limitations: the data types combinations currently supported forcusparsespgemmreuseare listed below. uniform-precision computation: mixed-precision computation: [deprecated] cusparsespgemmreuseroutine runs for the following algorithm: cusparsespgemmreuse()has the following properties: cusparsespgemmreuse()supports the followingoptimizations: refer tocusparsestatus_tfor the description of the return status. please visitcusparse library samples - cusparsespgemmreusefor a code example.",1
6.6.14.cusparsesparsetodense(),#cusparsesparsetodense,"6.6.14.cusparsesparsetodense() the function converts the sparse matrixmatain csr, csc, or coo format into its dense representationmatb. blocked-ell is not currently supported. the functioncusparsesparsetodense_buffersize()returns the size of the workspace needed bycusparsesparsetodense(). cusparsesparsetodense()supports the following index type for representing the sparse matrixmata: cusparsesparsetodense()supports the following data types: cusparsesparse2dense()supports the following algorithm: cusparsesparsetodense()has the following properties: cusparsesparsetodense()supports the followingoptimizations: seecusparsestatus_tfor the description of the return status. please visitcusparse library samples - cusparsesparsetodensefor a code example.",1
6.6.15.cusparsedensetosparse(),#cusparsedensetosparse,"6.6.15.cusparsedensetosparse() the function converts the dense matrixmatainto a sparse matrixmatbin csr, csc, coo, or blocked-ell format. the functioncusparsedensetosparse_buffersize()returns the size of the workspace needed bycusparsedensetosparse_analysis(). the functioncusparsedensetosparse_analysis()updates the number of non-zero elements in the sparse matrix descriptormatb. the user is responsible to allocate the memory required by the sparse matrix: finally, we callcusparsedensetosparse_convert()for filling the arrays allocated in the previous step. cusparsedensetosparse()supports the following index type for representing the sparse vectormatb: cusparsedensetosparse()supports the following data types: cusparsedense2sparse()supports the following algorithm: cusparsedensetosparse()has the following properties: cusparsedensetosparse()supports the followingoptimizations: seecusparsestatus_tfor the description of the return status. please visitcusparse library samples - cusparsedensetosparse (csr)andcusparse library samples - cusparsedensetosparse (blocked-ell)for code examples.",1
7.cusparse fortran bindings,#cusparse-fortran-bindings,"7.cusparse fortran bindings the cusparse library is implemented using the c-based cuda toolchain, and it thus provides a c-style api that makes interfacing to applications written in c or c++ trivial. there are also many applications implemented in fortran that would benefit from using cusparse, and therefore a cusparse fortran interface has been developed. unfortunately, fortran-to-c calling conventions are not standardized and differ by platform and toolchain. in particular, differences may exist in the following areas: symbol names (capitalization, name decoration) argument passing (by value or reference) passing of pointer arguments (size of the pointer) to provide maximum flexibility in addressing those differences, the cusparse fortran interface is provided in the form of wrapper functions, which are written in c and are located in the filecusparse_fortran.c. this file also contains a few additional wrapper functions (forcudamalloc(),cudamemset, and so on) that can be used to allocate memory on the gpu. the cusparse fortran wrapper code is provided as an example only and needs to be compiled into an application for it to call the cusparse api functions. providing this source code allows users to make any changes necessary for a particular platform and toolchain. the cusparse fortran wrapper code has been used to demonstrate interoperability with the compilers g95 0.91 (on 32-bit and 64-bit linux) and g95 0.92 (on 32-bit and 64-bit mac os x). in order to use other compilers, users have to make any changes to the wrapper code that may be required. the direct wrappers, intended for production code, substitute device pointers for vector and matrix arguments in all cusparse functions. to use these interfaces, existing applications need to be modified slightly to allocate and deallocate data structures in gpu memory space (usingcuda_malloc()andcuda_free()) and to copy data between gpu and cpu memory spaces (using thecuda_memcpy()routines). the sample wrappers provided incusparse_fortran.cmap device pointers to the os-dependent typesize_t, which is 32 bits wide on 32-bit platforms and 64 bits wide on a 64-bit platforms. one approach to dealing with index arithmetic on device pointers in fortran code is to use c-style macros and to use the c preprocessor to expand them. on linux and mac os x, preprocessing can be done by using the option'-cpp'with g95 or gfortran. the functionget_shifted_address(), provided with the cusparse fortran wrappers, can also be used, as shown in example b. example b shows the the c++ of example a implemented in fortran 77 on the host. this example should be compiled witharch_64defined as 1 on a 64-bit os system and as undefined on a 32-bit os system. for example, on g95 or gfortran, it can be done directly on the command line using the option-cpp-darch_64=1.",5
7.1.fortran application,#fortran-application,7.1.fortran application,9
8.acknowledgements,#acknowledgements,8.acknowledgements nvidia would like to thank the following individuals and institutions for their contributions:,4
9.bibliography,#bibliography,"9.bibliography [1] n. bell and m. garland,implementing sparse matrix-vector multiplication on throughput-oriented processors, supercomputing, 2009. [2] r. grimes, d. kincaid, and d. young, itpack 2.0 users guide, technical report cna-150, center for numerical analysis, university of texas, 1979. [3] m. naumov,incomplete-lu and cholesky preconditioned iterative methods using cusparse and cublas, technical report and white paper, 2011. [4] pedro valero-lara, ivan martnez-prez, ral sirvent, xavier martorell, and antonio j. pea. nvidia gpus scalability to solve multiple (batch) tridiagonal systems. implementation of cuthomasbatch. in parallel processing and applied mathematics - 12th international conference (ppam), 2017.",1
10.notices,#notices,10.notices,9
10.1.notice,#notice,"10.1.notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation (nvidia) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material (defined below), code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer (terms of sale). nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion and/or use of nvidia products in such equipment or applications and therefore such inclusion and/or use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions and/or requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the nvidia product in any manner that is contrary to this document or (ii) customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding third-party products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents (together and separately, materials) are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product.",4
10.2.opencl,#opencl,10.2.opencl opencl is a trademark of apple inc. used under license to the khronos group inc.,4
10.3.trademarks,#trademarks,10.3.trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.,4
1.introduction,#introduction,"1.introduction the cublas library is an implementation of blas (basic linear algebra subprograms) on top of the nvidiacuda runtime. it allows the user to access the computational resources of nvidia graphics processing unit (gpu). the cublas library exposes four sets of apis: to use the cublas api, the application must allocate the required matrices and vectors in the gpu memory space, fill them with data, call the sequence of desired cublas functions, and then upload the results from the gpu memory space back to the host. the cublas api also provides helper functions for writing and retrieving data from the gpu. to use the cublasxt api, the application may have the data on the host or any of the devices involved in the computation, and the library will take care of dispatching the operation to, and transferring the data to, one or multiple gpus present in the system, depending on the user request. the cublaslt is a lightweight library dedicated to general matrix-to-matrix multiply (gemm) operations with a new flexible api. this library adds flexibility in matrix data layouts, input types, compute types, and also in choosing the algorithmic implementations and heuristics through parameter programmability. after a set of options for the intended gemm operation are identified by the user, these options can be used repeatedly for different inputs. this is analogous to how cufft and fftw first create a plan and reuse for same size and type ffts with different input data.",3
1.1.data layout,#data-layout,"1.1.data layout for maximum compatibility with existing fortran environments, the cublas library uses column-major storage, and 1-based indexing. since c and c++ use row-major storage, applications written in these languages can not use the native array semantics for two-dimensional arrays. instead, macros or inline functions should be defined to implement matrices on top of one-dimensional arrays. for fortran code ported to c in mechanical fashion, one may chose to retain 1-based indexing to avoid the need to transform loops. in this case, the array index of a matrix element in row i and column j can be computed via the following macro here, ld refers to the leading dimension of the matrix, which in the case of column-major storage is the number of rows of the allocated matrix (even if only a submatrix of it is being used). for natively written c and c++ code, one would most likely choose 0-based indexing, in which case the array index of a matrix element in row i and column j can be computed via the following macro",3
1.2.new and legacy cublas api,#new-and-legacy-cublas-api,"1.2.new and legacy cublas api starting with version 4.0, the cublas library provides a new api, in addition to the existing legacy api. this section discusses why a new api is provided, the advantages of using it, and the differences with the existing legacy api. the new cublas library api can be used by including the header filecublas_v2.h. it has the following features that the legacy cublas api does not have: the legacy cublas api, explained in more detail inusing the cublas legacy api, can be used by including the header filecublas.h. since the legacy api is identical to the previously released cublas library api, existing applications will work out of the box and automatically use this legacy api without any source code changes. the current and the legacy cublas apis cannot be used simultaneously in a single translation unit: including bothcublas.handcublas_v2.hheader files will lead to compilation errors due to incompatible symbol redeclarations. in general, new applications should not use the legacy cublas api, and existing applications should convert to using the new api if it requires sophisticated and optimal stream parallelism, or if it calls cublas routines concurrently from multiple threads. for the rest of the document, the new cublas library api will simply be referred to as the cublas library api. as mentioned earlier the interfaces to the legacy and the cublas library apis are the header filecublas.handcublas_v2.h, respectively. in addition, applications using the cublas library need to link against:",3
1.3.example code,#example-code,1.3.example code for sample code references please see the two examples below. they show an application written in c using the cublas library api with two indexing styles (example 1. application using c and cublas: 1-based indexing and example 2. application using c and cublas: 0-based indexing).,3
2.using the cublas api,#using-the-cublas-api,2.using the cublas api,3
2.1.general description,#general-description,2.1.general description this section describes how to use the cublas library api.,3
2.1.1.error status,#error-status,2.1.1.error status all cublas library function calls return the error statuscublasstatus_t.,3
2.1.2.cublas context,#cublas-context,"2.1.2.cublas context the application must initialize a handle to the cublas library context by calling thecublascreate()function. then, the handle is explicitly passed to every subsequent library function call. once the application finishes using the library, it must call the functioncublasdestroy()to release the resources associated with the cublas library context. this approach allows the user to explicitly control the library setup when using multiple host threads and multiple gpus. for example, the application can usecudasetdevice()to associate different devices with different host threads and in each of those host threads it can initialize a unique handle to the cublas library context, which will use the particular device associated with that host thread. then, the cublas library function calls made with different handles will automatically dispatch the computation to different devices. the device associated with a particular cublas context is assumed to remain unchanged between the correspondingcublascreate()andcublasdestroy()calls. in order for the cublas library to use a different device in the same host thread, the application must set the new device to be used by callingcudasetdevice()and then create another cublas context, which will be associated with the new device, by callingcublascreate(). a cublas library context is tightly coupled with the cuda context that is current at the time of thecublascreate()call. an application that uses multiple cuda contexts is required to create a cublas context per cuda context and make sure the former never outlives the latter.",5
2.1.3.thread safety,#thread-safety,"2.1.3.thread safety the library is thread safe and its functions can be called from multiple host threads, even with the same handle. when multiple threads share the same handle, extreme care needs to be taken when the handle configuration is changed because that change will affect potentially subsequent cublas calls in all threads. it is even more true for the destruction of the handle. so it is not recommended that multiple thread share the same cublas handle.",5
2.1.4.results reproducibility,#results-reproducibility,"2.1.4.results reproducibility by design, all cublas api routines from a given toolkit version, generate the same bit-wise results at every run when executed on gpus with the same architecture and the same number of sms. however, bit-wise reproducibility is not guaranteed across toolkit versions because the implementation might differ due to some implementation changes. this guarantee holds when a single cuda stream is active only. if multiple concurrent streams are active, the library may optimize total performance by picking different internal implementations. any of those settings will allow for deterministic behavior even with multiple concurrent streams sharing a single cublas handle. this behavior is expected to change in a future release. for some routines such ascublas<t>symvandcublas<t>hemv, an alternate significantly faster routine can be chosen using the routinecublassetatomicsmode(). in that case, the results are not guaranteed to be bit-wise reproducible because atomics are used for the computation.",5
2.1.5.scalar parameters,#scalar-parameters,"2.1.5.scalar parameters there are two categories of the functions that use scalar parameters : for the functions of the first category, when the pointer mode is set tocublas_pointer_mode_host, the scalar parametersalphaand/orbetacan be on the stack or allocated on the heap, shouldnt be placed in managed memory. underneath, the cuda kernels related to those functions will be launched with the value ofalphaand/orbeta. therefore if they were allocated on the heap, they can be freed just after the return of the call even though the kernel launch is asynchronous. when the pointer mode is set tocublas_pointer_mode_device,alphaand/orbetamust be accessible on the device and their values should not be modified until the kernel is done. note that sincecudafree()does an implicitcudadevicesynchronize(),cudafree()can still be called onalphaand/orbetajust after the call but it would defeat the purpose of using this pointer mode in that case. for the functions of the second category, when the pointer mode is set tocublas_pointer_mode_host, these functions block the cpu, until the gpu has completed its computation and the results have been copied back to the host. when the pointer mode is set tocublas_pointer_mode_device, these functions return immediately. in this case, similar to matrix and vector results, the scalar result is ready only when execution of the routine on the gpu has completed. this requires proper synchronization in order to read the result from the host. in either case, the pointer modecublas_pointer_mode_deviceallows the library functions to execute completely asynchronously from the host even whenalphaand/orbetaare generated by a previous kernel. for example, this situation can arise when iterative methods for solution of linear systems and eigenvalue problems are implemented using the cublas library.",5
2.1.6.parallelism with streams,#parallelism-with-streams,"2.1.6.parallelism with streams if the application uses the results computed by multiple independent tasks, cuda streams can be used to overlap the computation performed in these tasks. the application can conceptually associate each stream with each task. in order to achieve the overlap of computation between the tasks, the user should create cuda streams using the functioncudastreamcreate()and set the stream to be used by each individual cublas library routine by callingcublassetstream()just before calling the actual cublas routine. note thatcublassetstream()resets the user-provided workspace to the default workspace pool; seecublassetworkspace(). then, the computation performed in separate streams would be overlapped automatically when possible on the gpu. this approach is especially useful when the computation performed by a single task is relatively small and is not enough to fill the gpu with work. we recommend using the new cublas api with scalar parameters and results passed by reference in the device memory to achieve maximum overlap of the computation when using streams. a particular application of streams, batching of multiple small kernels, is described in the following section.",5
2.1.7.batching kernels,#batching-kernels,"2.1.7.batching kernels in this section, we explain how to use streams to batch the execution of small kernels. for instance, suppose that we have an application where we need to make many small independent matrix-matrix multiplications with dense matrices. it is clear that even with millions of small independent matrices we will not be able to achieve the samegflopsrate as with a one large matrix. for example, a single\(n \times n\)large matrix-matrix multiplication performs\(n^{3}\)operations for\(n^{2}\)input size, while 1024\(\frac{n}{32} \times \frac{n}{32}\)small matrix-matrix multiplications perform\(1024\left( \frac{n}{32} \right)^{3} = \frac{n^{3}}{32}\)operations for the same input size. however, it is also clear that we can achieve a significantly better performance with many small independent matrices compared with a single small matrix. the architecture family of gpus allows us to execute multiple kernels simultaneously. hence, in order to batch the execution of independent kernels, we can run each of them in a separate stream. in particular, in the above example we could create 1024 cuda streams using the functioncudastreamcreate(), then preface each call tocublas<t>gemm()with a call tocublassetstream()with a different stream for each of the matrix-matrix multiplications (note thatcublassetstream()resets user-provided workspace to the default workspace pool, seecublassetworkspace()). this will ensure that when possible the different computations will be executed concurrently. although the user can create many streams, in practice it is not possible to have more than 32 concurrent kernels executing at the same time.",5
7.43.cuda_resource_view_desc_v1 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__RESOURCE__VIEW__DESC__v1.html#structCUDA__RESOURCE__VIEW__DESC__v1,7.43.cuda_resource_view_desc_v1 struct reference,8
7.18.cuda_ext_sem_wait_node_params_v2 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__EXT__SEM__WAIT__NODE__PARAMS__v2.html#structCUDA__EXT__SEM__WAIT__NODE__PARAMS__v2,7.18.cuda_ext_sem_wait_node_params_v2 struct reference,8
2.1.8.cache configuration,#cache-configuration,"2.1.8.cache configuration on some devices, l1 cache and shared memory use the same hardware resources. the cache configuration can be set directly with the cuda runtime function cudadevicesetcacheconfig. the cache configuration can also be set specifically for some functions using the routine cudafuncsetcacheconfig. please refer to the cuda runtime api documentation for details about the cache configuration settings. because switching from one configuration to another can affect kernels concurrency, the cublas library does not set any cache configuration preference and relies on the current setting. however, some cublas routines, especially level-3 routines, rely heavily on shared memory. thus the cache preference setting might affect adversely their performance.",5
6.15.stream ordered memory allocator,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MALLOC__ASYNC.html#group__CUDA__MALLOC__ASYNC,6.15.stream ordered memory allocator,5
7.59.culaunchmemsyncdomainmap struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUlaunchMemSyncDomainMap.html#structCUlaunchMemSyncDomainMap,7.59.culaunchmemsyncdomainmap struct reference,8
2.1.9.static library support,#static-library-support,"2.1.9.static library support the cublas library is also delivered in a static form aslibcublas_static.aon linux. the static cublas library and all other static math libraries depend on a common thread abstraction layer library calledlibculibos.a. for example, on linux, to compile a small application using cublas, against the dynamic library, the following command can be used: whereas to compile against the static cublas library, the following command must be used: it is also possible to use the native host c++ compiler. depending on the host operating system, some additional libraries likepthreadordlmight be needed on the linking line. the following command on linux is suggested : note that in the latter case, the librarycudais not needed. the cuda runtime will try to open explicitly thecudalibrary if needed. in the case of a system which does not have the cuda driver installed, this allows the application to gracefully manage this issue and potentially run if a cpu-only path is available. starting with release 11.2, using the typed functions instead of the extension functions (cublas**ex()) helps in reducing the binary size when linking to static cublas library.",0
6.34.coredump attributes control api,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__COREDUMP.html#group__CUDA__COREDUMP,6.34.coredump attributes control api,6
6.39.direct3d 9 interoperability,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__D3D9.html#group__CUDA__D3D9,6.39.direct3d 9 interoperability,9
7.44.cuda_texture_desc_v1 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__TEXTURE__DESC__v1.html#structCUDA__TEXTURE__DESC__v1,7.44.cuda_texture_desc_v1 struct reference,8
modules,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__D3D9.html#group__CUDA__D3D9,modules,9
6.21.stream memory operations,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEMOP.html#group__CUDA__MEMOP,6.21.stream memory operations,5
2.1.10.gemm algorithms numerical behavior,#gemm-algorithms-numerical-behavior,"2.1.10.gemm algorithms numerical behavior some gemm algorithms split the computation along the dimension k to increase the gpu occupancy, especially when the dimension k is large compared to dimensions m and n. when this type of algorithm is chosen by the cublas heuristics or explicitly by the user, the results of each split is summed deterministically into the resulting matrix to get the final result. for the routinescublas<t>gemmexandcublasgemmex(), when the compute type is greater than the output type, the sum of the split chunks can potentially lead to some intermediate overflows thus producing a final resulting matrix with some overflows. those overflows might not have occurred if all the dot products had been accumulated in the compute type before being converted at the end in the output type. this computation side-effect can be easily exposed when the computetype is cuda_r_32f and atype, btype and ctype are in cuda_r_16f. this behavior can be controlled using the compute precision modecublas_math_disallow_reduced_precision_reductionwithcublassetmathmode()",3
5.data fields,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,5.data fields,9
3.modules,https://docs.nvidia.com/cuda/debugger-api/modules.html#modules,3.modules,9
2.1.11.tensor core usage,#tensor-core-usage,"2.1.11.tensor core usage tensor cores were first introduced with volta gpus (compute capability 7.0 and above) and significantly accelerate matrix multiplications. starting with cublas version 11.0.0, the library may automatically make use of tensor core capabilities wherever possible, unless they are explicitly disabled by selecting pedantic compute modes in cublas (seecublassetmathmode(),cublasmath_t). it should be noted that the library will pick a tensor core enabled implementation wherever it determines that it would provide the best performance. the best performance when using tensor cores can be achieved when the matrix dimensions and pointers meet certain memory alignment requirements. specifically, all of the following conditions must be satisfied to get the most performance out of tensor cores: to conduct matrix multiplication with fp8 types (see8-bit floating point data types (fp8) usage), you must ensure that your matrix dimensions and pointers meet the optimal requirements listed above.  aside from fp8, there are no longer any restrictions on matrix dimensions and memory alignments to use tensor cores (starting with cublas version 11.0.0).",3
2.1.12.cuda graphs support,#cuda-graphs-support,"2.1.12.cuda graphs support cublas routines can be captured in cuda graph stream capture without restrictions in most situations. the exception are routines that output results into host buffers (e.g.cublas<t>dotwhile pointer modecublas_pointer_mode_hostis configured), as it enforces synchronization. for input coefficients (such asalpha,beta) behavior depends on the pointer mode setting:",3
2.1.13.64-bit integer interface,#bit-integer-interface,"2.1.13.64-bit integer interface cublas version 12 introduced 64-bit integer capable functions. each 64-bit integer function is equivalent to a 32-bit integer function with the following changes: for example, consider the following 32-bit integer functions: the equivalent 64-bit integer functions are: not every function has a 64-bit integer equivalent. for instance,cublassetmathmode()doesnt have any arguments that could meaningfully beint64_t. for documentation brevity, the 64-bit integer apis are not explicitly listed, but only mentioned that they exist for the relevant functions.",3
2.2.cublas datatypes reference,#cublas-datatypes-reference,2.2.cublas datatypes reference,3
2.2.1.cublashandle_t,#cublashandle-t,2.2.1.cublashandle_t thecublashandle_ttype is a pointer type to an opaque structure holding the cublas library context. the cublas library context must be initialized usingcublascreate()and the returned handle must be passed to all subsequent library function calls. the context should be destroyed at the end usingcublasdestroy().,3
2.2.2.cublasstatus_t,#cublasstatus-t,"2.2.2.cublasstatus_t the type is used for function status returns. all cublas library functions return their status, which can have the following values.",3
a,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,a,9
b,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,b,9
3.3.device execution control,https://docs.nvidia.com/cuda/debugger-api/group__EXEC.html,3.3.device execution control,5
2.2.3.cublasoperation_t,#cublasoperation-t,"2.2.3.cublasoperation_t thecublasoperation_ttype indicates which operation needs to be performed with the dense matrix. its values correspond to fortran charactersnorn(non-transpose),tort(transpose) andcorc(conjugate transpose) that are often used as parameters to legacy blas implementations.",3
c,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,c,9
6.43.egl interoperability,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EGL.html,6.43.egl interoperability,9
4.2.cudbgevent struct reference,https://docs.nvidia.com/cuda/debugger-api/structCUDBGEvent.html#structCUDBGEvent,4.2.cudbgevent struct reference,8
2.2.4.cublasfillmode_t,#cublasfillmode-t,2.2.4.cublasfillmode_t the type indicates which part (lower or upper) of the dense matrix was filled and consequently should be used by the function. its values correspond to fortran characterslorl(lower) anduoru(upper) that are often used as parameters to legacy blas implementations.,3
1.1.system requirements,#system-requirements,"1.1.system requirements to use nvidia cuda on your system, you will need the following installed: the cuda development environment relies on tight integration with the host development environment, including the host compiler and c runtime libraries, and is therefore only supported on distribution versions that have been qualified for this cuda toolkit release. the following table lists the supported linux distributions. please review the footnotes associated with the table.",0
3.2.initialization,https://docs.nvidia.com/cuda/debugger-api/group__INIT.html#group__INIT,3.2.initialization,9
d,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,d,9
4.8.cudbgevent::cases_st::elfimageloaded_st struct reference,https://docs.nvidia.com/cuda/debugger-api/structCUDBGEvent_1_1cases__st_1_1elfImageLoaded__st.html#structCUDBGEvent_1_1cases__st_1_1elfImageLoaded__st,4.8.cudbgevent::cases_st::elfimageloaded_st struct reference,8
direct3d 9 interoperability [deprecated],https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__D3D9.html#group__CUDA__D3D9,direct3d 9 interoperability [deprecated],9
2.2.5.cublasdiagtype_t,#cublasdiagtype-t,2.2.5.cublasdiagtype_t the type indicates whether the main diagonal of the dense matrix is unity and consequently should not be touched or modified by the function. its values correspond to fortran charactersnorn(non-unit) anduoru(unit) that are often used as parameters to legacy blas implementations.,3
2.2.6.cublassidemode_t,#cublassidemode-t,2.2.6.cublassidemode_t the type indicates whether the dense matrix is on the left or right side in the matrix equation solved by a particular function. its values correspond to fortran characterslorl(left) androrr(right) that are often used as parameters to legacy blas implementations.,3
2.2.7.cublaspointermode_t,#cublaspointermode-t,"2.2.7.cublaspointermode_t thecublaspointermode_ttype indicates whether the scalar values are passed by reference on the host or device. it is important to point out that if several scalar values are present in the function call, all of them must conform to the same single pointer mode. the pointer mode can be set and retrieved usingcublassetpointermode()andcublasgetpointermode()routines, respectively.",3
2.2.8.cublasatomicsmode_t,#cublasatomicsmode-t,"2.2.8.cublasatomicsmode_t the type indicates whether cublas routines which has an alternate implementation using atomics can be used. the atomics mode can be set and queried usingcublassetatomicsmode()andcublasgetatomicsmode()and routines, respectively.",3
2.2.9.cublasgemmalgo_t,#cublasgemmalgo-t,"2.2.9.cublasgemmalgo_t cublasgemmalgo_t type is an enumerant to specify the algorithm for matrix-matrix multiplication on gpu architectures up tosm_75. onsm_80and newer gpu architectures, this enumarant has no effect. cublas has the following algorithm options:",3
2.2.10.cublasmath_t,#cublasmath-t,"2.2.10.cublasmath_t cublasmath_tenumerate type is used incublassetmathmode()to choose compute precision modes as defined in the following table. since this setting does not directly control the use of tensor cores, the modecublas_tensor_op_mathis being deprecated, and will be removed in a future release.",3
2.2.11.cublascomputetype_t,#cublascomputetype-t,2.2.11.cublascomputetype_t cublascomputetype_tenumerate type is used incublasgemmex()andcublasltmatmul()(including all batched and strided batched variants) to choose compute precision modes as defined below.,3
2.3.cuda datatypes reference,#cuda-datatypes-reference,2.3.cuda datatypes reference the chapter describes types shared by multiple cuda libraries and defined in the header filelibrary_types.h.,0
e,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,e,9
f,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,f,9
7.12.cuda_conditional_node_params struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__CONDITIONAL__NODE__PARAMS.html#structCUDA__CONDITIONAL__NODE__PARAMS,7.12.cuda_conditional_node_params struct reference,8
2.3.1.cudadatatype_t,#cudadatatype-t,"2.3.1.cudadatatype_t thecudadatatype_ttype is an enumerant to specify the data precision. it is used when the data reference does not carry the type itself (e.g void *) for example, it is used in the routinecublassgemmex().",6
1.2.os support policy,#os-support-policy,1.2.os support policy refer to the support lifecycle for these supported oses to know their support timelines and plan to move to newer releases accordingly.,9
g,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,g,9
2.3.2.librarypropertytype_t,#librarypropertytype-t,2.3.2.librarypropertytype_t thelibrarypropertytype_tis used as a parameter to specify which property is requested when using the routinecublasgetproperty(),6
1.3.host compiler support policy,#host-compiler-support-policy,"1.3.host compiler support policy in order to compile the cpu host code in the cuda source, the cuda compiler nvcc requires a compatible host compiler to be installed on the system. the version of the host compiler supported on linux platforms is tabulated as below. nvcc performs a version check on the host compilers major version and so newer minor versions of the compilers listed below will be supported, but major versions falling outside the range will not be supported. for gcc and clang, the preceding table indicates the minimum version and the latest version supported. if you are on a linux distribution that may use an older version of gcc toolchain as default than what is listed above, it is recommended to upgrade to a newer toolchain cuda 11.0 or later toolkit. newer gcc toolchains are available with the red hat developer toolset for example. for platforms that ship a compiler version older than gcc 6 by default, linking to static or dynamic libraries that are shipped with the cuda toolkit is not supported. we only support libstdc++ (gccs implementation) for all the supported host compilers for the platforms listed above.",0
2.4.cublas helper function reference,#cublas-helper-function-reference,2.4.cublas helper function reference,3
2.4.1.cublascreate(),#cublascreate,"2.4.1.cublascreate() this function initializes the cublas library and creates a handle to an opaque structure holding the cublas library context. it allocates hardware resources on the host and device and must be called prior to making any other cublas library calls. the cublas library context is tied to the current cuda device. to use the library on multiple devices, one cublas handle needs to be created for each device. furthermore, for a given device, multiple cublas handles with different configurations can be created. becausecublascreate()allocates some internal resources and the release of those resources by callingcublasdestroy()will implicitly callcudadevicesynchronize(), it is recommended to minimize the number of times these functions are called. for multi-threaded applications that use the same device from different threads, the recommended programming model is to create one cublas handle per thread and use that cublas handle for the entire life of the thread.",5
2.4.2.cublasdestroy(),#cublasdestroy,"2.4.2.cublasdestroy() this function releases hardware resources used by the cublas library. this function is usually the last call with a particular handle to the cublas library. becausecublascreate()allocates some internal resources and the release of those resources by callingcublasdestroy()will implicitly callcudadevicesynchronize(), it is recommended to minimize the number of times these functions are called.",3
2.4.3.cublasgetversion(),#cublasgetversion,2.4.3.cublasgetversion() this function returns the version number of the cublas library.,3
2.4.4.cublasgetproperty(),#cublasgetproperty,2.4.4.cublasgetproperty() this function returns the value of the requested property in memory pointed to by value. refer tolibrarypropertytypefor supported types.,3
2.4.5.cublasgetstatusname(),#cublasgetstatusname,2.4.5.cublasgetstatusname() this function returns the string representation of a given status.,3
2.4.6.cublasgetstatusstring(),#cublasgetstatusstring,2.4.6.cublasgetstatusstring() this function returns the description string for a given status.,3
h,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,h,9
1.3.1.supported c++ dialects,#supported-c-dialects,"1.3.1.supported c++ dialects nvcc and nvrtc (cuda runtime compiler) support the following c++ dialect: c++11, c++14, c++17, c++20 on supported host compilers. the default c++ dialect of nvcc  is determined by the default dialect of the host compiler used for compilation. refer to host compiler documentation and thecuda programming guidefor more details on language support. c++20 is supported with the following flavors of host compiler in both host and device code.",0
1.1.the benefits of using gpus,#the-benefits-of-using-gpus,"1.1.the benefits of using gpus the graphics processing unit (gpu)1provides much higher instruction throughput and memory bandwidth than the cpu within a similar price and power envelope. many applications leverage these higher capabilities to run faster on the gpu than on the cpu (seegpu applications). other computing devices, like fpgas, are also very energy efficient, but offer much less programming flexibility than gpus. this difference in capabilities between the gpu and the cpu exists because they are designed with different goals in mind. while the cpu is designed to excel at executing a sequence of operations, called athread, as fast as possible and can execute a few tens of these threads in parallel, the gpu is designed to excel at executing thousands of them in parallel (amortizing the slower single-thread performance to achieve greater throughput). the gpu is specialized for highly parallel computations and therefore designed such that more transistors are devoted to data processing rather than data caching and flow control. the schematicfigure 1shows an example distribution of chip resources for a cpu versus a gpu. devoting more transistors to data processing, for example, floating-point computations, is beneficial for highly parallel computations; the gpu can hide memory access latencies with computation, instead of relying on large data caches and complex flow control to avoid long memory access latencies, both of which are expensive in terms of transistors. in general, an application has a mix of parallel parts and sequential parts, so systems are designed with a mix of gpus and cpus in order to maximize overall performance. applications with a high degree of parallelism can exploit this massively parallel nature of the gpu to achieve higher performance than on the cpu.",5
1.2.cuda: a general-purpose parallel computing platform and programming model,#cuda-a-general-purpose-parallel-computing-platform-and-programming-model,"1.2.cuda: a general-purpose parallel computing platform and programming model in november 2006, nvidiaintroduced cuda, a general purpose parallel computing platform and programming model that leverages the parallel compute engine in nvidia gpus to solve many complex computational problems in a more efficient way than on a cpu. cuda comes with a software environment that allows developers to use c++ as a high-level programming language. as illustrated byfigure 2, other languages, application programming interfaces, or directives-based approaches are supported, such as fortran, directcompute, openacc.",0
i,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,i,9
1.4.about this document,#about-this-document,1.4.about this document this document is intended for readers familiar with the linux environment and the compilation of c programs from the command line. you do not need previous experience with cuda or experience with parallel computation. note: this guide covers installation only on systems with x windows installed.,0
2.pre-installation actions,#pre-installation-actions,2.pre-installation actions some actions must be taken before the cuda toolkit and driver can be installed on linux:,0
2.1.verify you have a cuda-capable gpu,#verify-you-have-a-cuda-capable-gpu,"2.1.verify you have a cuda-capable gpu to verify that your gpu is cuda-capable, go to your distributions equivalent of system properties, or, from the command line, enter: if you do not see any settings, update the pci hardware database that linux maintains by enteringupdate-pciids(generally found in/sbin) at the command line and rerun the previouslspcicommand. if your graphics card is from nvidia and it is listed inhttps://developer.nvidia.com/cuda-gpus, your gpu is cuda-capable. the release notes for the cuda toolkit also contain a list of supported products.",0
2.2.verify you have a supported version of linux,#verify-you-have-a-supported-version-of-linux,"2.2.verify you have a supported version of linux the cuda development tools are only supported on some specific distributions of linux. these are listed in the cuda toolkit release notes. to determine which distribution and release number youre running, type the following at the command line: you should see output similar to the following, modified for your particular system: thex86_64line indicates you are running on a 64-bit system. the remainder gives information about your distribution.",0
2.3.verify the system has gcc installed,#verify-the-system-has-gcc-installed,"2.3.verify the system has gcc installed thegcccompiler is required for development using the cuda toolkit. it is not required for running cuda applications. it is generally installed as part of the linux installation, and in most cases the version of gcc installed with a supported version of linux will work correctly. to verify the version of gcc installed on your system, type the following on the command line: if an error message displays, you need to install the development tools from your linux distribution or obtain a version ofgccand its accompanying toolchain from the web.",0
2.4.7.cublassetstream(),#cublassetstream,"2.4.7.cublassetstream() this function sets the cublas library stream, which will be used to execute all subsequent calls to the cublas library functions. if the cublas library stream is not set, all kernels use thedefaultnullstream. in particular, this routine can be used to change the stream between kernel launches and then to reset the cublas library stream back tonull. additionally this function unconditionally resets the cublas library workspace back to the default workspace pool (seecublassetworkspace()).",3
1.3.a scalable programming model,#a-scalable-programming-model,"1.3.a scalable programming model the advent of multicore cpus and manycore gpus means that mainstream processor chips are now parallel systems. the challenge is to develop application software that transparently scales its parallelism to leverage the increasing number of processor cores, much as 3d graphics applications transparently scale their parallelism to manycore gpus with widely varying numbers of cores. the cuda parallel programming model is designed to overcome this challenge while maintaining a low learning curve for programmers familiar with standard programming languages such as c. at its core are three key abstractions  a hierarchy of thread groups, shared memories, and barrier synchronization  that are simply exposed to the programmer as a minimal set of language extensions. these abstractions provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism. they guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block. this decomposition preserves language expressivity by allowing threads to cooperate when solving each sub-problem, and at the same time enables automatic scalability. indeed, each block of threads can be scheduled on any of the available multiprocessors within a gpu, in any order, concurrently or sequentially, so that a compiled cuda program can execute on any number of multiprocessors as illustrated byfigure 3, and only the runtime system needs to know the physical multiprocessor count. this scalable programming model allows the gpu architecture to span a wide market range by simply scaling the number of multiprocessors and memory partitions: from the high-performance enthusiast geforce gpus and professional quadro and tesla computing products to a variety of inexpensive, mainstream geforce gpus (seecuda-enabled gpusfor a list of all cuda-enabled gpus).",5
1.4.document structure,#document-structure,1.4.document structure this document is organized into the following sections:,9
2.programming model,#programming-model,2.programming model this chapter introduces the main concepts behind the cuda programming model by outlining how they are exposed in c++. an extensive description of cuda c++ is given inprogramming interface. full code for the vector addition example used in this chapter and the next can be found in thevectoradd cuda sample.,0
k,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,k,9
2.4.verify the system has the correct kernel headers and development packages installed,#verify-the-system-has-the-correct-kernel-headers-and-development-packages-installed,"2.4.verify the system has the correct kernel headers and development packages installed the cuda driver requires that the kernel headers and development packages for the running version of the kernel be installed at the time of the driver installation, as well whenever the driver is rebuilt. for example, if your system is running kernel version 3.17.4-301, the 3.17.4-301 kernel headers and development packages must also be installed. while the runfile installation performs no package validation, the rpm and deb installations of the driver will make an attempt to install the kernel header and development packages if no version of these packages is currently installed. however, it will install the latest version of these packages, which may or may not match the version of the kernel your system is using.therefore, it is best to manually ensure the correct version of the kernel headers and development packages are installed prior to installing the cuda drivers, as well as whenever you change the kernel version. the version of the kernel your system is running can be found by running the following command: this is the version of the kernel headers and development packages that must be installed prior to installing the cuda drivers. this command will be used multiple times below to specify the version of the packages to install. note that below are the common-case scenarios for kernel usage. more advanced cases, such as custom kernel branches, should ensure that their kernel headers and sources match the kernel build they are running.",0
2.5.install gpudirect storage,#install-gpudirect-storage,"2.5.install gpudirect storage if you intend to use gpudirectstorage (gds), you must install the cuda package and mlnx_ofed package. gds packages can be installed using the cuda packaging guide. follow the instructions inmlnx_ofed requirements and installation. gds is supported in two different modes: gds (default/full perf mode) and compatibility mode. installation instructions for them differ slightly. compatibility mode is the only mode that is supported on certain distributions due to software dependency limitations. full gds support is restricted to the following linux distros: starting with cuda toolkit 12.2.2, gds kernel driver package nvidia-gds version 12.2.2-1 (provided by nvidia-fs-dkms 2.17.5-1) and above is only supported with the nvidia open kernel driver. follow the instructions inremoving cuda toolkit and driverto remove existing nvidia driver packages and then follow instructions innvidia open gpu kernel modulesto install nvidia open kernel driver packages.",0
2.6.choose an installation method,#choose-an-installation-method,"2.6.choose an installation method the cuda toolkit can be installed using either of two different installation mechanisms: distribution-specific packages (rpm and deb packages), or a distribution-independent package (runfile packages). the distribution-independent package has the advantage of working across a wider set of linux distributions, but does not update the distributions native package management system. the distribution-specific packages interface with the distributions native package management system. it is recommended to use the distribution-specific packages, where possible.",0
2.7.download the nvidia cuda toolkit,#download-the-nvidia-cuda-toolkit,"2.7.download the nvidia cuda toolkit the nvidia cuda toolkit is available athttps://developer.nvidia.com/cuda-downloads. choose the platform you are using and download the nvidia cuda toolkit. the cuda toolkit contains the cuda driver and tools needed to create, build and run a cuda application as well as libraries, header files, and other resources. download verification the download can be verified by comparing the md5 checksum posted athttps://developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txtwith that of the downloaded file. if either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again. to calculate the md5 checksum of the downloaded file, run the following:",0
"2.8.address custom xorg.conf, if applicable",#address-custom-xorg-conf-if-applicable,"2.8.address custom xorg.conf, if applicable the driver relies on an automatically generatedxorg.conffile at/etc/x11/xorg.conf. if a custom-builtxorg.conffile is present, this functionality will be disabled and the driver may not work. you can try removing the existingxorg.conffile, or adding the contents of/etc/x11/xorg.conf.d/00-nvidia.confto thexorg.conffile. thexorg.conffile will most likely need manual tweaking for systems with a non-trivial gpu configuration.",0
2.9.handle conflicting installation methods,#handle-conflicting-installation-methods,"2.9.handle conflicting installation methods before installing cuda, any previous installations that could conflict should be uninstalled. this will not affect systems which have not had cuda installed previously, or systems where the installation method has been preserved (rpm/deb vs. runfile). see the following charts for specifics. use the following command to uninstall a toolkit runfile installation: use the following command to uninstall a driver runfile installation: use the following commands to uninstall an rpm/deb installation:",0
3.package manager installation,#package-manager-installation,3.package manager installation basic instructions can be found in thequick start guide. read on for more detailed instructions.,9
3.1.overview,#overview,"3.1.overview installation using rpm or debian packages interfaces with your systems package management system. when using rpm or debian local repo installers, the downloaded package contains a repository snapshot stored on the local filesystem in /var/. such a package only informs the package manager where to find the actual installation packages, but will not install them. if the online network repository is enabled, rpm or debian packages will be automatically downloaded at installation time using the package manager: apt-get, dnf, yum, or zypper. distribution-specific instructions detail how to install cuda: finally, some helpfulpackage manager capabilitiesare detailed. these instructions are for native development only. for cross-platform development, see thecuda cross-platform environmentsection.",0
3.2.rhel 8 / rocky 8,#rhel-8-rocky-8,3.2.rhel 8 / rocky 8,9
3.2.1.prepare rhel 8 / rocky 8,#prepare-rhel-8-rocky-8,3.2.1.prepare rhel 8 / rocky 8,9
3.2.2.local repo installation for rhel 8 / rocky 8,#local-repo-installation-for-rhel-8-rocky-8,3.2.2.local repo installation for rhel 8 / rocky 8,9
3.2.3.network repo installation for rhel 8 / rocky 8,#network-repo-installation-for-rhel-8-rocky-8,3.2.3.network repo installation for rhel 8 / rocky 8,9
3.2.4.common instructions for rhel 8 / rocky 8,#common-instructions-for-rhel-8-rocky-8,3.2.4.common instructions for rhel 8 / rocky 8 these instructions apply to both local and network installation.,9
3.3.rhel 9 / rocky 9,#rhel-9-rocky-9,3.3.rhel 9 / rocky 9,9
3.3.1.prepare rhel 9 / rocky 9,#prepare-rhel-9-rocky-9,3.3.1.prepare rhel 9 / rocky 9,9
2.4.8.cublassetworkspace(),#cublassetworkspace,"2.4.8.cublassetworkspace() this function sets the cublas library workspace to a user-owned device buffer, which will be used to execute all subsequent calls to the cublas library functions (on the currently set stream). if the cublas library workspace is not set, all kernels will use the default workspace pool allocated during the cublas context creation. in particular, this routine can be used to change the workspace between kernel launches. the workspace pointer has to be aligned to at least 256 bytes, otherwisecublas_status_invalid_valueerror is returned. thecublassetstream()function unconditionally resets the cublas library workspace back to the default workspace pool. calling this function, including withworkspacesizeinbytesequal to 0, will prevent the cublas library from utilizing the default workspace. too smallworkspacesizeinbytesmay cause some routines to fail withcublas_status_alloc_failederror returned or cause large regressions in performance. workspace size equal to or larger than 16kib is enough to preventcublas_status_alloc_failederror, while a larger workspace can provide performance benefits for some routines. the table below shows the recommended size of user-provided workspace.
this is based on the cublas default workspace pool size which is gpu architecture dependent. the possible error values returned by this function and their meanings are listed below.",3
2.4.9.cublasgetstream(),#cublasgetstream,"2.4.9.cublasgetstream() this function gets the cublas library stream, which is being used to execute all calls to the cublas library functions. if the cublas library stream is not set, all kernels use thedefaultnullstream.",3
2.4.10.cublasgetpointermode(),#cublasgetpointermode,2.4.10.cublasgetpointermode() this function obtains the pointer mode used by the cublas library. please see the section on thecublaspointermode_ttype for more details.,3
2.4.11.cublassetpointermode(),#cublassetpointermode,2.4.11.cublassetpointermode() this function sets the pointer mode used by the cublas library. thedefaultis for the values to be passed by reference on the host. please see the section on thecublaspointermode_ttype for more details.,3
2.4.12.cublassetvector(),#cublassetvector,"2.4.12.cublassetvector() this function supports the64-bit integer interface. this function copiesnelements from a vectorxin host memory space to a vectoryin gpu memory space. elements in both vectors are assumed to have a size ofelemsizebytes. the storage spacing between consecutive elements is given byincxfor the source vectorxand byincyfor the destination vectory. since column-major format for two-dimensional matrices is assumed, if a vector is part of a matrix, a vector increment equal to1accesses a (partial) column of that matrix. similarly, using an increment equal to the leading dimension of the matrix results in accesses to a (partial) row of that matrix.",3
2.4.13.cublasgetvector(),#cublasgetvector,"2.4.13.cublasgetvector() this function supports the64-bit integer interface. this function copiesnelements from a vectorxin gpu memory space to a vectoryin host memory space. elements in both vectors are assumed to have a size ofelemsizebytes. the storage spacing between consecutive elements is given byincxfor the source vector andincyfor the destination vectory. since column-major format for two-dimensional matrices is assumed, if a vector is part of a matrix, a vector increment equal to1accesses a (partial) column of that matrix. similarly, using an increment equal to the leading dimension of the matrix results in accesses to a (partial) row of that matrix.",3
2.4.14.cublassetmatrix(),#cublassetmatrix,"2.4.14.cublassetmatrix() this function supports the64-bit integer interface. this function copies a tile ofrowsxcolselements from a matrixain host memory space to a matrixbin gpu memory space. it is assumed that each element requires storage ofelemsizebytes and that both matrices are stored in column-major format, with the leading dimension of the source matrixaand destination matrixbgiven inldaandldb, respectively. the leading dimension indicates the number of rows of the allocated matrix, even if only a submatrix of it is being used.",3
2.4.15.cublasgetmatrix(),#cublasgetmatrix,"2.4.15.cublasgetmatrix() this function supports the64-bit integer interface. this function copies a tile ofrowsxcolselements from a matrixain gpu memory space to a matrixbin host memory space. it is assumed that each element requires storage ofelemsizebytes and that both matrices are stored in column-major format, with the leading dimension of the source matrixaand destination matrixbgiven inldaandldb, respectively. the leading dimension indicates the number of rows of the allocated matrix, even if only a submatrix of it is being used.",3
2.4.16.cublassetvectorasync(),#cublassetvectorasync,"2.4.16.cublassetvectorasync() this function supports the64-bit integer interface. this function has the same functionality ascublassetvector(), with the exception that the data transfer is done asynchronously (with respect to the host) using the given cuda stream parameter.",3
2.4.17.cublasgetvectorasync(),#cublasgetvectorasync,"2.4.17.cublasgetvectorasync() this function supports the64-bit integer interface. this function has the same functionality ascublasgetvector(), with the exception that the data transfer is done asynchronously (with respect to the host) using the given cuda stream parameter.",3
l,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,l,9
m,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,m,9
o,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,o,9
p,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,p,9
r,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,r,9
s,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,s,9
t,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,t,9
u,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,u,9
w,https://docs.nvidia.com/cuda/debugger-api/functions.html#functions,w,9
2.1.kernels,#kernels,"2.1.kernels cuda c++ extends c++ by allowing the programmer to define c++ functions, calledkernels, that, when called, are executed n times in parallel by n differentcuda threads, as opposed to only once like regular c++ functions. a kernel is defined using the__global__declaration specifier and the number of cuda threads that execute that kernel for a given kernel call is specified using a new<<<...>>>execution configurationsyntax (seec++ language extensions). each thread that executes the kernel is given a uniquethread idthat is accessible within the kernel through built-in variables. as an illustration, the following sample code, using the built-in variablethreadidx, adds two vectorsaandbof sizenand stores the result into vectorc: here, each of thenthreads that executevecadd()performs one pair-wise addition.",5
2.2.thread hierarchy,#thread-hierarchy,"2.2.thread hierarchy for convenience,threadidxis a 3-component vector, so that threads can be identified using a one-dimensional, two-dimensional, or three-dimensionalthread index, forming a one-dimensional, two-dimensional, or three-dimensional block of threads, called athread block. this provides a natural way to invoke computation across the elements in a domain such as a vector, matrix, or volume. the index of a thread and its thread id relate to each other in a straightforward way: for a one-dimensional block, they are the same; for a two-dimensional block of size(dx, dy), the thread id of a thread of index(x, y)is(x + y dx); for a three-dimensional block of size(dx, dy, dz), the thread id of a thread of index(x, y, z)is(x + y dx + z dx dy). as an example, the following code adds two matricesaandbof sizenxnand stores the result into matrixc: there is a limit to the number of threads per block, since all threads of a block are expected to reside on the same streaming multiprocessor core and must share the limited memory resources of that core. on current gpus, a thread block may contain up to 1024 threads. however, a kernel can be executed by multiple equally-shaped thread blocks, so that the total number of threads is equal to the number of threads per block times the number of blocks. blocks are organized into a one-dimensional, two-dimensional, or three-dimensionalgridof thread blocks as illustrated byfigure 4. the number of thread blocks in a grid is usually dictated by the size of the data being processed, which typically exceeds the number of processors in the system. the number of threads per block and the number of blocks per grid specified in the<<<...>>>syntax can be of typeintordim3. two-dimensional blocks or grids can be specified as in the example above. each block within the grid can be identified by a one-dimensional, two-dimensional, or three-dimensional unique index accessible within the kernel through the built-inblockidxvariable. the dimension of the thread block is accessible within the kernel through the built-inblockdimvariable. extending the previousmatadd()example to handle multiple blocks, the code becomes as follows. a thread block size of 16x16 (256 threads), although arbitrary in this case, is a common choice. the grid is created with enough blocks to have one thread per matrix element as before. for simplicity, this example assumes that the number of threads per grid in each dimension is evenly divisible by the number of threads per block in that dimension, although that need not be the case. thread blocks are required to execute independently: it must be possible to execute them in any order, in parallel or in series. this independence requirement allows thread blocks to be scheduled in any order across any number of cores as illustrated byfigure 3, enabling programmers to write code that scales with the number of cores. threads within a block can cooperate by sharing data through someshared memoryand by synchronizing their execution to coordinate memory accesses. more precisely, one can specify synchronization points in the kernel by calling the__syncthreads()intrinsic function;__syncthreads()acts as a barrier at which all threads in the block must wait before any is allowed to proceed.shared memorygives an example of using shared memory. in addition to__syncthreads(), thecooperative groups apiprovides a rich set of thread-synchronization primitives. for efficient cooperation, the shared memory is expected to be a low-latency memory near each processor core (much like an l1 cache) and__syncthreads()is expected to be lightweight.",5
2.4.18.cublassetmatrixasync(),#cublassetmatrixasync,"2.4.18.cublassetmatrixasync() this function supports the64-bit integer interface. this function has the same functionality ascublassetmatrix(), with the exception that the data transfer is done asynchronously (with respect to the host) using the given cuda stream parameter.",3
2.4.19.cublasgetmatrixasync(),#cublasgetmatrixasync,"2.4.19.cublasgetmatrixasync() this function supports the64-bit integer interface. this function has the same functionality ascublasgetmatrix(), with the exception that the data transfer is done asynchronously (with respect to the host) using the given cuda stream parameter.",3
2.4.20.cublassetatomicsmode(),#cublassetatomicsmode,"2.4.20.cublassetatomicsmode() some routines likecublas<t>symvandcublas<t>hemvhave an alternate implementation that use atomics to cumulate results. this implementation is generally significantly faster but can generate results that are not strictly identical from one run to the others. mathematically, those different results are not significant but when debugging those differences can be prejudicial. this function allows or disallows the usage of atomics in the cublas library for all routines which have an alternate implementation. when not explicitly specified in the documentation of any cublas routine, it means that this routine does not have an alternate implementation that use atomics. when atomics mode is disabled, each cublas routine should produce the same results from one run to the other when called with identical parameters on the same hardware. the default atomics mode of default initializedcublashandle_tobject iscublas_atomics_not_allowed. please see the section on the type for more details.",3
2.4.21.cublasgetatomicsmode(),#cublasgetatomicsmode,2.4.21.cublasgetatomicsmode() this function queries the atomic mode of a specific cublas context. the default atomics mode of default initializedcublashandle_tobject iscublas_atomics_not_allowed. please see the section on the type for more details.,3
2.4.22.cublassetmathmode(),#cublassetmathmode,"2.4.22.cublassetmathmode() thecublassetmathmode()function enables you to choose the compute precision modes as defined bycublasmath_t. users are allowed to set the compute precision mode as a logical combination of them (except the deprecatedcublas_tensor_op_math). for example,cublassetmathmode(handle,cublas_default_math|cublas_math_disallow_reduced_precision_reduction). please note that the default math mode iscublas_default_math. for matrix and compute precisions allowed forcublasgemmex()andcublasltmatmul()apis and their strided variants please refer to:cublasgemmex(),cublasgemmbatchedex(),cublasgemmstridedbatchedex(), andcublasltmatmul().",3
2.4.23.cublasgetmathmode(),#cublasgetmathmode,2.4.23.cublasgetmathmode() this function returns the math mode used by the library routines.,3
2.4.24.cublassetsmcounttarget(),#cublassetsmcounttarget,"2.4.24.cublassetsmcounttarget() thecublassetsmcounttarget()function allows overriding the number of multiprocessors available to the library during kernels execution. this option can be used to improve the library performance when cublas routines are known to run concurrently with other work on different cuda streams. e.g. a nvidia a100 gpu has 108 sm and there is a concurrent kenrel running with grid size of 8, one can usecublassetsmcounttarget()with value100to override the library heuristics to optimize for running on 100 multiprocessors. when set to0the library returns to its default behavior. the input value should not exceed the devices multiprocessor count, which can be obtained usingcudadevicegetattribute. negative values are not accepted. the user must ensure thread safety when modifying the library handle with this routine similar to when usingcublassetstream(), etc.",5
2.4.25.cublasgetsmcounttarget(),#cublasgetsmcounttarget,2.4.25.cublasgetsmcounttarget() this function obtains the value previously programmed to the library handle.,3
2.4.26.cublasloggerconfigure(),#cublasloggerconfigure,"2.4.26.cublasloggerconfigure() this function configures logging during runtime. besides this type of configuration, it is possible to configure logging with special environment variables which will be checked by libcublas: parameters returns",3
2.4.27.cublasgetloggercallback(),#cublasgetloggercallback,2.4.27.cublasgetloggercallback() this function retrieves function pointer to previously installed custom user defined callback function viacublassetloggercallback()or zero otherwise. parameters the possible error values returned by this function and their meanings are listed below.,3
2.4.28.cublassetloggercallback(),#cublassetloggercallback,2.4.28.cublassetloggercallback() this function installs a custom user defined callback function via cublas c public api. parameters returns,3
2.2.1.thread block clusters,#thread-block-clusters,"2.2.1.thread block clusters with the introduction of nvidiacompute capability 9.0, the cuda programming model introduces an optional level of hierarchy called thread block clusters that are made up of thread blocks. similar to how threads in a thread block are guaranteed to be co-scheduled on a streaming multiprocessor, thread blocks in a cluster are also guaranteed to be co-scheduled on a gpu processing cluster (gpc) in the gpu. similar to thread blocks, clusters are also organized into a one-dimension, two-dimension, or three-dimension as illustrated byfigure 5. the number of thread blocks in a cluster can be user-defined, and a maximum of 8 thread blocks in a cluster is supported as a portable cluster size in cuda.
note that on gpu hardware or mig configurations which are too small to support 8 multiprocessors the maximum cluster size will be reduced accordingly. identification of these smaller configurations, as well as of larger configurations supporting a thread block cluster size beyond 8, is architecture-specific and can be queried using thecudaoccupancymaxpotentialclustersizeapi. a thread block cluster can be enabled in a kernel either using a compiler time kernel attribute using__cluster_dims__(x,y,z)or using the cuda kernel launch apicudalaunchkernelex. the example below shows how to launch a cluster using compiler time kernel attribute. the cluster size using kernel attribute is fixed at compile time and then the kernel can be launched using the classical<<<,>>>. if a kernel uses compile-time cluster size, the cluster size cannot be modified when launching the kernel. a thread block cluster size can also be set at runtime and the kernel can be launched using the cuda kernel launch apicudalaunchkernelex. the code example below shows how to launch a cluster kernel using the extensible api. in gpus with compute capability 9.0, all the thread blocks in the cluster are guaranteed to be co-scheduled on a single gpu processing cluster (gpc) and allow thread blocks in the cluster to perform hardware-supported synchronization using thecluster groupapicluster.sync(). cluster group also provides member functions to query cluster group size in terms of number of threads or number of blocks usingnum_threads()andnum_blocks()api respectively. the rank of a thread or block in the cluster group can be queried usingdim_threads()anddim_blocks()api respectively. thread blocks that belong to a cluster have access to the distributed shared memory. thread blocks in a cluster have the ability to read, write, and perform atomics to any address in the distributed shared memory.distributed shared memorygives an example of performing histograms in distributed shared memory.",5
2.3.memory hierarchy,#memory-hierarchy,"2.3.memory hierarchy cuda threads may access data from multiple memory spaces during their execution as illustrated byfigure 6. each thread has private local memory. each thread block has shared memory visible to all threads of the block and with the same lifetime as the block. thread blocks in a thread block cluster can perform read, write, and atomics operations on each others shared memory. all threads have access to the same global memory. there are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces. the global, constant, and texture memory spaces are optimized for different memory usages (seedevice memory accesses). texture memory also offers different addressing modes, as well as data filtering, for some specific data formats (seetexture and surface memory). the global, constant, and texture memory spaces are persistent across kernel launches by the same application.",5
3.3.2.local repo installation for rhel 9 / rocky 9,#local-repo-installation-for-rhel-9-rocky-9,3.3.2.local repo installation for rhel 9 / rocky 9,9
3.3.3.network repo installation for rhel 9 / rocky 9,#network-repo-installation-for-rhel-9-rocky-9,3.3.3.network repo installation for rhel 9 / rocky 9,9
3.3.4.common instructions for rhel 9 / rocky 9,#common-instructions-for-rhel-9-rocky-9,3.3.4.common instructions for rhel 9 / rocky 9 these instructions apply to both local and network installation.,9
3.4.kylinos 10,#kylinos-10,3.4.kylinos 10,9
3.4.1.prepare kylinos 10,#prepare-kylinos-10,3.4.1.prepare kylinos 10,9
3.4.2.local repo installation for kylinos,#local-repo-installation-for-kylinos,3.4.2.local repo installation for kylinos,9
3.4.3.network repo installation for kylinos,#network-repo-installation-for-kylinos,3.4.3.network repo installation for kylinos,9
3.4.4.common instructions for kylinos 10,#common-instructions-for-kylinos-10,3.4.4.common instructions for kylinos 10 these instructions apply to both local and network installation.,9
3.5.fedora,#fedora,3.5.fedora,9
3.5.1.prepare fedora,#prepare-fedora,3.5.1.prepare fedora,9
2.5.cublas level-1 function reference,#cublas-level-1-function-reference,"2.5.cublas level-1 function reference in this chapter we describe the level-1 basic linear algebra subprograms (blas1) functions that perform scalar and vector based operations. we will use abbreviations <type> for type and <t> for the corresponding short type to make a more concise and clear presentation of the implemented functions. unless otherwise specified <type> and <t> have the following meanings: when the parameters and returned values of the function differ, which sometimes happens for complex input, the<t>can also have the following meaningssc,cs,dzandzd. the abbreviation\(\mathbf{re}(\cdot)\)and\(\mathbf{im}(\cdot)\)will stand for the real and imaginary part of a number, respectively. since imaginary part of a real number does not exist, we will consider it to be zero and can usually simply discard it from the equation where it is being used. also, the\(\bar{\alpha}\)will denote the complex conjugate of\(\alpha\). in general throughout the documentation, the lower case greek symbols\(\alpha\)and\(\beta\)will denote scalars, lower case english letters in bold type\(\mathbf{x}\)and\(\mathbf{y}\)will denote vectors and capital english letters\(a\),\(b\)and\(c\)will denote matrices.",3
2.4.heterogeneous programming,#heterogeneous-programming,"2.4.heterogeneous programming as illustrated byfigure 7, the cuda programming model assumes that the cuda threads execute on a physically separatedevicethat operates as a coprocessor to thehostrunning the c++ program. this is the case, for example, when the kernels execute on a gpu and the rest of the c++ program executes on a cpu. the cuda programming model also assumes that both the host and the device maintain their own separate memory spaces in dram, referred to ashost memoryanddevice memory, respectively. therefore, a program manages the global, constant, and texture memory spaces visible to kernels through calls to the cuda runtime (described inprogramming interface). this includes device memory allocation and deallocation as well as data transfer between host and device memory. unified memory providesmanaged memoryto bridge the host and device memory spaces. managed memory is accessible from all cpus and gpus in the system as a single, coherent memory image with a common address space. this capability enables oversubscription of device memory and can greatly simplify the task of porting applications by eliminating the need to explicitly mirror data on host and device. seeunified memory programmingfor an introduction to unified memory.",5
3.5.2.local repo installation for fedora,#local-repo-installation-for-fedora,3.5.2.local repo installation for fedora,9
3.5.3.network repo installation for fedora,#network-repo-installation-for-fedora,3.5.3.network repo installation for fedora,9
3.5.4.common installation instructions for fedora,#common-installation-instructions-for-fedora,3.5.4.common installation instructions for fedora these instructions apply to both local and network installation for fedora.,9
3.6.sles,#sles,3.6.sles,9
3.6.1.prepare sles,#prepare-sles,3.6.1.prepare sles,9
3.6.2.local repo installation for sles,#local-repo-installation-for-sles,3.6.2.local repo installation for sles,9
3.6.3.network repo installation for sles,#network-repo-installation-for-sles,3.6.3.network repo installation for sles,9
3.6.4.common installation instructions for sles,#common-installation-instructions-for-sles,3.6.4.common installation instructions for sles these instructions apply to both local and network installation for sles.,9
3.7.opensuse,#opensuse,3.7.opensuse,9
3.7.1.prepare opensuse,#prepare-opensuse,3.7.1.prepare opensuse,9
2.5.asynchronous simt programming model,#asynchronous-simt-programming-model,"2.5.asynchronous simt programming model in the cuda programming model a thread is the lowest level of abstraction for doing a computation or a memory operation. starting with devices based on the nvidia ampere gpu architecture, the cuda programming model provides acceleration to memory operations via the asynchronous programming model. the asynchronous programming model defines the behavior of asynchronous operations with respect to cuda threads. the asynchronous programming model defines the behavior ofasynchronous barrierfor synchronization between cuda threads. the model also explains and defines howcuda::memcpy_asynccan be used to move data asynchronously from global memory while computing in the gpu.",5
3.7.2.local repo installation for opensuse,#local-repo-installation-for-opensuse,3.7.2.local repo installation for opensuse,9
2.5.1.cublasi<t>amax(),#cublasi-t-amax,"2.5.1.cublasi<t>amax() this function supports the64-bit integer interface. this function finds the (smallest) index of the element of the maximum magnitude. hence, the result is the first\(i\)such that\(\left| \mathbf{im}\left( {x\lbrack j\rbrack} \right) \middle| + \middle| \mathbf{re}\left( {x\lbrack j\rbrack} \right) \right|\)is maximum for\(i = 1,\ldots,n\)and\(j = 1 + \left( {i - 1} \right)*\text{ incx}\). notice that the last equation reflects 1-based indexing used for compatibility with fortran. the possible error values returned by this function and their meanings are listed below. for references please refer to: isamax,idamax,icamax,izamax",3
2.5.2.cublasi<t>amin(),#cublasi-t-amin,"2.5.2.cublasi<t>amin() this function supports the64-bit integer interface. this function finds the (smallest) index of the element of the minimum magnitude. hence, the result is the first\(i\)such that\(\left| \mathbf{im}\left( {x\lbrack j\rbrack} \right) \middle| + \middle| \mathbf{re}\left( {x\lbrack j\rbrack} \right) \right|\)is minimum for\(i = 1,\ldots,n\)and\(j = 1 + \left( {i - 1} \right)*\text{incx}\)notice that the last equation reflects 1-based indexing used for compatibility with fortran. the possible error values returned by this function and their meanings are listed below. for references please refer to: isamin",3
2.5.3.cublas<t>asum(),#cublas-t-asum,"2.5.3.cublas<t>asum() this function supports the64-bit integer interface. this function computes the sum of the absolute values of the elements of vectorx. hence, the result is\(\left. \sum_{i = 1}^{n} \middle| \mathbf{im}\left( {x\lbrack j\rbrack} \right) \middle| + \middle| \mathbf{re}\left( {x\lbrack j\rbrack} \right) \right|\)where\(j = 1 + \left( {i - 1} \right)*\text{incx}\). notice that the last equation reflects 1-based indexing used for compatibility with fortran. the possible error values returned by this function and their meanings are listed below. for references please refer to: sasum,dasum,scasum,dzasum",3
2.5.4.cublas<t>axpy(),#cublas-t-axpy,"2.5.4.cublas<t>axpy() this function supports the64-bit integer interface. this function multiplies the vectorxby the scalar\(\alpha\)and adds it to the vectoryoverwriting the latest vector with the result. hence, the performed operation is\(\mathbf{y}\lbrack j\rbrack = \alpha \times \mathbf{x}\lbrack k\rbrack + \mathbf{y}\lbrack j\rbrack\)for\(i = 1,\ldots,n\),\(k = 1 + \left( {i - 1} \right)*\text{incx}\)and\(j = 1 + \left( {i - 1} \right)*\text{incy}\). notice that the last two equations reflect 1-based indexing used for compatibility with fortran. the possible error values returned by this function and their meanings are listed below. for references please refer to: saxpy,daxpy,caxpy,zaxpy",3
2.5.5.cublas<t>copy(),#cublas-t-copy,"2.5.5.cublas<t>copy() this function supports the64-bit integer interface. this function copies the vectorxinto the vectory. hence, the performed operation is\(\mathbf{y}\lbrack j\rbrack = \mathbf{x}\lbrack k\rbrack\)for\(i = 1,\ldots,n\),\(k = 1 + \left( {i - 1} \right)*\text{incx}\)and\(j = 1 + \left( {i - 1} \right)*\text{incy}\). notice that the last two equations reflect 1-based indexing used for compatibility with fortran. the possible error values returned by this function and their meanings are listed below. for references please refer to: scopy,dcopy,ccopy,zcopy",3
3.7.3.network repo installation for opensuse,#network-repo-installation-for-opensuse,3.7.3.network repo installation for opensuse,9
3.7.4.common installation instructions for opensuse,#common-installation-instructions-for-opensuse,3.7.4.common installation instructions for opensuse these instructions apply to both local and network installation for opensuse.,9
3.8.wsl,#wsl,3.8.wsl these instructions must be used if you are installing in a wsl environment. do not use the ubuntu instructions in this case; it is important to not install thecuda-driverspackages within the wsl environment.,9
2.5.1.asynchronous operations,#asynchronous-operations,"2.5.1.asynchronous operations an asynchronous operation is defined as an operation that is initiated by a cuda thread and is executed asynchronously as-if by another thread. in a well formed program one or more cuda threads synchronize with the asynchronous operation. the cuda thread that initiated the asynchronous operation is not required to be among the synchronizing threads. such an asynchronous thread (an as-if thread) is always associated with the cuda thread that initiated the asynchronous operation. an asynchronous operation uses a synchronization object to synchronize the completion of the operation. such a synchronization object can be explicitly managed by a user (e.g.,cuda::memcpy_async) or implicitly managed within a library (e.g.,cooperative_groups::memcpy_async). a synchronization object could be acuda::barrieror acuda::pipeline. these objects are explained in detail inasynchronous barrierandasynchronous data copies using cuda::pipeline. these synchronization objects can be used at different thread scopes. a scope defines the set of threads that may use the synchronization object to synchronize with the asynchronous operation. the following table defines the thread scopes available in cuda c++ and the threads that can be synchronized with each. these thread scopes are implemented as extensions to standard c++ in thecuda standard c++library.",5
2.6.compute capability,#compute-capability,"2.6.compute capability thecompute capabilityof a device is represented by a version number, also sometimes called its sm version. this version number identifies the features supported by the gpu hardware and is used by applications at runtime to determine which hardware features and/or instructions are available on the present gpu. the compute capability comprises a major revision numberxand a minor revision numberyand is denoted byx.y. devices with the same major revision number are of the same core architecture. the major revision number is 9 for devices based on thenvidia hopper gpuarchitecture, 8 for devices based on thenvidia ampere gpuarchitecture, 7 for devices based on thevoltaarchitecture, 6 for devices based on thepascalarchitecture, 5 for devices based on themaxwellarchitecture, and 3 for devices based on thekeplerarchitecture. the minor revision number corresponds to an incremental improvement to the core architecture, possibly including new features. turingis the architecture for devices of compute capability 7.5, and is an incremental update based on thevoltaarchitecture. cuda-enabled gpuslists of all cuda-enabled devices along with their compute capability.compute capabilitiesgives the technical specifications of each compute capability. theteslaandfermiarchitectures are no longer supported starting with cuda 7.0 and cuda 9.0, respectively.",5
3.programming interface,#programming-interface,"3.programming interface cuda c++ provides a simple path for users familiar with the c++ programming language to easily write programs for execution by the device. it consists of a minimal set of extensions to the c++ language and a runtime library. the core language extensions have been introduced inprogramming model. they allow programmers to define a kernel as a c++ function and use some new syntax to specify the grid and block dimension each time the function is called. a complete description of all extensions can be found inc++ language extensions. any source file that contains some of these extensions must be compiled withnvccas outlined incompilation with nvcc. the runtime is introduced incuda runtime. it provides c and c++ functions that execute on the host to allocate and deallocate device memory, transfer data between host memory and device memory, manage systems with multiple devices, etc. a complete description of the runtime can be found in the cuda reference manual. the runtime is built on top of a lower-level c api, the cuda driver api, which is also accessible by the application. the driver api provides an additional level of control by exposing lower-level concepts such as cuda contexts - the analogue of host processes for the device - and cuda modules - the analogue of dynamically loaded libraries for the device. most applications do not use the driver api as they do not need this additional level of control and when using the runtime, context and module management are implicit, resulting in more concise code. as the runtime is interoperable with the driver api, most applications that need some driver api features can default to use the runtime api and only use the driver api where needed. the driver api is introduced indriver apiand fully described in the reference manual.",0
3.1.compilation with nvcc,#compilation-with-nvcc,"3.1.compilation with nvcc kernels can be written using the cuda instruction set architecture, calledptx, which is described in the ptx reference manual. it is however usually more effective to use a high-level programming language such as c++. in both cases, kernels must be compiled into binary code bynvccto execute on the device. nvccis a compiler driver that simplifies the process of compilingc++orptxcode: it provides simple and familiar command line options and executes them by invoking the collection of tools that implement the different compilation stages. this section gives an overview ofnvccworkflow and command options. a complete description can be found in thenvccuser manual.",0
3.1.1.compilation workflow,#compilation-workflow,3.1.1.compilation workflow,9
3.8.1.prepare wsl,#prepare-wsl,3.8.1.prepare wsl,9
2.5.6.cublas<t>dot(),#cublas-t-dot,"2.5.6.cublas<t>dot() this function supports the64-bit integer interface. this function computes the dot product of vectorsxandy. hence, the result is\(\sum_{i = 1}^{n}\left( {\mathbf{x}\lbrack k\rbrack \times \mathbf{y}\lbrack j\rbrack} \right)\)where\(k = 1 + \left( {i - 1} \right)*\text{incx}\)and\(j = 1 + \left( {i - 1} \right)*\text{incy}\). notice that in the first equation the conjugate of the element of vector x should be used if the function name ends in character c and that the last two equations reflect 1-based indexing used for compatibility with fortran. the possible error values returned by this function and their meanings are listed below. for references please refer to: sdot,ddot,cdotu,cdotc,zdotu,zdotc",3
2.5.7.cublas<t>nrm2(),#cublas-t-nrm2,"2.5.7.cublas<t>nrm2() this function supports the64-bit integer interface. this function computes the euclidean norm of the vectorx. the code uses a multiphase model of accumulation to avoid intermediate underflow and overflow, with the result being equivalent to\(\sqrt{\sum_{i = 1}^{n}\left( {\mathbf{x}\lbrack j\rbrack \times \mathbf{x}\lbrack j\rbrack} \right)}\)where\(j = 1 + \left( {i - 1} \right)*\text{incx}\)in exact arithmetic. notice that the last equation reflects 1-based indexing used for compatibility with fortran. the possible error values returned by this function and their meanings are listed below. for references please refer to: snrm2,dnrm2,scnrm2,dznrm2",3
2.5.8.cublas<t>rot(),#cublas-t-rot,"2.5.8.cublas<t>rot() this function supports the64-bit integer interface. this function applies givens rotation matrix (i.e., rotation in the x,y plane counter-clockwise by angle defined by cos(alpha)=c, sin(alpha)=s): \(g = \begin{pmatrix}
c & s \\
{- s} & c \\
\end{pmatrix}\) to vectorsxandy. hence, the result is\(\mathbf{x}\lbrack k\rbrack = c \times \mathbf{x}\lbrack k\rbrack + s \times \mathbf{y}\lbrack j\rbrack\)and\(\mathbf{y}\lbrack j\rbrack = - s \times \mathbf{x}\lbrack k\rbrack + c \times \mathbf{y}\lbrack j\rbrack\)where\(k = 1 + \left( {i - 1} \right)*\text{incx}\)and\(j = 1 + \left( {i - 1} \right)*\text{incy}\). notice that the last two equations reflect 1-based indexing used for compatibility with fortran. the possible error values returned by this function and their meanings are listed below. for references please refer to: srot,drot,crot,csrot,zrot,zdrot",3
2.5.9.cublas<t>rotg(),#cublas-t-rotg,"2.5.9.cublas<t>rotg() this function supports the64-bit integer interface. this function constructs the givens rotation matrix \(g = \begin{pmatrix}
c & s \\
{- s} & c \\
\end{pmatrix}\) that zeros out the second entry of a\(2 \times 1\)vector\(\left( {a,b} \right)^{t}\). then, for real numbers we can write \(\begin{pmatrix}
c & s \\
{- s} & c \\
\end{pmatrix}\begin{pmatrix}
a \\
b \\
\end{pmatrix} = \begin{pmatrix}
r \\
0 \\
\end{pmatrix}\) where\(c^{2} + s^{2} = 1\)and\(r = a^{2} + b^{2}\). the parameters\(a\)and\(b\)are overwritten with\(r\)and\(z\), respectively. the value of\(z\)is such that\(c\)and\(s\)may be recovered using the following rules: \(\left( {c,s} \right) = \begin{cases}
\left( {\sqrt{1 - z^{2}},z} \right) & {\text{ if }\left| z \middle| < 1 \right.} \\
\left( {0.0,1.0} \right) & {\text{ if }\left| z \middle| = 1 \right.} \\
\left( 1/z,\sqrt{1 - z^{2}} \right) & {\text{ if }\left| z \middle| > 1 \right.} \\
\end{cases}\) for complex numbers we can write \(\begin{pmatrix}
c & s \\
{- \bar{s}} & c \\
\end{pmatrix}\begin{pmatrix}
a \\
b \\
\end{pmatrix} = \begin{pmatrix}
r \\
0 \\
\end{pmatrix}\) where\(c^{2} + \left( {\bar{s} \times s} \right) = 1\)and\(r = \frac{a}{|a|} \times \parallel \left( {a,b} \right)^{t} \parallel_{2}\)with\(\parallel \left( {a,b} \right)^{t} \parallel_{2} = \sqrt{\left| a|^{2} + \middle| b|^{2} \right.}\)for\(a \neq 0\)and\(r = b\)for\(a = 0\). finally, the parameter\(a\)is overwritten with\(r\)on exit. the possible error values returned by this function and their meanings are listed below. for references please refer to: srotg,drotg,crotg,zrotg",3
3.8.2.local repo installation for wsl,#local-repo-installation-for-wsl,3.8.2.local repo installation for wsl,9
2.5.10.cublas<t>rotm(),#cublas-t-rotm,"2.5.10.cublas<t>rotm() this function supports the64-bit integer interface. this function applies the modified givens transformation \(h = \begin{pmatrix}
h_{11} & h_{12} \\
h_{21} & h_{22} \\
\end{pmatrix}\) to vectorsxandy. hence, the result is\(\mathbf{x}\lbrack k\rbrack = h_{11} \times \mathbf{x}\lbrack k\rbrack + h_{12} \times \mathbf{y}\lbrack j\rbrack\)and\(\mathbf{y}\lbrack j\rbrack = h_{21} \times \mathbf{x}\lbrack k\rbrack + h_{22} \times \mathbf{y}\lbrack j\rbrack\)where\(k = 1 + \left( {i - 1} \right)*\text{incx}\)and\(j = 1 + \left( {i - 1} \right)*\text{incy}\). notice that the last two equations reflect 1-based indexing used for compatibility with fortran. the elements , , and of matrix\(h\)are stored inparam[1],param[2],param[3]andparam[4], respectively. theflag=param[0]defines the following predefined values for the matrix\(h\)entries notice that the values -1.0, 0.0 and 1.0 implied by the flag are not stored in param. the possible error values returned by this function and their meanings are listed below. for references please refer to: srotm,drotm",3
3.1.2.binary compatibility,#binary-compatibility,"3.1.2.binary compatibility binary code is architecture-specific. acubinobject is generated using the compiler option-codethat specifies the targeted architecture: for example, compiling with-code=sm_80produces binary code for devices ofcompute capability8.0. binary compatibility is guaranteed from one minor revision to the next one, but not from one minor revision to the previous one or across major revisions. in other words, acubinobject generated for compute capabilityx.ywill only execute on devices of compute capabilityx.zwherezy.",6
3.1.3.ptx compatibility,#ptx-compatibility,"3.1.3.ptx compatibility someptxinstructions are only supported on devices of higher compute capabilities. for example,warp shuffle functionsare only supported on devices of compute capability 5.0 and above. the-archcompiler option specifies the compute capability that is assumed when compiling c++ toptxcode. so, code that contains warp shuffle, for example, must be compiled with-arch=compute_50(or higher). ptxcode produced for some specific compute capability can always be compiled to binary code of greater or equal compute capability. note that a binary compiled from an earlier ptx version may not make use of some hardware features. for example, a binary targeting devices of compute capability 7.0 (volta) compiled from ptx generated for compute capability 6.0 (pascal) will not make use of tensor core instructions, since these were not available on pascal. as a result, the final binary may perform worse than would be possible if the binary were generated using the latest version of ptx. ptxcode compiled to targetarchitecture conditional featuresonly run on the exact same physical architecture and nowhere else. arch conditionalptxcode is not forward and backward compatible.
example code compiled withsm_90aorcompute_90aonly runs on devices with compute capability 9.0 and is not backward or forward compatible.",7
3.1.4.application compatibility,#application-compatibility,"3.1.4.application compatibility to execute code on devices of specific compute capability, an application must load binary orptxcode that is compatible with this compute capability as described inbinary compatibilityandptx compatibility. in particular, to be able to execute code on future architectures with higher compute capability (for which no binary code can be generated yet), an application must loadptxcode that will be just-in-time compiled for these devices (seejust-in-time compilation). whichptxand binary code gets embedded in a cuda c++ application is controlled by the-archand-codecompiler options or the-gencodecompiler option as detailed in thenvccuser manual. for example, embeds binary code compatible with compute capability 5.0 and 6.0 (first and second-gencodeoptions) andptxand binary code compatible with compute capability 7.0 (third-gencodeoption). host code is generated to automatically select at runtime the most appropriate code to load and execute, which, in the above example, will be: x.cucan have an optimized code path that uses warp reduction operations, for example, which are only supported in devices of compute capability 8.0 and higher. the__cuda_arch__macro can be used to differentiate various code paths based on compute capability. it is only defined for device code. when compiling with-arch=compute_80for example,__cuda_arch__is equal to800. ifx.cuis compiled forarchitecture conditional featuresexample withsm_90aorcompute_90a, the code can only run on devices with compute capability 9.0. applications using the driver api must compile code to separate files and explicitly load and execute the most appropriate file at runtime. the volta architecture introducesindependent thread schedulingwhich changes the way threads are scheduled on the gpu. for code relying on specific behavior ofsimt schedulingin previous architectures, independent thread scheduling may alter the set of participating threads, leading to incorrect results. to aid migration while implementing the corrective actions detailed inindependent thread scheduling, volta developers can opt-in to pascals thread scheduling with the compiler option combination-arch=compute_60-code=sm_70. thenvccuser manual lists various shorthands for the-arch,-code, and-gencodecompiler options. for example,-arch=sm_70is a shorthand for-arch=compute_70-code=compute_70,sm_70(which is the same as-gencodearch=compute_70,code=\""compute_70,sm_70\"").",0
3.1.5.c++ compatibility,#c-compatibility,"3.1.5.c++ compatibility the front end of the compiler processes cuda source files according to c++ syntax rules. full c++ is supported for the host code. however, only a subset of c++ is fully supported for the device code as described inc++ language support.",0
3.1.6.64-bit compatibility,#bit-compatibility,"3.1.6.64-bit compatibility the 64-bit version ofnvcccompiles device code in 64-bit mode (i.e., pointers are 64-bit). device code compiled in 64-bit mode is only supported with host code compiled in 64-bit mode.",6
3.2.cuda runtime,#cuda-runtime,"3.2.cuda runtime the runtime is implemented in thecudartlibrary, which is linked to the application, either statically viacudart.liborlibcudart.a, or dynamically viacudart.dllorlibcudart.so. applications that requirecudart.dlland/orcudart.sofor dynamic linking typically include them as part of the application installation package. it is only safe to pass the address of cuda runtime symbols between components that link to the same instance of the cuda runtime. all its entry points are prefixed withcuda. as mentioned inheterogeneous programming, the cuda programming model assumes a system composed of a host and a device, each with their own separate memory.device memorygives an overview of the runtime functions used to manage device memory. shared memoryillustrates the use of shared memory, introduced inthread hierarchy, to maximize performance. page-locked host memoryintroduces page-locked host memory that is required to overlap kernel execution with data transfers between host and device memory. asynchronous concurrent executiondescribes the concepts and api used to enable asynchronous concurrent execution at various levels in the system. multi-device systemshows how the programming model extends to a system with multiple devices attached to the same host. error checkingdescribes how to properly check the errors generated by the runtime. call stackmentions the runtime functions used to manage the cuda c++ call stack. texture and surface memorypresents the texture and surface memory spaces that provide another way to access device memory; they also expose a subset of the gpu texturing hardware. graphics interoperabilityintroduces the various functions the runtime provides to interoperate with the two main graphics apis, opengl and direct3d.",5
3.2.1.initialization,#initialization,"3.2.1.initialization as of cuda 12.0, thecudainitdevice()andcudasetdevice()calls initialize the runtime and the primary context associated with the specified device. absent these calls, the runtime will implicitly use device 0 and self-initialize as needed to process other runtime api requests. one needs to keep this in mind when timing runtime function calls and when interpreting the error code from the first call into the runtime. before 12.0,cudasetdevice()would not initialize the runtime and applications would often use the no-op runtime callcudafree(0)to isolate the runtime initialization from other api activity (both for the sake of timing and error handling). the runtime creates a cuda context for each device in the system (seecontextfor more details on cuda contexts). this context is theprimary contextfor this device and is initialized at the first runtime function which requires an active context on this device. it is shared among all the host threads of the application. as part of this context creation, the device code is just-in-time compiled if necessary (seejust-in-time compilation) and loaded into device memory. this all happens transparently. if needed, for example, for driver api interoperability, the primary context of a device can be accessed from the driver api as described ininteroperability between runtime and driver apis. when a host thread callscudadevicereset(), this destroys the primary context of the device the host thread currently operates on (i.e., the current device as defined indevice selection). the next runtime function call made by any host thread that has this device as current will create a new primary context for this device.",0
3.2.2.device memory,#device-memory,"3.2.2.device memory as mentioned inheterogeneous programming, the cuda programming model assumes a system composed of a host and a device, each with their own separate memory. kernels operate out of device memory, so the runtime provides functions to allocate, deallocate, and copy device memory, as well as transfer data between host memory and device memory. device memory can be allocated either aslinear memoryor ascuda arrays. cuda arrays are opaque memory layouts optimized for texture fetching. they are described intexture and surface memory. linear memory is allocated in a single unified address space, which means that separately allocated entities can reference one another via pointers, for example, in a binary tree or linked list. the size of the address space depends on the host system (cpu) and the compute capability of the used gpu: linear memory is typically allocated usingcudamalloc()and freed usingcudafree()and data transfer between host memory and device memory are typically done usingcudamemcpy(). in the vector addition code sample ofkernels, the vectors need to be copied from host memory to device memory: linear memory can also be allocated throughcudamallocpitch()andcudamalloc3d(). these functions are recommended for allocations of 2d or 3d arrays as it makes sure that the allocation is appropriately padded to meet the alignment requirements described indevice memory accesses, therefore ensuring best performance when accessing the row addresses or performing copies between 2d arrays and other regions of device memory (using thecudamemcpy2d()andcudamemcpy3d()functions). the returned pitch (or stride) must be used to access array elements. the following code sample allocates awidthxheight2d array of floating-point values and shows how to loop over the array elements in device code: the following code sample allocates awidthxheightxdepth3d array of floating-point values and shows how to loop over the array elements in device code: the reference manual lists all the various functions used to copy memory between linear memory allocated withcudamalloc(), linear memory allocated withcudamallocpitch()orcudamalloc3d(), cuda arrays, and memory allocated for variables declared in global or constant memory space. the following code sample illustrates various ways of accessing global variables via the runtime api: cudagetsymboladdress()is used to retrieve the address pointing to the memory allocated for a variable declared in global memory space. the size of the allocated memory is obtained throughcudagetsymbolsize().",5
3.2.3.device memory l2 access management,#device-memory-l2-access-management,"3.2.3.device memory l2 access management when a cuda kernel accesses a data region in the global memory repeatedly, such data accesses can be considered to bepersisting. on the other hand, if the data is only accessed once, such data accesses can be considered to bestreaming. starting with cuda 11.0, devices of compute capability 8.0 and above have the capability to influence persistence of data in the l2 cache, potentially providing higher bandwidth and lower latency accesses to global memory.",5
2.5.11.cublas<t>rotmg(),#cublas-t-rotmg,"2.5.11.cublas<t>rotmg() this function supports the64-bit integer interface. this function constructs the modified givens transformation \(h = \begin{pmatrix}
h_{11} & h_{12} \\
h_{21} & h_{22} \\
\end{pmatrix}\) that zeros out the second entry of a\(2 \times 1\)vector\(\left( {\sqrt{d1}*x1,\sqrt{d2}*y1} \right)^{t}\). theflag=param[0]defines the following predefined values for the matrix\(h\)entries notice that the values -1.0, 0.0 and 1.0 implied by the flag are not stored in param. the possible error values returned by this function and their meanings are listed below. for references please refer to: srotmg,drotmg",3
2.5.12.cublas<t>scal(),#cublas-t-scal,"2.5.12.cublas<t>scal() this function supports the64-bit integer interface. this function scales the vectorxby the scalar\(\alpha\)and overwrites it with the result. hence, the performed operation is\(\mathbf{x}\lbrack j\rbrack = \alpha \times \mathbf{x}\lbrack j\rbrack\)for\(i = 1,\ldots,n\)and\(j = 1 + \left( {i - 1} \right)*\text{incx}\). notice that the last two equations reflect 1-based indexing used for compatibility with fortran. the possible error values returned by this function and their meanings are listed below. for references please refer to: sscal,dscal,csscal,cscal,zdscal,zscal",3
2.5.13.cublas<t>swap(),#cublas-t-swap,"2.5.13.cublas<t>swap() this function supports the64-bit integer interface. this function interchanges the elements of vectorxandy. hence, the performed operation is\(\left. \mathbf{y}\lbrack j\rbrack\leftrightarrow\mathbf{x}\lbrack k\rbrack \right.\)for\(i = 1,\ldots,n\),\(k = 1 + \left( {i - 1} \right)*\text{incx}\)and\(j = 1 + \left( {i - 1} \right)*\text{incy}\). notice that the last two equations reflect 1-based indexing used for compatibility with fortran. the possible error values returned by this function and their meanings are listed below. for references please refer to: sswap,dswap,cswap,zswap",3
2.6.cublas level-2 function reference,#cublas-level-2-function-reference,2.6.cublas level-2 function reference in this chapter we describe the level-2 basic linear algebra subprograms (blas2) functions that perform matrix-vector operations.,3
2.6.1.cublas<t>gbmv(),#cublas-t-gbmv,"2.6.1.cublas<t>gbmv() this function supports the64-bit integer interface. this function performs the banded matrix-vector multiplication \(\mathbf{y} = \alpha\text{ op}(a)\mathbf{x} + \beta\mathbf{y}\) where\(a\)is a banded matrix with\(kl\)subdiagonals and\(ku\)superdiagonals,\(\mathbf{x}\)and\(\mathbf{y}\)are vectors, and\(\alpha\)and\(\beta\)are scalars. also, for matrix\(a\) \(\text{ op}(a) = \begin{cases}
a & \text{ if transa == $\mathrm{cublas\_op\_n}$} \\
a^{t} & \text{ if transa == $\mathrm{cublas\_op\_t}$} \\
a^{h} & \text{ if transa == $\mathrm{cublas\_op\_c}$} \\
\end{cases}\) the banded matrix\(a\)is stored column by column, with the main diagonal stored in row\(ku + 1\)(starting in first position), the first superdiagonal stored in row\(ku\)(starting in second position), the first subdiagonal stored in row\(ku + 2\)(starting in first position), etc. so that in general, the element\(a\left( {i,j} \right)\)is stored in the memory locationa(ku+1+i-j,j)for\(j = 1,\ldots,n\)and\(i \in \left\lbrack {\max\left( {1,j - ku} \right),\min\left( {m,j + kl} \right)} \right\rbrack\). also, the elements in the array\(a\)that do not conceptually correspond to the elements in the banded matrix (the top left\(ku \times ku\)and bottom right\(kl \times kl\)triangles) are not referenced. the possible error values returned by this function and their meanings are listed below. for references please refer to: sgbmv,dgbmv,cgbmv,zgbmv",3
3.2.4.shared memory,#shared-memory,"3.2.4.shared memory as detailed invariable memory space specifiersshared memory is allocated using the__shared__memory space specifier. shared memory is expected to be much faster than global memory as mentioned inthread hierarchyand detailed inshared memory. it can be used as scratchpad memory (or software managed cache) to minimize global memory accesses from a cuda block as illustrated by the following matrix multiplication example. the following code sample is a straightforward implementation of matrix multiplication that does not take advantage of shared memory. each thread reads one row ofaand one column ofband computes the corresponding element ofcas illustrated infigure 8.ais therefore readb.widthtimes from global memory andbis reada.heighttimes. the following code sample is an implementation of matrix multiplication that does take advantage of shared memory. in this implementation, each thread block is responsible for computing one square sub-matrixcsubofcand each thread within the block is responsible for computing one element ofcsub. as illustrated infigure 9,csubis equal to the product of two rectangular matrices: the sub-matrix ofaof dimension (a.width, block_size) that has the same row indices ascsub, and the sub-matrix ofbof dimension (block_size, a.width)that has the same column indices ascsub. in order to fit into the devices resources, these two rectangular matrices are divided into as many square matrices of dimensionblock_sizeas necessary andcsubis computed as the sum of the products of these square matrices. each of these products is performed by first loading the two corresponding square matrices from global memory to shared memory with one thread loading one element of each matrix, and then by having each thread compute one element of the product. each thread accumulates the result of each of these products into a register and once done writes the result to global memory. by blocking the computation this way, we take advantage of fast shared memory and save a lot of global memory bandwidth sinceais only read (b.width / block_size) times from global memory andbis read (a.height / block_size) times. thematrixtype from the previous code sample is augmented with astridefield, so that sub-matrices can be efficiently represented with the same type.__device__functions are used to get and set elements and build any sub-matrix from a matrix.",5
3.2.5.distributed shared memory,#distributed-shared-memory,"3.2.5.distributed shared memory thread block clusters introduced in compute capability 9.0 provide the ability for threads in a thread block cluster to access shared memory of all the participating thread blocks in a cluster. this partitioned shared memory is calleddistributed shared memory, and the corresponding address space is called distributed shared memory address space. threads that belong to a thread block cluster, can read, write or perform atomics in the distributed address space, regardless whether the address belongs to the local thread block or a remote thread block. whether a kernel uses distributed shared memory or not, the shared memory size specifications, static or dynamic is still per thread block. the size of distributed shared memory is just the number of thread blocks per cluster multiplied by the size of shared memory per thread block. accessing data in distributed shared memory requires all the thread blocks to exist. a user can guarantee that all thread blocks have started executing usingcluster.sync()fromcluster groupapi.
the user also needs to ensure that all distributed shared memory operations happen before the exit of a thread block, e.g., if a remote thread block is trying to read a given thread blocks shared memory, user needs to ensure that the shared memory read by remote thread block is completed before it can exit. cuda provides a mechanism to access to distributed shared memory, and applications can benefit from leveraging its capabilities. lets look at a simple histogram computation and how to optimize it on the gpu using thread block cluster. a standard way of computing histograms is do the computation in the shared memory of each thread block and then perform global memory atomics. a limitation of this approach is the shared memory capacity. once the histogram bins no longer fit in the shared memory, a user needs to directly compute histograms and hence the atomics in the global memory. with distributed shared memory, cuda provides an intermediate step, where a depending on the histogram bins size, histogram can be computed in shared memory, distributed shared memory or global memory directly. the cuda kernel example below shows how to compute histograms in shared memory or distributed shared memory, depending on the number of histogram bins. the above kernel can be launched at runtime with a cluster size depending on the amount of distributed shared memory required. if histogram is small enough to fit in shared memory of just one block, user can launch kernel with cluster size 1. the code snippet below shows how to launch a cluster kernel dynamically based depending on shared memory requirements.",5
3.2.6.page-locked host memory,#page-locked-host-memory,3.2.6.page-locked host memory the runtime provides functions to allow the use ofpage-locked(also known aspinned) host memory (as opposed to regular pageable host memory allocated bymalloc()): using page-locked host memory has several benefits: the simple zero-copy cuda sample comes with a detailed document on the page-locked memory apis.,5
3.2.7.memory synchronization domains,#memory-synchronization-domains,3.2.7.memory synchronization domains,5
2.6.2.cublas<t>gemv(),#cublas-t-gemv,"2.6.2.cublas<t>gemv() this function supports the64-bit integer interface. this function performs the matrix-vector multiplication \(\textbf{y} = \alpha\text{ op}(a)\textbf{x} + \beta\textbf{y}\) where\(a\)is a\(m \times n\)matrix stored in column-major format,\(\mathbf{x}\)and\(\mathbf{y}\)are vectors, and\(\alpha\)and\(\beta\)are scalars. also, for matrix\(a\) \(\text{ op}(a) = \begin{cases}
a & \text{ if transa == $\mathrm{cublas\_op\_n}$} \\
a^{t} & \text{ if transa == $\mathrm{cublas\_op\_t}$} \\
a^{h} & \text{ if transa == $\mathrm{cublas\_op\_c}$} \\
\end{cases}\) the possible error values returned by this function and their meanings are listed below. for references please refer to: sgemv,dgemv,cgemv,zgemv",3
2.6.3.cublas<t>ger(),#cublas-t-ger,"2.6.3.cublas<t>ger() this function supports the64-bit integer interface. this function performs the rank-1 update \(a = \begin{cases}
{\alpha\mathbf{xy}^{t} + a} & \text{if ger(),geru() is called} \\
{\alpha\mathbf{xy}^{h} + a} & \text{if gerc() is called} \\
\end{cases}\) where\(a\)is a\(m \times n\)matrix stored in column-major format,\(\mathbf{x}\)and\(\mathbf{y}\)are vectors, and\(\alpha\)is a scalar. the possible error values returned by this function and their meanings are listed below. for references please refer to: sger,dger,cgeru,cgerc,zgeru,zgerc",3
2.6.4.cublas<t>sbmv(),#cublas-t-sbmv,"2.6.4.cublas<t>sbmv() this function supports the64-bit integer interface. this function performs the symmetric banded matrix-vector multiplication \(\textbf{y} = \alpha a\textbf{x} + \beta\textbf{y}\) where\(a\)is a\(n \times n\)symmetric banded matrix with\(k\)subdiagonals and superdiagonals,\(\mathbf{x}\)and\(\mathbf{y}\)are vectors, and\(\alpha\)and\(\beta\)are scalars. ifuplo==cublas_fill_mode_lowerthen the symmetric banded matrix\(a\)is stored column by column, with the main diagonal of the matrix stored in row 1, the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc. so that in general, the element\(a(i,j)\)is stored in the memory locationa(1+i-j,j)for\(j = 1,\ldots,n\)and\(i \in \lbrack j,\min(m,j + k)\rbrack\). also, the elements in the arrayathat do not conceptually correspond to the elements in the banded matrix (the bottom right\(k \times k\)triangle) are not referenced. ifuplo==cublas_fill_mode_upperthen the symmetric banded matrix\(a\)is stored column by column, with the main diagonal of the matrix stored in rowk+1, the first superdiagonal in rowk(starting at second position), the second superdiagonal in rowk-1(starting at third position), etc. so that in general, the element\(a(i,j)\)is stored in the memory locationa(1+k+i-j,j)for\(j = 1,\ldots,n\)and\(i \in \lbrack\max(1,j - k),j\rbrack\). also, the elements in the arrayathat do not conceptually correspond to the elements in the banded matrix (the top left\(k \times k\)triangle) are not referenced. the possible error values returned by this function and their meanings are listed below. for references please refer to: ssbmv,dsbmv",3
2.6.5.cublas<t>spmv(),#cublas-t-spmv,"2.6.5.cublas<t>spmv() this function supports the64-bit integer interface. this function performs the symmetric packed matrix-vector multiplication \(\textbf{y} = \alpha a\textbf{x} + \beta\textbf{y}\) where\(a\)is a\(n \times n\)symmetric matrix stored in packed format,\(\mathbf{x}\)and\(\mathbf{y}\)are vectors, and\(\alpha\)and\(\beta\)are scalars. ifuplo==cublas_fill_mode_lowerthen the elements in the lower triangular part of the symmetric matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+((2*n-j+1)*j)/2]for\(j = 1,\ldots,n\)and\(i \geq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. ifuplo==cublas_fill_mode_upperthen the elements in the upper triangular part of the symmetric matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+(j*(j+1))/2]for\(j = 1,\ldots,n\)and\(i \leq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. the possible error values returned by this function and their meanings are listed below. for references please refer to: sspmv,dspmv",3
2.6.6.cublas<t>spr(),#cublas-t-spr,"2.6.6.cublas<t>spr() this function supports the64-bit integer interface. this function performs the packed symmetric rank-1 update \(a = \alpha\textbf{x}\textbf{x}^{t} + a\) where\(a\)is a\(n \times n\)symmetric matrix stored in packed format,\(\mathbf{x}\)is a vector, and\(\alpha\)is a scalar. ifuplo==cublas_fill_mode_lowerthen the elements in the lower triangular part of the symmetric matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+((2*n-j+1)*j)/2]for\(j = 1,\ldots,n\)and\(i \geq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. ifuplo==cublas_fill_mode_upperthen the elements in the upper triangular part of the symmetric matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+(j*(j+1))/2]for\(j = 1,\ldots,n\)and\(i \leq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. the possible error values returned by this function and their meanings are listed below. for references please refer to: sspr,dspr",3
2.6.7.cublas<t>spr2(),#cublas-t-spr2,"2.6.7.cublas<t>spr2() this function supports the64-bit integer interface. this function performs the packed symmetric rank-2 update \(a = \alpha\left( {\textbf{x}\textbf{y}^{t} + \textbf{y}\textbf{x}^{t}} \right) + a\) where\(a\)is a\(n \times n\)symmetric matrix stored in packed format,\(\mathbf{x}\)is a vector, and\(\alpha\)is a scalar. ifuplo==cublas_fill_mode_lowerthen the elements in the lower triangular part of the symmetric matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+((2*n-j+1)*j)/2]for\(j = 1,\ldots,n\)and\(i \geq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. ifuplo==cublas_fill_mode_upperthen the elements in the upper triangular part of the symmetric matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+(j*(j+1))/2]for\(j = 1,\ldots,n\)and\(i \leq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. the possible error values returned by this function and their meanings are listed below. for references please refer to: sspr2,dspr2",3
2.6.8.cublas<t>symv(),#id2,"2.6.8.cublas<t>symv() this function supports the64-bit integer interface. this function performs the symmetric matrix-vector multiplication. \(\textbf{y} = \alpha a\textbf{x} + \beta\textbf{y}\)where\(a\)is a\(n \times n\)symmetric matrix stored in lower or upper mode,\(\mathbf{x}\)and\(\mathbf{y}\)are vectors, and\(\alpha\)and\(\beta\)are scalars. this function has an alternate faster implementation using atomics that can be enabled withcublassetatomicsmode(). please see the section on the functioncublassetatomicsmode()for more details about the usage of atomics. the possible error values returned by this function and their meanings are listed below. for references please refer to: ssymv,dsymv",3
2.6.9.cublas<t>syr(),#cublas-t-syr,"2.6.9.cublas<t>syr() this function supports the64-bit integer interface. this function performs the symmetric rank-1 update \(a = \alpha\textbf{x}\textbf{x}^{t} + a\) where\(a\)is a\(n \times n\)symmetric matrix stored in column-major format,\(\mathbf{x}\)is a vector, and\(\alpha\)is a scalar. the possible error values returned by this function and their meanings are listed below. for references please refer to: ssyr,dsyr",3
2.6.10.cublas<t>syr2(),#cublas-t-syr2,"2.6.10.cublas<t>syr2() this function supports the64-bit integer interface. this function performs the symmetric rank-2 update \(a = \alpha\left( {\textbf{x}\textbf{y}^{t} + \textbf{y}\textbf{x}^{t}} \right) + a\) where\(a\)is a\(n \times n\)symmetric matrix stored in column-major format,\(\mathbf{x}\)and\(\mathbf{y}\)are vectors, and\(\alpha\)is a scalar. the possible error values returned by this function and their meanings are listed below. for references please refer to: ssyr2,dsyr2",3
2.6.11.cublas<t>tbmv(),#cublas-t-tbmv,"2.6.11.cublas<t>tbmv() this function supports the64-bit integer interface. this function performs the triangular banded matrix-vector multiplication \(\textbf{x} = \text{op}(a)\textbf{x}\) where\(a\)is a triangular banded matrix, and\(\mathbf{x}\)is a vector. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) ifuplo==cublas_fill_mode_lowerthen the triangular banded matrix\(a\)is stored column by column, with the main diagonal of the matrix stored in row1, the first subdiagonal in row2(starting at first position), the second subdiagonal in row3(starting at first position), etc. so that in general, the element\(a(i,j)\)is stored in the memory locationa(1+i-j,j)for\(j = 1,\ldots,n\)and\(i \in \lbrack j,\min(m,j + k)\rbrack\). also, the elements in the arrayathat do not conceptually correspond to the elements in the banded matrix (the bottom right\(k \times k\)triangle) are not referenced. ifuplo==cublas_fill_mode_upperthen the triangular banded matrix\(a\)is stored column by column, with the main diagonal of the matrix stored in rowk+1, the first superdiagonal in rowk(starting at second position), the second superdiagonal in rowk-1(starting at third position), etc. so that in general, the element\(a(i,j)\)is stored in the memory locationa(1+k+i-j,j)for\(j = 1,\ldots,n\)and\(i \in \lbrack\max(1,j - k,j)\rbrack\). also, the elements in the arrayathat do not conceptually correspond to the elements in the banded matrix (the top left\(k \times k\)triangle) are not referenced. the possible error values returned by this function and their meanings are listed below. for references please refer to: stbmv,dtbmv,ctbmv,ztbmv",3
2.6.12.cublas<t>tbsv(),#cublas-t-tbsv,"2.6.12.cublas<t>tbsv() this function supports the64-bit integer interface. this function solves the triangular banded linear system with a single right-hand-side \(\text{op}(a)\textbf{x} = \textbf{b}\) where\(a\)is a triangular banded matrix, and\(\mathbf{x}\)and\(\mathbf{b}\)are vectors. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) the solution\(\mathbf{x}\)overwrites the right-hand-sides\(\mathbf{b}\)on exit. no test for singularity or near-singularity is included in this function. ifuplo==cublas_fill_mode_lowerthen the triangular banded matrix\(a\)is stored column by column, with the main diagonal of the matrix stored in row1, the first subdiagonal in row2(starting at first position), the second subdiagonal in row3(starting at first position), etc. so that in general, the element\(a(i,j)\)is stored in the memory locationa(1+i-j,j)for\(j = 1,\ldots,n\)and\(i \in \lbrack j,\min(m,j + k)\rbrack\). also, the elements in the arrayathat do not conceptually correspond to the elements in the banded matrix (the bottom right\(k \times k\)triangle) are not referenced. ifuplo==cublas_fill_mode_upperthen the triangular banded matrix\(a\)is stored column by column, with the main diagonal of the matrix stored in rowk+1, the first superdiagonal in rowk(starting at second position), the second superdiagonal in rowk-1(starting at third position), etc. so that in general, the element\(a(i,j)\)is stored in the memory locationa(1+k+i-j,j)for\(j = 1,\ldots,n\)and\(i \in \lbrack\max(1,j - k,j)\rbrack\). also, the elements in the arrayathat do not conceptually correspond to the elements in the banded matrix (the top left\(k \times k\)triangle) are not referenced. the possible error values returned by this function and their meanings are listed below. for references please refer to: stbsv,dtbsv,ctbsv,ztbsv",3
2.6.13.cublas<t>tpmv(),#cublas-t-tpmv,"2.6.13.cublas<t>tpmv() this function supports the64-bit integer interface. this function performs the triangular packed matrix-vector multiplication \(\textbf{x} = \text{op}(a)\textbf{x}\) where\(a\)is a triangular matrix stored in packed format, and\(\mathbf{x}\)is a vector. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) ifuplo==cublas_fill_mode_lowerthen the elements in the lower triangular part of the triangular matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+((2*n-j+1)*j)/2]for\(j = 1,\ldots,n\)and\(i \geq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. ifuplo==cublas_fill_mode_upperthen the elements in the upper triangular part of the triangular matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+(j*(j+1))/2]for\(a(i,j)\)and\(i \leq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. the possible error values returned by this function and their meanings are listed below. for references please refer to: stpmv,dtpmv,ctpmv,ztpmv",3
2.6.14.cublas<t>tpsv(),#cublas-t-tpsv,"2.6.14.cublas<t>tpsv() this function supports the64-bit integer interface. this function solves the packed triangular linear system with a single right-hand-side \(\text{op}(a)\textbf{x} = \textbf{b}\) where\(a\)is a triangular matrix stored in packed format, and\(\mathbf{x}\)and\(\mathbf{b}\)are vectors. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) the solution\(\mathbf{x}\)overwrites the right-hand-sides\(\mathbf{b}\)on exit. no test for singularity or near-singularity is included in this function. ifuplo==cublas_fill_mode_lowerthen the elements in the lower triangular part of the triangular matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+((2*n-j+1)*j)/2]for\(j = 1,\ldots,n\)and\(i \geq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. ifuplo==cublas_fill_mode_upperthen the elements in the upper triangular part of the triangular matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+(j*(j+1))/2]for\(j = 1,\ldots,n\)and\(i \leq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. the possible error values returned by this function and their meanings are listed below. for references please refer to: stpsv,dtpsv,ctpsv,ztpsv",3
3.2.8.asynchronous concurrent execution,#asynchronous-concurrent-execution,3.2.8.asynchronous concurrent execution cuda exposes the following operations as independent tasks that can operate concurrently with one another: the level of concurrency achieved between these operations will depend on the feature set and compute capability of the device as described below.,5
3.8.3.network repo installation for wsl,#network-repo-installation-for-wsl,"3.8.3.network repo installation for wsl the new gpg public key for the cuda repository (debian-based distros) is3bf863cc. this must be enrolled on the system, either using thecuda-keyringpackage or manually; theapt-keycommand is deprecated and not recommended.",0
2.6.15.cublas<t>trmv(),#cublas-t-trmv,"2.6.15.cublas<t>trmv() this function supports the64-bit integer interface. this function performs the triangular matrix-vector multiplication \(\textbf{x} = \text{op}(a)\textbf{x}\) where\(a\)is a triangular matrix stored in lower or upper mode with or without the main diagonal, and\(\mathbf{x}\)is a vector. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) the possible error values returned by this function and their meanings are listed below. for references please refer to: strmv,dtrmv,ctrmv,ztrmv",3
2.6.16.cublas<t>trsv(),#cublas-t-trsv,"2.6.16.cublas<t>trsv() this function supports the64-bit integer interface. this function solves the triangular linear system with a single right-hand-side \(\text{op}(a)\textbf{x} = \textbf{b}\) where\(a\)is a triangular matrix stored in lower or upper mode with or without the main diagonal, and\(\mathbf{x}\)and\(\mathbf{b}\)are vectors. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) the solution\(\mathbf{x}\)overwrites the right-hand-sides\(\mathbf{b}\)on exit. no test for singularity or near-singularity is included in this function. the possible error values returned by this function and their meanings are listed below. for references please refer to: strsv,dtrsv,ctrsv,ztrsv",3
3.2.9.multi-device system,#multi-device-system,3.2.9.multi-device system,9
3.2.10.unified virtual address space,#unified-virtual-address-space,"3.2.10.unified virtual address space when the application is run as a 64-bit process, a single address space is used for the host and all the devices of compute capability 2.0 and higher. all host memory allocations made via cuda api calls and all device memory allocations on supported devices are within this virtual address range. as a consequence: applications may query if the unified address space is used for a particular device by checking that theunifiedaddressingdevice property (seedevice enumeration) is equal to 1.",5
4.5.cudbgevent::cases_st::contextdestroy_st struct reference,https://docs.nvidia.com/cuda/debugger-api/structCUDBGEvent_1_1cases__st_1_1contextDestroy__st.html#structCUDBGEvent_1_1cases__st_1_1contextDestroy__st,4.5.cudbgevent::cases_st::contextdestroy_st struct reference,8
2.6.17.cublas<t>hemv(),#id3,"2.6.17.cublas<t>hemv() this function supports the64-bit integer interface. this function performs the hermitian matrix-vector multiplication \(\textbf{y} = \alpha a\textbf{x} + \beta\textbf{y}\) where\(a\)is a\(n \times n\)hermitian matrix stored in lower or upper mode,\(\mathbf{x}\)and\(\mathbf{y}\)are vectors, and\(\alpha\)and\(\beta\)are scalars. this function has an alternate faster implementation using atomics that can be enabled with please see the section on the for more details about the usage of atomics the possible error values returned by this function and their meanings are listed below. for references please refer to: chemv,zhemv",3
3.2.11.interprocess communication,#interprocess-communication,"3.2.11.interprocess communication any device memory pointer or event handle created by a host thread can be directly referenced by any other thread within the same process. it is not valid outside this process however, and therefore cannot be directly referenced by threads belonging to a different process. to share device memory pointers and events across processes, an application must use the inter process communication api, which is described in detail in the reference manual. the ipc api is only supported for 64-bit processes on linux and for devices of compute capability 2.0 and higher. note that the ipc api is not supported forcudamallocmanagedallocations. using this api, an application can get the ipc handle for a given device memory pointer usingcudaipcgetmemhandle(), pass it to another process using standard ipc mechanisms (for example, interprocess shared memory or files), and usecudaipcopenmemhandle()to retrieve a device pointer from the ipc handle that is a valid pointer within this other process. event handles can be shared using similar entry points. note that allocations made bycudamalloc()may be sub-allocated from a larger block of memory for performance reasons. in such case, cuda ipc apis will share the entire underlying memory block which may cause other sub-allocations to be shared, which can potentially lead to information disclosure between processes. to prevent this behavior, it is recommended to only share allocations with a 2mib aligned size. an example of using the ipc api is where a single primary process generates a batch of input data, making the data available to multiple secondary processes without requiring regeneration or copying. applications using cuda ipc to communicate with each other should be compiled, linked, and run with the same cuda driver and runtime.",5
3.8.4.common installation instructions for wsl,#common-installation-instructions-for-wsl,3.8.4.common installation instructions for wsl these instructions apply to both local and network installation for wsl.,9
6.25.egl interoperability,https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EGL.html#group__CUDART__EGL_1g631d1080365d32a35a19b87584725748,6.25.egl interoperability,9
3.9.ubuntu,#ubuntu,3.9.ubuntu,9
3.9.1.prepare ubuntu,#prepare-ubuntu,3.9.1.prepare ubuntu,9
3.9.2.local repo installation for ubuntu,#local-repo-installation-for-ubuntu,3.9.2.local repo installation for ubuntu,9
3.9.3.network repo installation for ubuntu,#network-repo-installation-for-ubuntu,"3.9.3.network repo installation for ubuntu the new gpg public key for the cuda repository is3bf863cc. this must be enrolled on the system, either using thecuda-keyringpackage or manually; theapt-keycommand is deprecated and not recommended.",0
3.9.4.common installation instructions for ubuntu,#common-installation-instructions-for-ubuntu,3.9.4.common installation instructions for ubuntu these instructions apply to both local and network installation for ubuntu.,9
3.10.debian,#debian,3.10.debian,9
3.10.1.prepare debian,#prepare-debian,3.10.1.prepare debian,9
3.10.2.local repo installation for debian,#local-repo-installation-for-debian,3.10.2.local repo installation for debian,9
2.6.18.cublas<t>hbmv(),#cublas-t-hbmv,"2.6.18.cublas<t>hbmv() this function supports the64-bit integer interface. this function performs the hermitian banded matrix-vector multiplication \(\textbf{y} = \alpha a\textbf{x} + \beta\textbf{y}\) where\(a\)is a\(n \times n\)hermitian banded matrix with\(k\)subdiagonals and superdiagonals,\(\mathbf{x}\)and\(\mathbf{y}\)are vectors, and\(\alpha\)and\(\beta\)are scalars. ifuplo==cublas_fill_mode_lowerthen the hermitian banded matrix\(a\)is stored column by column, with the main diagonal of the matrix stored in row1, the first subdiagonal in row2(starting at first position), the second subdiagonal in row3(starting at first position), etc. so that in general, the element\(a(i,j)\)is stored in the memory locationa(1+i-j,j)for\(j = 1,\ldots,n\)and\(i \in \lbrack j,\min(m,j + k)\rbrack\). also, the elements in the arrayathat do not conceptually correspond to the elements in the banded matrix (the bottom right\(k \times k\)triangle) are not referenced. ifuplo==cublas_fill_mode_upperthen the hermitian banded matrix\(a\)is stored column by column, with the main diagonal of the matrix stored in rowk+1, the first superdiagonal in rowk(starting at second position), the second superdiagonal in rowk-1(starting at third position), etc. so that in general, the element\(a(i,j)\)is stored in the memory locationa(1+k+i-j,j)for\(j = 1,\ldots,n\)and\(i \in \lbrack\max(1,j - k),j\rbrack\). also, the elements in the arrayathat do not conceptually correspond to the elements in the banded matrix (the top left\(k \times k\)triangle) are not referenced. the possible error values returned by this function and their meanings are listed below. for references please refer to: chbmv,zhbmv",3
2.6.19.cublas<t>hpmv(),#cublas-t-hpmv,"2.6.19.cublas<t>hpmv() this function supports the64-bit integer interface. this function performs the hermitian packed matrix-vector multiplication \(\textbf{y} = \alpha a\textbf{x} + \beta\textbf{y}\) where\(a\)is a\(n \times n\)hermitian matrix stored in packed format,\(\mathbf{x}\)and\(\mathbf{y}\)are vectors, and\(\alpha\)and\(\beta\)are scalars. ifuplo==cublas_fill_mode_lowerthen the elements in the lower triangular part of the hermitian matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+((2*n-j+1)*j)/2]for\(j = 1,\ldots,n\)and\(i \geq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. ifuplo==cublas_fill_mode_upperthen the elements in the upper triangular part of the hermitian matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+(j*(j+1))/2]for\(j = 1,\ldots,n\)and\(i \leq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. the possible error values returned by this function and their meanings are listed below. for references please refer to: chpmv,zhpmv",3
2.6.20.cublas<t>her(),#cublas-t-her,"2.6.20.cublas<t>her() this function supports the64-bit integer interface. this function performs the hermitian rank-1 update \(a = \alpha\textbf{x}\textbf{x}^{h} + a\) where\(a\)is a\(n \times n\)hermitian matrix stored in column-major format,\(\mathbf{x}\)is a vector, and\(\alpha\)is a scalar. the possible error values returned by this function and their meanings are listed below. for references please refer to: cher,zher",3
2.6.21.cublas<t>her2(),#cublas-t-her2,"2.6.21.cublas<t>her2() this function supports the64-bit integer interface. this function performs the hermitian rank-2 update \(a = \alpha\textbf{x}\textbf{y}^{h} + \overset{}{\alpha}\textbf{y}\textbf{x}^{h} + a\) where\(a\)is a\(n \times n\)hermitian matrix stored in column-major format,\(\mathbf{x}\)and\(\mathbf{y}\)are vectors, and\(\alpha\)is a scalar. the possible error values returned by this function and their meanings are listed below. for references please refer to: cher2, zher2",3
2.6.22.cublas<t>hpr(),#cublas-t-hpr,"2.6.22.cublas<t>hpr() this function supports the64-bit integer interface. this function performs the packed hermitian rank-1 update \(a = \alpha\textbf{x}\textbf{x}^{h} + a\) where\(a\)is a\(n \times n\)hermitian matrix stored in packed format,\(\mathbf{x}\)is a vector, and\(\alpha\)is a scalar. ifuplo==cublas_fill_mode_lowerthen the elements in the lower triangular part of the hermitian matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+((2*n-j+1)*j)/2]for\(j = 1,\ldots,n\)and\(i \geq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. ifuplo==cublas_fill_mode_upperthen the elements in the upper triangular part of the hermitian matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+(j*(j+1))/2]for\(j = 1,\ldots,n\)and\(i \leq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. the possible error values returned by this function and their meanings are listed below. for references please refer to: chpr,zhpr",3
2.6.23.cublas<t>hpr2(),#cublas-t-hpr2,"2.6.23.cublas<t>hpr2() this function supports the64-bit integer interface. this function performs the packed hermitian rank-2 update \(a = \alpha\textbf{x}\textbf{y}^{h} + \overset{}{\alpha}\textbf{y}\textbf{x}^{h} + a\) where\(a\)is a\(n \times n\)hermitian matrix stored in packed format,\(\mathbf{x}\)and\(\mathbf{y}\)are vectors, and\(\alpha\)is a scalar. ifuplo==cublas_fill_mode_lowerthen the elements in the lower triangular part of the hermitian matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+((2*n-j+1)*j)/2]for\(j = 1,\ldots,n\)and\(i \geq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. ifuplo==cublas_fill_mode_upperthen the elements in the upper triangular part of the hermitian matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+(j*(j+1))/2]for\(j = 1,\ldots,n\)and\(i \leq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. the possible error values returned by this function and their meanings are listed below. for references please refer to: chpr2, zhpr2",3
2.6.24.cublas<t>gemvbatched(),#cublas-t-gemvbatched,"2.6.24.cublas<t>gemvbatched() this function supports the64-bit integer interface. this function performs the matrix-vector multiplication of a batch of matrices and vectors. the batch is considered to be uniform, i.e. all instances have the same dimensions (m, n), leading dimension (lda), increments (incx, incy) and transposition (trans) for their respective a matrix, x and y vectors. the address of the input matrix and vector, and the output vector of each instance of the batch are read from arrays of pointers passed to the function by the caller. \(\textbf{y}\lbrack i\rbrack = \alpha\text{op}(a\lbrack i\rbrack)\textbf{x}\lbrack i\rbrack + \beta\textbf{y}\lbrack i\rbrack,\text{ for i} \in \lbrack 0,batchcount - 1\rbrack\) where\(\alpha\)and\(\beta\)are scalars, and\(a\)is an array of pointers to matrice\(a\lbrack i\rbrack\)stored in column-major format with dimension\(m \times n\), and\(\textbf{x}\)and\(\textbf{y}\)are arrays of pointers to vectors. also, for matrix\(a\lbrack i\rbrack\), \(\text{op}(a\lbrack i\rbrack) = \left\{ \begin{matrix}
{a\lbrack i\rbrack} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_n}$}} \\
{a\lbrack i\rbrack}^{t} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_t}$}} \\
{a\lbrack i\rbrack}^{h} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) on certain problem sizes, it might be advantageous to make multiple calls tocublas<t>gemvin different cuda streams, rather than use this api. if math mode enables fast math modes when usingcublassgemvbatched(), pointers (not the pointer arrays) placed in the gpu memory must be properly aligned to avoid misaligned memory access errors. ideally all pointers are aligned to at least 16 bytes. otherwise it is recommended that they meet the following rule: the possible error values returned by this function and their meanings are listed below.",3
3.2.12.error checking,#error-checking,"3.2.12.error checking all runtime functions return an error code, but for an asynchronous function (seeasynchronous concurrent execution), this error code cannot possibly report any of the asynchronous errors that could occur on the device since the function returns before the device has completed the task; the error code only reports errors that occur on the host prior to executing the task, typically related to parameter validation; if an asynchronous error occurs, it will be reported by some subsequent unrelated runtime function call. the only way to check for asynchronous errors just after some asynchronous function call is therefore to synchronize just after the call by callingcudadevicesynchronize()(or by using any other synchronization mechanisms described inasynchronous concurrent execution) and checking the error code returned bycudadevicesynchronize(). the runtime maintains an error variable for each host thread that is initialized tocudasuccessand is overwritten by the error code every time an error occurs (be it a parameter validation error or an asynchronous error).cudapeekatlasterror()returns this variable.cudagetlasterror()returns this variable and resets it tocudasuccess. kernel launches do not return any error code, socudapeekatlasterror()orcudagetlasterror()must be called just after the kernel launch to retrieve any pre-launch errors. to ensure that any error returned bycudapeekatlasterror()orcudagetlasterror()does not originate from calls prior to the kernel launch, one has to make sure that the runtime error variable is set tocudasuccessjust before the kernel launch, for example, by callingcudagetlasterror()just before the kernel launch. kernel launches are asynchronous, so to check for asynchronous errors, the application must synchronize in-between the kernel launch and the call tocudapeekatlasterror()orcudagetlasterror(). note thatcudaerrornotreadythat may be returned bycudastreamquery()andcudaeventquery()is not considered an error and is therefore not reported bycudapeekatlasterror()orcudagetlasterror().",5
3.2.13.call stack,#call-stack,"3.2.13.call stack on devices of compute capability 2.x and higher, the size of the call stack can be queried usingcudadevicegetlimit()and set usingcudadevicesetlimit(). when the call stack overflows, the kernel call fails with a stack overflow error if the application is run via a cuda debugger (cuda-gdb, nsight) or an unspecified launch error, otherwise.
when the compiler cannot determine the stack size, it issues a warning saying stack size cannot be statically determined. this is usually the case with recursive functions.
once this warning is issued, user will need to set stack size manually if default stack size is not sufficient.",6
6.22.execution control,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EXEC.html#group__CUDA__EXEC_1g5e92a1b0d8d1b82cb00dcfb2de15961b,6.22.execution control,9
6.42.vdpau interoperability,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__VDPAU.html,6.42.vdpau interoperability,9
3.2.14.texture and surface memory,#texture-and-surface-memory,3.2.14.texture and surface memory cuda supports a subset of the texturing hardware that the gpu uses for graphics to access texture and surface memory. reading data from texture or surface memory instead of global memory can have several performance benefits as described indevice memory accesses.,5
3.2.15.graphics interoperability,#graphics-interoperability,"3.2.15.graphics interoperability some resources from opengl and direct3d may be mapped into the address space of cuda, either to enable cuda to read data written by opengl or direct3d, or to enable cuda to write data for consumption by opengl or direct3d. a resource must be registered to cuda before it can be mapped using the functions mentioned inopengl interoperabilityanddirect3d interoperability. these functions return a pointer to a cuda graphics resource of typestructcudagraphicsresource. registering a resource is potentially high-overhead and therefore typically called only once per resource. a cuda graphics resource is unregistered usingcudagraphicsunregisterresource(). each cuda context which intends to use the resource is required to register it separately. once a resource is registered to cuda, it can be mapped and unmapped as many times as necessary usingcudagraphicsmapresources()andcudagraphicsunmapresources().cudagraphicsresourcesetmapflags()can be called to specify usage hints (write-only, read-only) that the cuda driver can use to optimize resource management. a mapped resource can be read from or written to by kernels using the device memory address returned bycudagraphicsresourcegetmappedpointer()for buffers andcudagraphicssubresourcegetmappedarray()for cuda arrays. accessing a resource through opengl, direct3d, or another cuda context while it is mapped produces undefined results.opengl interoperabilityanddirect3d interoperabilitygive specifics for each graphics api and some code samples.sli interoperabilitygives specifics for when the system is in sli mode.",5
3.2.16.external resource interoperability,#external-resource-interoperability,"3.2.16.external resource interoperability external resource interoperability allows cuda to import certain resources that are explicitly exported by other apis. these objects are typically exported by other apis using handles native to the operating system, like file descriptors on linux or nt handles on windows. they could also be exported using other unified interfaces such as the nvidia software communication interface. there are two types of resources that can be imported: memory objects and synchronization objects. memory objects can be imported into cuda usingcudaimportexternalmemory(). an imported memory object can be accessed from within kernels using device pointers mapped onto the memory object viacudaexternalmemorygetmappedbuffer()or cuda mipmapped arrays mapped viacudaexternalmemorygetmappedmipmappedarray(). depending on the type of memory object, it may be possible for more than one mapping to be setup on a single memory object. the mappings must match the mappings setup in the exporting api. any mismatched mappings result in undefined behavior. imported memory objects must be freed usingcudadestroyexternalmemory(). freeing a memory object does not free any mappings to that object. therefore, any device pointers mapped onto that object must be explicitly freed usingcudafree()and any cuda mipmapped arrays mapped onto that object must be explicitly freed usingcudafreemipmappedarray(). it is illegal to access mappings to an object after it has been destroyed. synchronization objects can be imported into cuda usingcudaimportexternalsemaphore(). an imported synchronization object can then be signaled usingcudasignalexternalsemaphoresasync()and waited on usingcudawaitexternalsemaphoresasync(). it is illegal to issue a wait before the corresponding signal has been issued. also, depending on the type of the imported synchronization object, there may be additional constraints imposed on how they can be signaled and waited on, as described in subsequent sections. imported semaphore objects must be freed usingcudadestroyexternalsemaphore(). all outstanding signals and waits must have completed before the semaphore object is destroyed.",5
3.3.versioning and compatibility,#versioning-and-compatibility,"3.3.versioning and compatibility there are two version numbers that developers should care about when developing a cuda application: the compute capability that describes the general specifications and features of the compute device (seecompute capability) and the version of the cuda driver api that describes the features supported by the driver api and runtime. the version of the driver api is defined in the driver header file ascuda_version. it allows developers to check whether their application requires a newer device driver than the one currently installed. this is important, because the driver api isbackward compatible, meaning that applications, plug-ins, and libraries (including the cuda runtime) compiled against a particular version of the driver api will continue to work on subsequent device driver releases as illustrated infigure 12. the driver api is notforward compatible, which means that applications, plug-ins, and libraries (including the cuda runtime) compiled against a particular version of the driver api will not work on previous versions of the device driver. it is important to note that there are limitations on the mixing and matching of versions that is supported: for tesla gpu products, cuda 10 introduced a new forward-compatible upgrade path for the user-mode components of the cuda driver. this feature is described incuda compatibility. the requirements on the cuda driver version described here apply to the version of the user-mode components.",0
3.4.compute modes,#compute-modes,"3.4.compute modes on tesla solutions running windows server 2008 and later or linux, one can set any device in a system in one of the three following modes using nvidias system management interface (nvidia-smi), which is a tool distributed as part of the driver: this means, in particular, that a host thread using the runtime api without explicitly callingcudasetdevice()might be associated with a device other than device 0 if device 0 turns out to be in prohibited mode or in exclusive-process mode and used by another process.cudasetvaliddevices()can be used to set a device from a prioritized list of devices. note also that, for devices featuring the pascal architecture onwards (compute capability with major revision number 6 and higher), there exists support for compute preemption. this allows compute tasks to be preempted at instruction-level granularity, rather than thread block granularity as in prior maxwell and kepler gpu architecture, with the benefit that applications with long-running kernels can be prevented from either monopolizing the system or timing out. however, there will be context switch overheads associated with compute preemption, which is automatically enabled on those devices for which support exists. the individual attribute query functioncudadevicegetattribute()with the attributecudadevattrcomputepreemptionsupportedcan be used to determine if the device in use supports compute preemption. users wishing to avoid context switch overheads associated with different processes can ensure that only one process is active on the gpu by selecting exclusive-process mode. applications may query the compute mode of a device by checking thecomputemodedevice property (seedevice enumeration).",5
3.10.3.network repo installation for debian,#network-repo-installation-for-debian,"3.10.3.network repo installation for debian the new gpg public key for the cuda repository (debian-based distros) is3bf863cc. this must be enrolled on the system, either using the cuda-keyring package or manually; theapt-keycommand is deprecated and not recommended.",0
3.10.4.common installation instructions for debian,#common-installation-instructions-for-debian,3.10.4.common installation instructions for debian these instructions apply to both local and network installation for debian.,9
3.11.amazon linux 2023,#amazon-linux-2023,3.11.amazon linux 2023,9
3.11.1.prepare amazon linux 2023,#prepare-amazon-linux-2023,3.11.1.prepare amazon linux 2023,9
3.11.2.local repo installation for amazon linux,#local-repo-installation-for-amazon-linux,3.11.2.local repo installation for amazon linux,9
3.11.3.network repo installation for amazon linux,#network-repo-installation-for-amazon-linux,3.11.3.network repo installation for amazon linux,9
3.11.4.common installation instructions for amazon linux,#common-installation-instructions-for-amazon-linux,3.11.4.common installation instructions for amazon linux these instructions apply to both local and network installation for amazon linux.,9
3.12.additional package manager capabilities,#additional-package-manager-capabilities,3.12.additional package manager capabilities below are some additional capabilities of the package manager that users can take advantage of.,9
3.12.1.available packages,#available-packages,"3.12.1.available packages the recommended installation package is thecudapackage. this package will install the full set of other cuda packages required for native development and should cover most scenarios. thecudapackage installs all the available packages for native developments. that includes the compiler, the debugger, the profiler, the math libraries, and so on. for x86_64 platforms, this also includes nsight eclipse edition and the visual profilers. it also includes the nvidia driver package. on supported platforms, thecuda-cross-aarch64andcuda-cross-sbsapackages install all the packages required for cross-platform development to arm64-jetson and arm64-server, respectively. the libraries and header files of the target architectures display driver package are also installed to enable the cross compilation of driver applications. thecuda-cross-<arch>packages do not install the native display driver. the packages installed by the packages above can also be installed individually by specifying their names explicitly. the list of available packages be can obtained with:",0
3.12.2.meta packages,#meta-packages,3.12.2.meta packages meta packages are rpm/deb/conda packages which contain no (or few) files but have multiple dependencies. they are used to install many cuda packages when you may not know the details of the packages you want. the following table lists the meta packages.,0
3.12.3.optional 32-bit packages for linux x86_64 .deb/.rpm,#optional-32-bit-packages-for-linux-x86-64-deb-rpm,"3.12.3.optional 32-bit packages for linux x86_64 .deb/.rpm these packages provide 32-bit driver libraries needed for things such as steam (popular game app store/launcher), older video games, and some compute applications. for debian 10 and debian 11: for debian 12: for ubuntu: where<branch>is the driver version, for example 495. for fedora and rhel8+: for opensuse/sles:",9
3.12.4.package upgrades,#package-upgrades,"3.12.4.package upgrades thecudapackage points to the latest stable release of the cuda toolkit. when a new version is available, use the following commands to upgrade the toolkit and driver: thecuda-cross-<arch>packages can also be upgraded in the same manner. the cuda-drivers package points to the latest driver release available in the cuda repository. when a new version is available, use the following commands to upgrade the driver: some desktop environments, such as gnome or kde, will display a notification alert when new packages are available. to avoid any automatic upgrade, and lock down the toolkit installation to the x.y release, install thecuda-x-yorcuda-cross-<arch>-x-ypackage. side-by-side installations are supported. for instance, to install both the x.y cuda toolkit and the x.y+1 cuda toolkit, install thecuda-x.yandcuda-x.y+1packages.",0
4.driver installation,#driver-installation,4.driver installation this section is for users who want to install a specific driver version. for debian and ubuntu: for example: for opensuse and sles: for example: this allows you to get the highest version in the specified branch. for fedora and rhel8+: where profile by default is default and does not need to be specified. to uninstall or change streams on fedora and rhel8:,9
2.6.25.cublas<t>gemvstridedbatched(),#cublas-t-gemvstridedbatched,"2.6.25.cublas<t>gemvstridedbatched() this function supports the64-bit integer interface. this function performs the matrix-vector multiplication of a batch of matrices and vectors. the batch is considered to be uniform, i.e. all instances have the same dimensions (m, n), leading dimension (lda), increments (incx, incy) and transposition (trans) for their respective a matrix, x and y vectors. input matrix a and vector x, and output vector y for each instance of the batch are located at fixed offsets in number of elements from their locations in the previous instance. pointers to a matrix, x and y vectors for the first instance are passed to the function by the user along with offsets in number of elements - stridea, stridex and stridey that determine the locations of input matrices and vectors, and output vectors in future instances. \(\textbf{y} + i*{stridey} = \alpha\text{op}(a + i*{stridea})(\textbf{x} + i*{stridex}) + \beta(\textbf{y} + i*{stridey}),\text{ for i } \in \lbrack 0,batchcount - 1\rbrack\) where\(\alpha\)and\(\beta\)are scalars, and\(a\)is an array of pointers to matrix stored in column-major format with dimension\(a\lbrack i\rbrack\)\(m \times n\), and\(\textbf{x}\)and\(\textbf{y}\)are arrays of pointers to vectors. also, for matrix\(a\lbrack i\rbrack\) \(\text{op}(a\lbrack i\rbrack) = \left\{ \begin{matrix}
{a\lbrack i\rbrack} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_n}$}} \\
{a\lbrack i\rbrack}^{t} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_t}$}} \\
{a\lbrack i\rbrack}^{h} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) on certain problem sizes, it might be advantageous to make multiple calls tocublas<t>gemvin different cuda streams, rather than use this api. the possible error values returned by this function and their meanings are listed below.",3
6.9.execution control [deprecated],https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EXECUTION__DEPRECATED.html#group__CUDART__EXECUTION__DEPRECATED_1gbd189716def6fdb5f819dae77452d30b,6.9.execution control [deprecated],6
3.5.mode switches,#mode-switches,"3.5.mode switches gpus that have a display output dedicate some dram memory to the so-calledprimary surface, which is used to refresh the display device whose output is viewed by the user. when users initiate amode switchof the display by changing the resolution or bit depth of the display (using nvidia control panel or the display control panel on windows), the amount of memory needed for the primary surface changes. for example, if the user changes the display resolution from 1280x1024x32-bit to 1600x1200x32-bit, the system must dedicate 7.68 mb to the primary surface rather than 5.24 mb. (full-screen graphics applications running with anti-aliasing enabled may require much more display memory for the primary surface.) on windows, other events that may initiate display mode switches include launching a full-screen directx application, hitting alt+tab to task switch away from a full-screen directx application, or hitting ctrl+alt+del to lock the computer. if a mode switch increases the amount of memory needed for the primary surface, the system may have to cannibalize memory allocations dedicated to cuda applications. therefore, a mode switch results in any call to the cuda runtime to fail and return an invalid context error.",5
3.6.tesla compute cluster mode for windows,#tesla-compute-cluster-mode-for-windows,"3.6.tesla compute cluster mode for windows using nvidias system management interface (nvidia-smi), the windows device driver can be put in tcc (tesla compute cluster) mode for devices of the tesla and quadro series. tcc mode removes support for any graphics functionality.",5
4.hardware implementation,#hardware-implementation,"4.hardware implementation the nvidia gpu architecture is built around a scalable array of multithreadedstreaming multiprocessors(sms). when a cuda program on the host cpu invokes a kernel grid, the blocks of the grid are enumerated and distributed to multiprocessors with available execution capacity. the threads of a thread block execute concurrently on one multiprocessor, and multiple thread blocks can execute concurrently on one multiprocessor. as thread blocks terminate, new blocks are launched on the vacated multiprocessors. a multiprocessor is designed to execute hundreds of threads concurrently. to manage such a large number of threads, it employs a unique architecture calledsimt(single-instruction, multiple-thread) that is described insimt architecture. the instructions are pipelined, leveraging instruction-level parallelism within a single thread, as well as extensive thread-level parallelism through simultaneous hardware multithreading as detailed inhardware multithreading. unlike cpu cores, they are issued in order and there is no branch prediction or speculative execution. simt architectureandhardware multithreadingdescribe the architecture features of the streaming multiprocessor that are common to all devices.compute capability 5.x,compute capability 6.x, andcompute capability 7.xprovide the specifics for devices of compute capabilities 5.x, 6.x, and 7.x respectively. the nvidia gpu architecture uses a little-endian representation.",5
4.1.simt architecture,#simt-architecture,"4.1.simt architecture the multiprocessor creates, manages, schedules, and executes threads in groups of 32 parallel threads calledwarps. individual threads composing a warp start together at the same program address, but they have their own instruction address counter and register state and are therefore free to branch and execute independently. the termwarporiginates from weaving, the first parallel thread technology. ahalf-warpis either the first or second half of a warp. aquarter-warpis either the first, second, third, or fourth quarter of a warp. when a multiprocessor is given one or more thread blocks to execute, it partitions them into warps and each warp gets scheduled by awarp schedulerfor execution. the way a block is partitioned into warps is always the same; each warp contains threads of consecutive, increasing thread ids with the first warp containing thread 0.thread hierarchydescribes how thread ids relate to thread indices in the block. a warp executes one common instruction at a time, so full efficiency is realized when all 32 threads of a warp agree on their execution path. if threads of a warp diverge via a data-dependent conditional branch, the warp executes each branch path taken, disabling threads that are not on that path. branch divergence occurs only within a warp; different warps execute independently regardless of whether they are executing common or disjoint code paths. the simt architecture is akin to simd (single instruction, multiple data) vector organizations in that a single instruction controls multiple processing elements. a key difference is that simd vector organizations expose the simd width to the software, whereas simt instructions specify the execution and branching behavior of a single thread. in contrast with simd vector machines, simt enables programmers to write thread-level parallel code for independent, scalar threads, as well as data-parallel code for coordinated threads. for the purposes of correctness, the programmer can essentially ignore the simt behavior; however, substantial performance improvements can be realized by taking care that the code seldom requires threads in a warp to diverge. in practice, this is analogous to the role of cache lines in traditional code: cache line size can be safely ignored when designing for correctness but must be considered in the code structure when designing for peak performance. vector architectures, on the other hand, require the software to coalesce loads into vectors and manage divergence manually. prior to nvidia volta, warps used a single program counter shared amongst all 32 threads in the warp together with an active mask specifying the active threads of the warp. as a result, threads from the same warp in divergent regions or different states of execution cannot signal each other or exchange data, and algorithms requiring fine-grained sharing of data guarded by locks or mutexes can easily lead to deadlock, depending on which warp the contending threads come from. starting with the nvidia volta architecture,independent thread schedulingallows full concurrency between threads, regardless of warp. with independent thread scheduling, the gpu maintains execution state per thread, including a program counter and call stack, and can yield execution at a per-thread granularity, either to make better use of execution resources or to allow one thread to wait for data to be produced by another. a schedule optimizer determines how to group active threads from the same warp together into simt units. this retains the high throughput of simt execution as in prior nvidia gpus, but with much more flexibility: threads can now diverge and reconverge at sub-warp granularity. independent thread scheduling can lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity2of previous hardware architectures. in particular, any warp-synchronous code (such as synchronization-free, intra-warp reductions) should be revisited to ensure compatibility with nvidia volta and beyond. seecompute capability 7.xfor further details.",5
4.2.hardware multithreading,#hardware-multithreading,"4.2.hardware multithreading the execution context (program counters, registers, and so on) for each warp processed by a multiprocessor is maintained on-chip during the entire lifetime of the warp. therefore, switching from one execution context to another has no cost, and at every instruction issue time, a warp scheduler selects a warp that has threads ready to execute its next instruction (theactive threadsof the warp) and issues the instruction to those threads. in particular, each multiprocessor has a set of 32-bit registers that are partitioned among the warps, and aparallel data cacheorshared memorythat is partitioned among the thread blocks. the number of blocks and warps that can reside and be processed together on the multiprocessor for a given kernel depends on the amount of registers and shared memory used by the kernel and the amount of registers and shared memory available on the multiprocessor. there are also a maximum number of resident blocks and a maximum number of resident warps per multiprocessor. these limits as well the amount of registers and shared memory available on the multiprocessor are a function of the compute capability of the device and are given incompute capabilities. if there are not enough registers or shared memory available per multiprocessor to process at least one block, the kernel will fail to launch. the total number of warps in a block is as follows: \(\text{ceil}\left( \frac{t}{w_{size}},1 \right)\) the total number of registers and total amount of shared memory allocated for a block are documented in the cuda occupancy calculator provided in the cuda toolkit.",5
5.performance guidelines,#performance-guidelines,5.performance guidelines,9
5.1.overall performance optimization strategies,#overall-performance-optimization-strategies,"5.1.overall performance optimization strategies performance optimization revolves around four basic strategies: which strategies will yield the best performance gain for a particular portion of an application depends on the performance limiters for that portion; optimizing instruction usage of a kernel that is mostly limited by memory accesses will not yield any significant performance gain, for example. optimization efforts should therefore be constantly directed by measuring and monitoring the performance limiters, for example using the cuda profiler. also, comparing the floating-point operation throughput or memory throughputwhichever makes more senseof a particular kernel to the corresponding peak theoretical throughput of the device indicates how much room for improvement there is for the kernel.",5
5.2.maximize utilization,#maximize-utilization,5.2.maximize utilization to maximize utilization the application should be structured in a way that it exposes as much parallelism as possible and efficiently maps this parallelism to the various components of the system to keep them busy most of the time.,5
5.2.1.application level,#application-level,"5.2.1.application level at a high level, the application should maximize parallel execution between the host, the devices, and the bus connecting the host to the devices, by using asynchronous functions calls and streams as described inasynchronous concurrent execution. it should assign to each processor the type of work it does best: serial workloads to the host; parallel workloads to the devices. for the parallel workloads, at points in the algorithm where parallelism is broken because some threads need to synchronize in order to share data with each other, there are two cases: either these threads belong to the same block, in which case they should use__syncthreads()and share data through shared memory within the same kernel invocation, or they belong to different blocks, in which case they must share data through global memory using two separate kernel invocations, one for writing to and one for reading from global memory. the second case is much less optimal since it adds the overhead of extra kernel invocations and global memory traffic. its occurrence should therefore be minimized by mapping the algorithm to the cuda programming model in such a way that the computations that require inter-thread communication are performed within a single thread block as much as possible.",5
5.nvidia open gpu kernel modules,#nvidia-open-gpu-kernel-modules,"5.nvidia open gpu kernel modules the nvidia linux gpu driver contains several kernel modules: starting in the 515 driver release series, two flavors of these kernel modules are provided: verify that your nvidia gpu is at least turing or newer generation. experimental support for geforce and quadro skus can be enabled with: to install nvidia open gpu kernel modules, follow the instructions below.",4
7.49.cuexecaffinityparam_v1 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUexecAffinityParam__v1.html#structCUexecAffinityParam__v1,7.49.cuexecaffinityparam_v1 struct reference,8
6.1.data types used by cuda driver,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TYPES.html#group__CUDA__TYPES_1gge12b8a782bebe21b1ac0091bf9f4e2a320db4c9e8d81eb05a937d4ed5c132575,6.1.data types used by cuda driver,0
2.7.cublas level-3 function reference,#cublas-level-3-function-reference,2.7.cublas level-3 function reference in this chapter we describe the level-3 basic linear algebra subprograms (blas3) functions that perform matrix-matrix operations.,3
2.7.1.cublas<t>gemm(),#cublas-t-gemm,"2.7.1.cublas<t>gemm() this function supports the64-bit integer interface. this function performs the matrix-matrix multiplication \(c = \alpha\text{op}(a)\text{op}(b) + \beta c\) where\(\alpha\)and\(\beta\)are scalars, and\(a\),\(b\)and\(c\)are matrices stored in column-major format with dimensions\(\text{op}(a)\)\(m \times k\),\(\text{op}(b)\)\(k \times n\)and\(c\)\(m \times n\), respectively. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) and\(\text{op}(b)\)is defined similarly for matrix\(b\). the possible error values returned by this function and their meanings are listed below. for references please refer to: sgemm,dgemm,cgemm,zgemm",3
2.7.2.cublas<t>gemm3m(),#cublas-t-gemm3m,"2.7.2.cublas<t>gemm3m() this function supports the64-bit integer interface. this function performs the complex matrix-matrix multiplication, using gauss complexity reduction algorithm. this can lead to an increase in performance up to 25% \(c = \alpha\text{op}(a)\text{op}(b) + \beta c\) where\(\alpha\)and\(\beta\)are scalars, and\(a\),\(b\)and\(c\)are matrices stored in column-major format with dimensions\(\text{op}(a)\)\(m \times k\),\(\text{op}(b)\)\(k \times n\)and\(c\)\(m \times n\), respectively. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) and\(\text{op}(b)\)is defined similarly for matrix\(b\). the possible error values returned by this function and their meanings are listed in the following table: for references please refer to: cgemm,zgemm",3
5.2.2.device level,#device-level,"5.2.2.device level at a lower level, the application should maximize parallel execution between the multiprocessors of a device. multiple kernels can execute concurrently on a device, so maximum utilization can also be achieved by using streams to enable enough kernels to execute concurrently as described inasynchronous concurrent execution.",5
5.2.3.multiprocessor level,#multiprocessor-level,"5.2.3.multiprocessor level at an even lower level, the application should maximize parallel execution between the various functional units within a multiprocessor. as described inhardware multithreading, a gpu multiprocessor primarily relies on thread-level parallelism to maximize utilization of its functional units. utilization is therefore directly linked to the number of resident warps. at every instruction issue time, a warp scheduler selects an instruction that is ready to execute. this instruction can be another independent instruction of the same warp, exploiting instruction-level parallelism, or more commonly an instruction of another warp, exploiting thread-level parallelism. if a ready to execute instruction is selected it is issued to theactivethreads of the warp. the number of clock cycles it takes for a warp to be ready to execute its next instruction is called thelatency, and full utilization is achieved when all warp schedulers always have some instruction to issue for some warp at every clock cycle during that latency period, or in other words, when latency is completely hidden. the number of instructions required to hide a latency of l clock cycles depends on the respective throughputs of these instructions (seearithmetic instructionsfor the throughputs of various arithmetic instructions). if we assume instructions with maximum throughput, it is equal to: the most common reason a warp is not ready to execute its next instruction is that the instructions input operands are not available yet. if all input operands are registers, latency is caused by register dependencies, i.e., some of the input operands are written by some previous instruction(s) whose execution has not completed yet. in this case, the latency is equal to the execution time of the previous instruction and the warp schedulers must schedule instructions of other warps during that time. execution time varies depending on the instruction. on devices of compute capability 7.x, for most arithmetic instructions, it is typically 4 clock cycles. this means that 16 active warps per multiprocessor (4 cycles, 4 warp schedulers) are required to hide arithmetic instruction latencies (assuming that warps execute instructions with maximum throughput, otherwise fewer warps are needed). if the individual warps exhibit instruction-level parallelism, i.e. have multiple independent instructions in their instruction stream, fewer warps are needed because multiple independent instructions from a single warp can be issued back to back. if some input operand resides in off-chip memory, the latency is much higher: typically hundreds of clock cycles. the number of warps required to keep the warp schedulers busy during such high latency periods depends on the kernel code and its degree of instruction-level parallelism. in general, more warps are required if the ratio of the number of instructions with no off-chip memory operands (i.e., arithmetic instructions most of the time) to the number of instructions with off-chip memory operands is low (this ratio is commonly called the arithmetic intensity of the program). another reason a warp is not ready to execute its next instruction is that it is waiting at some memory fence (memory fence functions) or synchronization point (synchronization functions). a synchronization point can force the multiprocessor to idle as more and more warps wait for other warps in the same block to complete execution of instructions prior to the synchronization point. having multiple resident blocks per multiprocessor can help reduce idling in this case, as warps from different blocks do not need to wait for each other at synchronization points. the number of blocks and warps residing on each multiprocessor for a given kernel call depends on the execution configuration of the call (execution configuration), the memory resources of the multiprocessor, and the resource requirements of the kernel as described inhardware multithreading. register and shared memory usage are reported by the compiler when compiling with the--ptxas-options=-voption. the total amount of shared memory required for a block is equal to the sum of the amount of statically allocated shared memory and the amount of dynamically allocated shared memory. the number of registers used by a kernel can have a significant impact on the number of resident warps. for example, for devices of compute capability 6.x, if a kernel uses 64
registers and each block has 512 threads and requires very little shared memory, then two blocks (i.e., 32 warps) can reside on the multiprocessor since they require 2x512x64
registers, which exactly matches the number of registers available on the multiprocessor. but as soon as the kernel uses one more register, only one block (i.e., 16 warps) can be
resident since two blocks would require 2x512x65 registers, which are more registers than are available on the multiprocessor. therefore, the compiler attempts to minimize register
usage while keeping register spilling (seedevice memory accesses) and the number of instructions to a minimum. register usage can be
controlled using themaxrregcountcompiler option, the__launch_bounds__()qualifier as described inlaunch bounds, or the__maxnreg__()qualifier as described inmaximum number of registers per thread. the register file is organized as 32-bit registers. so, each variable stored in a register needs at least one 32-bit register, for example, adoublevariable uses two 32-bit registers. the effect of execution configuration on performance for a given kernel call generally depends on the kernel code. experimentation is therefore recommended. applications can also parametrize execution configurations based on register file size and shared memory size, which depends on the compute capability of the device, as well as on the number of multiprocessors and memory bandwidth of the device, all of which can be queried using the runtime (see reference manual). the number of threads per block should be chosen as a multiple of the warp size to avoid wasting computing resources with under-populated warps as much as possible.",5
5.3.maximize memory throughput,#maximize-memory-throughput,"5.3.maximize memory throughput the first step in maximizing overall memory throughput for the application is to minimize data transfers with low bandwidth. that means minimizing data transfers between the host and the device, as detailed indata transfer between host and device, since these have much lower bandwidth than data transfers between global memory and the device. that also means minimizing data transfers between global memory and the device by maximizing use of on-chip memory: shared memory and caches (i.e., l1 cache and l2 cache available on devices of compute capability 2.x and higher, texture cache and constant cache available on all devices). shared memory is equivalent to a user-managed cache: the application explicitly allocates and accesses it. as illustrated incuda runtime, a typical programming pattern is to stage data coming from device memory into shared memory; in other words, to have each thread of a block: for some applications (for example, for which global memory access patterns are data-dependent), a traditional hardware-managed cache is more appropriate to exploit data locality. as mentioned incompute capability 7.x,compute capability 8.xandcompute capability 9.0, for devices of compute capability 7.x, 8.x and 9.0, the same on-chip memory is used for both l1 and shared memory, and how much of it is dedicated to l1 versus shared memory is configurable for each kernel call. the throughput of memory accesses by a kernel can vary by an order of magnitude depending on access pattern for each type of memory. the next step in maximizing memory throughput is therefore to organize memory accesses as optimally as possible based on the optimal memory access patterns described indevice memory accesses. this optimization is especially important for global memory accesses as global memory bandwidth is low compared to available on-chip bandwidths and arithmetic instruction throughput, so non-optimal global memory accesses generally have a high impact on performance.",5
5.3.1.data transfer between host and device,#data-transfer-between-host-and-device,"5.3.1.data transfer between host and device applications should strive to minimize data transfer between the host and the device. one way to accomplish this is to move more code from the host to the device, even if that means running kernels that do not expose enough parallelism to execute on the device with full efficiency. intermediate data structures may be created in device memory, operated on by the device, and destroyed without ever being mapped by the host or copied to host memory. also, because of the overhead associated with each transfer, batching many small transfers into a single large transfer always performs better than making each transfer separately. on systems with a front-side bus, higher performance for data transfers between host and device is achieved by using page-locked host memory as described inpage-locked host memory. in addition, when using mapped page-locked memory (mapped memory), there is no need to allocate any device memory and explicitly copy data between device and host memory. data transfers are implicitly performed each time the kernel accesses the mapped memory. for maximum performance, these memory accesses must be coalesced as with accesses to global memory (seedevice memory accesses). assuming that they are and that the mapped memory is read or written only once, using mapped page-locked memory instead of explicit copies between device and host memory can be a win for performance. on integrated systems where device memory and host memory are physically the same, any copy between host and device memory is superfluous and mapped page-locked memory should be used instead. applications may query a device isintegratedby checking that the integrated device property (seedevice enumeration) is equal to 1.",5
5.3.2.device memory accesses,#device-memory-accesses,"5.3.2.device memory accesses an instruction that accesses addressable memory (i.e., global, local, shared, constant, or texture memory) might need to be re-issued multiple times depending on the distribution of the memory addresses across the threads within the warp. how the distribution affects the instruction throughput this way is specific to each type of memory and described in the following sections. for example, for global memory, as a general rule, the more scattered the addresses are, the more reduced the throughput is. global memory global memory resides in device memory and device memory is accessed via 32-, 64-, or 128-byte memory transactions. these memory transactions must be naturally aligned: only the 32-, 64-, or 128-byte segments of device memory that are aligned to their size (i.e., whose first address is a multiple of their size) can be read or written by memory transactions. when a warp executes an instruction that accesses global memory, it coalesces the memory accesses of the threads within the warp into one or more of these memory transactions depending on the size of the word accessed by each thread and the distribution of the memory addresses across the threads. in general, the more transactions are necessary, the more unused words are transferred in addition to the words accessed by the threads, reducing the instruction throughput accordingly. for example, if a 32-byte memory transaction is generated for each threads 4-byte access, throughput is divided by 8. how many transactions are necessary and how much throughput is ultimately affected varies with the compute capability of the device.compute capability 5.x,compute capability 6.x,compute capability 7.x,compute capability 8.xandcompute capability 9.0give more details on how global memory accesses are handled for various compute capabilities. to maximize global memory throughput, it is therefore important to maximize coalescing by: size and alignment requirement global memory instructions support reading or writing words of size equal to 1, 2, 4, 8, or 16 bytes. any access (via a variable or a pointer) to data residing in global memory compiles to a single global memory instruction if and only if the size of the data type is 1, 2, 4, 8, or 16 bytes and the data is naturally aligned (i.e., its address is a multiple of that size). if this size and alignment requirement is not fulfilled, the access compiles to multiple instructions with interleaved access patterns that prevent these instructions from fully coalescing. it is therefore recommended to use types that meet this requirement for data that resides in global memory. the alignment requirement is automatically fulfilled for thebuilt-in vector types. for structures, the size and alignment requirements can be enforced by the compiler using the alignment specifiers__align__(8)or__align__(16), such as or any address of a variable residing in global memory or returned by one of the memory allocation routines from the driver or runtime api is always aligned to at least 256 bytes. reading non-naturally aligned 8-byte or 16-byte words produces incorrect results (off by a few words), so special care must be taken to maintain alignment of the starting address of any value or array of values of these types. a typical case where this might be easily overlooked is when using some custom global memory allocation scheme, whereby the allocations of multiple arrays (with multiple calls tocudamalloc()orcumemalloc()) is replaced by the allocation of a single large block of memory partitioned into multiple arrays, in which case the starting address of each array is offset from the blocks starting address. two-dimensional arrays a common global memory access pattern is when each thread of index(tx,ty)uses the following address to access one element of a 2d array of widthwidth, located at addressbaseaddressof typetype*(wheretypemeets the requirement described inmaximize utilization): for these accesses to be fully coalesced, both the width of the thread block and the width of the array must be a multiple of the warp size. in particular, this means that an array whose width is not a multiple of this size will be accessed much more efficiently if it is actually allocated with a width rounded up to the closest multiple of this size and its rows padded accordingly. thecudamallocpitch()andcumemallocpitch()functions and associated memory copy functions described in the reference manual enable programmers to write non-hardware-dependent code to allocate arrays that conform to these constraints. local memory local memory accesses only occur for some automatic variables as mentioned invariable memory space specifiers. automatic variables that the compiler is likely to place in local memory are: inspection of theptxassembly code (obtained by compiling with the-ptxor-keepoption) will tell if a variable has been placed in local memory during the first compilation phases as it will be declared using the.localmnemonic and accessed using theld.localandst.localmnemonics. even if it has not, subsequent compilation phases might still decide otherwise though if they find it consumes too much register space for the targeted architecture: inspection of thecubinobject usingcuobjdumpwill tell if this is the case. also, the compiler reports total local memory usage per kernel (lmem) when compiling with the--ptxas-options=-voption. note that some mathematical functions have implementation paths that might access local memory. the local memory space resides in device memory, so local memory accesses have the same high latency and low bandwidth as global memory accesses and are subject to the same requirements for memory coalescing as described indevice memory accesses. local memory is however organized such that consecutive 32-bit words are accessed by consecutive thread ids. accesses are therefore fully coalesced as long as all threads in a warp access the same relative address (for example, same index in an array variable, same member in a structure variable). on devices of compute capability 5.x onwards, local memory accesses are always cached in l2 in the same way as global memory accesses (seecompute capability 5.xandcompute capability 6.x). shared memory because it is on-chip, shared memory has much higher bandwidth and much lower latency than local or global memory. to achieve high bandwidth, shared memory is divided into equally-sized memory modules, called banks, which can be accessed simultaneously. any memory read or write request made ofnaddresses that fall inndistinct memory banks can therefore be serviced simultaneously, yielding an overall bandwidth that isntimes as high as the bandwidth of a single module. however, if two addresses of a memory request fall in the same memory bank, there is a bank conflict and the access has to be serialized. the hardware splits a memory request with bank conflicts into as many separate conflict-free requests as necessary, decreasing throughput by a factor equal to the number of separate memory requests. if the number of separate memory requests isn, the initial memory request is said to causen-way bank conflicts. to get maximum performance, it is therefore important to understand how memory addresses map to memory banks in order to schedule the memory requests so as to minimize bank conflicts. this is described incompute capability 5.x,compute capability 6.x,compute capability 7.x,compute capability 8.x, andcompute capability 9.0for devices of compute capability 5.x, 6.x, 7.x, 8.x, and 9.0 respectively. constant memory the constant memory space resides in device memory and is cached in the constant cache. a request is then split into as many separate requests as there are different memory addresses in the initial request, decreasing throughput by a factor equal to the number of separate requests. the resulting requests are then serviced at the throughput of the constant cache in case of a cache hit, or at the throughput of device memory otherwise. texture and surface memory the texture and surface memory spaces reside in device memory and are cached in texture cache, so a texture fetch or surface read costs one memory read from device memory only on a cache miss, otherwise it just costs one read from texture cache. the texture cache is optimized for 2d spatial locality, so threads of the same warp that read texture or surface addresses that are close together in 2d will achieve best performance. also, it is designed for streaming fetches with a constant latency; a cache hit reduces dram bandwidth demand but not fetch latency. reading device memory through texture or surface fetching present some benefits that can make it an advantageous alternative to reading device memory from global or constant memory:",5
5.4.maximize instruction throughput,#maximize-instruction-throughput,"5.4.maximize instruction throughput to maximize instruction throughput the application should: in this section, throughputs are given in number of operations per clock cycle per multiprocessor. for a warp size of 32, one instruction corresponds to 32 operations, so if n is the number of operations per clock cycle, the instruction throughput is n/32 instructions per clock cycle. all throughputs are for one multiprocessor. they must be multiplied by the number of multiprocessors in the device to get throughput for the whole device.",5
5.1.cuda runfile,#cuda-runfile,5.1.cuda runfile pass the cli argument to the cuda runfile to opt in to nvidia open gpu kernel modules:,0
defines,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TYPES.html#group__CUDA__TYPES_1gge12b8a782bebe21b1ac0091bf9f4e2a320db4c9e8d81eb05a937d4ed5c132575,defines,9
2.7.3.cublas<t>gemmbatched(),#cublas-t-gemmbatched,"2.7.3.cublas<t>gemmbatched() this function supports the64-bit integer interface. this function performs the matrix-matrix multiplication of a batch of matrices. the batch is considered to be uniform, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective a, b and c matrices. the address of the input matrices and the output matrix of each instance of the batch are read from arrays of pointers passed to the function by the caller. \(c\lbrack i\rbrack = \alpha\text{op}(a\lbrack i\rbrack)\text{op}(b\lbrack i\rbrack) + \beta c\lbrack i\rbrack,\text{ for i } \in \lbrack 0,batchcount - 1\rbrack\) where\(\alpha\)and\(\beta\)are scalars, and\(a\),\(b\)and\(c\)are arrays of pointers to matrices stored in column-major format with dimensions\(\text{op}(a\lbrack i\rbrack)\)\(m \times k\),\(\text{op}(b\lbrack i\rbrack)\)\(k \times n\)and\(c\lbrack i\rbrack\)\(m \times n\), respectively. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) and\(\text{op}(b\lbrack i\rbrack)\)is defined similarly for matrix\(b\lbrack i\rbrack\). on certain problem sizes, it might be advantageous to make multiple calls tocublas<t>gemmin different cuda streams, rather than use this api. if math mode enables fast math modes when usingcublassgemmbatched(), pointers (not the pointer arrays) placed in the gpu memory must be properly aligned to avoid misaligned memory access errors. ideally all pointers are aligned to at least 16 bytes. otherwise it is recommended that they meet the following rule: the possible error values returned by this function and their meanings are listed below.",3
7.55.cuipcmemhandle_v1 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUipcMemHandle__v1.html#structCUipcMemHandle__v1,7.55.cuipcmemhandle_v1 struct reference,8
5.2.debian,#open-debian-installation,5.2.debian or to install a specific driver version,9
5.3.fedora,#open-fedora-installation,5.3.fedora or to install a specific driver version,9
5.4.kylinos 10,#kylinos10-installation,5.4.kylinos 10 or to install a specific driver version,9
5.4.1.arithmetic instructions,#arithmetic-instructions,"5.4.1.arithmetic instructions the following table gives the throughputs of the arithmetic instructions that are natively supported in hardware for devices of various compute capabilities. other instructions and functions are implemented on top of the native instructions. the implementation may be different for devices of different compute capabilities, and the number of native instructions after compilation may fluctuate with every compiler version. for complicated functions, there can be multiple code paths depending on input.cuobjdumpcan be used to inspect a particular implementation in acubinobject. the implementation of some functions are readily available on the cuda header files (math_functions.h,device_functions.h, ). in general, code compiled with-ftz=true(denormalized numbers are flushed to zero) tends to have higher performance than code compiled with-ftz=false. similarly, code compiled with-prec-div=false(less precise division) tends to have higher performance code than code compiled with-prec-div=true, and code compiled with-prec-sqrt=false(less precise square root) tends to have higher performance than code compiled with-prec-sqrt=true. the nvcc user manual describes these compilation flags in more details. single-precision floating-point division __fdividef(x,y)(seeintrinsic functions) provides faster single-precision floating-point division than the division operator. single-precision floating-point reciprocal square root to preserve ieee-754 semantics the compiler can optimize1.0/sqrtf()intorsqrtf()only when both reciprocal and square root are approximate, (i.e., with-prec-div=falseand-prec-sqrt=false). it is therefore recommended to invokersqrtf()directly where desired. single-precision floating-point square root single-precision floating-point square root is implemented as a reciprocal square root followed by a reciprocal instead of a reciprocal square root followed by a multiplication so that it gives correct results for 0 and infinity. sine and cosine sinf(x),cosf(x),tanf(x),sincosf(x), and corresponding double-precision instructions are much more expensive and even more so if the argument x is large in magnitude. more precisely, the argument reduction code (seemathematical functionsfor implementation) comprises two code paths referred to as the fast path and the slow path, respectively. the fast path is used for arguments sufficiently small in magnitude and essentially consists of a few multiply-add operations. the slow path is used for arguments large in magnitude and consists of lengthy computations required to achieve correct results over the entire argument range. at present, the argument reduction code for the trigonometric functions selects the fast path for arguments whose magnitude is less than105615.0ffor the single-precision functions, and less than2147483648.0for the double-precision functions. as the slow path requires more registers than the fast path, an attempt has been made to reduce register pressure in the slow path by storing some intermediate variables in local memory, which may affect performance because of local memory high latency and bandwidth (seedevice memory accesses). at present, 28 bytes of local memory are used by single-precision functions, and 44 bytes are used by double-precision functions. however, the exact amount is subject to change. due to the lengthy computations and use of local memory in the slow path, the throughput of these trigonometric functions is lower by one order of magnitude when the slow path reduction is required as opposed to the fast path reduction. integer arithmetic integer division and modulo operation are costly as they compile to up to 20 instructions. they can be replaced with bitwise operations in some cases: ifnis a power of 2, (i/n) is equivalent to(i>>log2(n))and(i%n)is equivalent to (i&(n-1)); the compiler will perform these conversions ifnis literal. __brevand__popcmap to a single instruction and__brevlland__popcllto a few instructions. __[u]mul24are legacy intrinsic functions that no longer have any reason to be used. half precision arithmetic in order to achieve good performance for 16-bit precision floating-point add, multiply or multiply-add, it is recommended that thehalf2datatype is used forhalfprecision and__nv_bfloat162be used for__nv_bfloat16precision. vector intrinsics (for example,__hadd2,__hsub2,__hmul2,__hfma2) can then be used to do two operations in a single instruction. usinghalf2or__nv_bfloat162in place of two calls usinghalfor__nv_bfloat16may also help performance of other intrinsics, such as warp shuffles. the intrinsic__halves2half2is provided to convert twohalfprecision values to thehalf2datatype. the intrinsic__halves2bfloat162is provided to convert two__nv_bfloatprecision values to the__nv_bfloat162datatype. type conversion sometimes, the compiler must insert conversion instructions, introducing additional execution cycles. this is the case for: this last case can be avoided by using single-precision floating-point constants, defined with anfsuffix such as3.141592653589793f,1.0f,0.5f.",6
5.4.2.control flow instructions,#control-flow-instructions,"5.4.2.control flow instructions any flow control instruction (if,switch,do,for,while) can significantly impact the effective instruction throughput by causing threads of the same warp to diverge (i.e., to follow different execution paths). if this happens, the different executions paths have to be serialized, increasing the total number of instructions executed for this warp. to obtain best performance in cases where the control flow depends on the thread id, the controlling condition should be written so as to minimize the number of divergent warps. this is possible because the distribution of the warps across the block is deterministic as mentioned insimt architecture. a trivial example is when the controlling condition only depends on (threadidx/warpsize) wherewarpsizeis the warp size. in this case, no warp diverges since the controlling condition is perfectly aligned with the warps. sometimes, the compiler may unroll loops or it may optimize out shortiforswitchblocks by using branch predication instead, as detailed below. in these cases, no warp can ever diverge. the programmer can also control loop unrolling using thepragmaunrolldirective (seepragma unroll). when using branch predication none of the instructions whose execution depends on the controlling condition gets skipped. instead, each of them is associated with a per-thread condition code or predicate that is set to true or false based on the controlling condition and although each of these instructions gets scheduled for execution, only the instructions with a true predicate are actually executed. instructions with a false predicate do not write results, and also do not evaluate addresses or read operands.",5
5.4.3.synchronization instruction,#synchronization-instruction,"5.4.3.synchronization instruction throughput for__syncthreads()is 32 operations per clock cycle for devices of compute capability 6.0, 16 operations per clock cycle for devices of compute capability 7.x as well as 8.x and 64 operations per clock cycle for devices of compute capability 5.x, 6.1 and 6.2. note that__syncthreads()can impact performance by forcing the multiprocessor to idle as detailed indevice memory accesses.",5
5.5.minimize memory thrashing,#minimize-memory-thrashing,"5.5.minimize memory thrashing applications that constantly allocate and free memory too often may find that the allocation calls tend to get slower over time up to a limit. this is typically expected due to the nature of releasing memory back to the operating system for its own use. for best performance in this regard, we recommend the following:",5
6.cuda-enabled gpus,#cuda-enabled-gpus,"6.cuda-enabled gpus https://developer.nvidia.com/cuda-gpuslists all cuda-enabled devices with their compute capability. the compute capability, number of multiprocessors, clock frequency, total amount of device memory, and other properties can be queried using the runtime (see reference manual).",0
7.c++ language extensions,#c-language-extensions,7.c++ language extensions,6
7.1.function execution space specifiers,#function-execution-space-specifiers,7.1.function execution space specifiers function execution space specifiers denote whether a function executes on the host or on the device and whether it is callable from the host or from the device.,6
7.1.1.__global__,#global,"7.1.1.__global__ the__global__execution space specifier declares a function as being a kernel. such a function is: a__global__function must have void return type, and cannot be a member of a class. any call to a__global__function must specify its execution configuration as described inexecution configuration. a call to a__global__function is asynchronous, meaning it returns before the device has completed its execution.",6
5.5.rhel 9 and rocky 9,#rhel-9-and-rocky-9,5.5.rhel 9 and rocky 9 or to install a specific driver version,9
5.6.rhel 8 and rocky 8,#rhel-8-and-rocky-8,5.6.rhel 8 and rocky 8 or to install a specific driver version,9
5.7.opensuse and sles,#opensuse-and-sles,5.7.opensuse and sles or to install a specific driver version,9
5.8.ubuntu,#open-ubuntu-installation,5.8.ubuntu or to install a specific driver version,9
6.precompiled streams,#precompiled-streams,"6.precompiled streams precompiled streams offer an optional method of streamlining the installation process. the advantages of precompiled streams: when using precompiled drivers, a plugin for the dnf package manager is enabled that cleans up stale .ko files. to prevent system breakages, the nvidia dnf plugin also prevents upgrading to a kernel for which no precompiled driver yet exists. this can delay the application of security fixes but ensures that a tested kernel and driver combination is always used. a warning is displayed bydnfduring that upgrade situation: packaging templates and instructions are provided on github to allow you to maintain your own precompiled kernel module packages for custom kernels and derivative linux distros:nvidia/yum-packaging-precompiled-kmod to use the new driver packages on rhel 8 or rhel 9:",0
6.1.precompiled streams support matrix,#precompiled-streams-support-matrix,"6.1.precompiled streams support matrix this table shows the supported precompiled and legacy dkms streams for each driver. prior to switching between module streams, first reset: or alternatively:",6
4.graph object thread safety,https://docs.nvidia.com/cuda/cuda-driver-api/graphs-thread-safety.html,4.graph object thread safety,5
7.7.cuda_array_descriptor_v2 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__ARRAY__DESCRIPTOR__v2.html#structCUDA__ARRAY__DESCRIPTOR__v2,7.7.cuda_array_descriptor_v2 struct reference,8
6.2.modularity profiles,#modularity-profiles,"6.2.modularity profiles modularity profiles work with any supported modularity stream and allow for additional use cases. these modularity profiles are available on rhel8+ and fedora. for example: you can install multiple modularity profiles using bash curly brace expansion, for example: seehttps://developer.nvidia.com/blog/streamlining-nvidia-driver-deployment-on-rhel-8-with-modularity-streamsin the developer blog andhttps://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/precompiled/for more information.",0
7.kickstart installation,#kickstart-installation,7.kickstart installation,9
7.1.rhel 8 / rocky linux 8,#rhel-8-rocky-linux-8,7.1.rhel 8 / rocky linux 8,9
7.2.rhel 9 / rocky linux 9,#rhel-9-rocky-linux-9,7.2.rhel 9 / rocky linux 9,9
8.runfile installation,#runfile-installation,8.runfile installation basic instructions can be found in thequick start guide. read on for more detailed instructions. this section describes the installation and configuration of cuda when using the standalone installer. the standalone installer is a .run file and is completely self-contained.,0
2.7.4.cublas<t>gemmstridedbatched(),#cublas-t-gemmstridedbatched,"2.7.4.cublas<t>gemmstridedbatched() this function supports the64-bit integer interface. this function performs the matrix-matrix multiplication of a batch of matrices. the batch is considered to be uniform, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective a, b and c matrices. input matrices a, b and output matrix c for each instance of the batch are located at fixed offsets in number of elements from their locations in the previous instance. pointers to a, b and c matrices for the first instance are passed to the function by the user along with offsets in number of elements - stridea, strideb and stridec that determine the locations of input and output matrices in future instances. \(c + i*{stridec} = \alpha\text{op}(a + i*{stridea})\text{op}(b + i*{strideb}) + \beta(c + i*{stridec}),\text{ for i } \in \lbrack 0,batchcount - 1\rbrack\) where\(\alpha\)and\(\beta\)are scalars, and\(a\),\(b\)and\(c\)are arrays of pointers to matrices stored in column-major format with dimensions\(\text{op}(a\lbrack i\rbrack)\)\(m \times k\),\(\text{op}(b\lbrack i\rbrack)\)\(k \times n\)and\(c\lbrack i\rbrack\)\(m \times n\), respectively. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) and\(\text{op}(b\lbrack i\rbrack)\)is defined similarly for matrix\(b\lbrack i\rbrack\). on certain problem sizes, it might be advantageous to make multiple calls tocublas<t>gemmin different cuda streams, rather than use this api. the possible error values returned by this function and their meanings are listed below.",3
2.7.5.cublas<t>gemmgroupedbatched(),#cublas-t-gemmgroupedbatched,"2.7.5.cublas<t>gemmgroupedbatched() this function supports the64-bit integer interface. this function performs the matrix-matrix multiplication on groups of matrices. a given group is considered to be uniform, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective a, b and c matrices. however, the dimensions, leading dimensions, transpositions, and scaling factors (alpha, beta) may vary between groups. the address of the input matrices and the output matrix of each instance of the batch are read from arrays of pointers passed to the function by the caller.  this is functionally equivalent to the following: where\(\text{$\mathrm{alpha\_array}$}\)and\(\text{$\mathrm{beta\_array}$}\)are arrays of scaling factors, and\(\text{aarray}\),\(\text{barray}\)and\(\text{carray}\)are arrays of pointers to matrices stored in column-major format.  for a given index,\(\text{idx}\), that is part of group\(i\), the dimensions are: for matrix\(a[\text{idx}]\)in group\(i\) \(\text{op}(a[\text{idx}]) = \left\{ \begin{matrix}
a[\text{idx}] & {\text{if }\textsf{$\mathrm{transa\_array}\lbrack i\rbrack$ == $\mathrm{cublas\_op\_n}$}} \\
a[\text{idx}]^{t} & {\text{if }\textsf{$\mathrm{transa\_array}\lbrack i\rbrack$ == $\mathrm{cublas\_op\_t}$}} \\
a[\text{idx}]^{h} & {\text{if }\textsf{$\mathrm{transa\_array}\lbrack i\rbrack$ == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) and\(\text{op}(b[\text{idx}])\)is defined similarly for matrix\(b[\text{idx}]\)in group\(i\). on certain problem sizes, it might be advantageous to make multiple calls tocublas<t>gemmbatchedin different cuda streams, rather than use this api. if math mode enables fast math modes when usingcublassgemmgroupedbatched(), pointers (not the pointer arrays) placed in the gpu memory must be properly aligned to avoid misaligned memory access errors. ideally all pointers are aligned to at least 16 bytes. otherwise it is required that they meet the following rule: the possible error values returned by this function and their meanings are listed below.",3
8.1.runfile overview,#runfile-overview,"8.1.runfile overview the runfile installation installs the nvidia driver and cuda toolkit via an interactive ncurses-based interface. theinstallation stepsare listed below. distribution-specific instructions ondisabling the nouveau driversas well as steps forverifying device node creationare also provided. finally,advanced optionsfor the installer anduninstallation stepsare detailed below. the runfile installation does not include support for cross-platform development. for cross-platform development, see thecuda cross-platform environmentsection.",0
6.3.initialization,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__INITIALIZE.html#group__CUDA__INITIALIZE,6.3.initialization,9
1.nvidia ada gpu architecture compatibility,#nvidia-ada-gpu-architecture-compatibility,1.nvidia ada gpu architecture compatibility,0
1.1.about this document,#about-this-document,"1.1.about this document this application note,nvidia ada gpu architecture compatibility guide for cuda applications, is intended to help developers ensure that their nvidiacudaapplications will run on the nvidiaada architecture based gpus. this document provides guidance to developers who are familiar with programming in cuda c++ and want to make sure that their software applications are compatible with the nvidia ada gpu architecture.",0
1.2.application compatibility on the nvidia ada gpu architecture,#application-compatibility-on-the-nvidia-ada-gpu-architecture,"1.2.application compatibility on the nvidia ada gpu architecture a cuda application binary (with one or more gpu kernels) can contain the compiled gpu code in two forms, binarycubinobjects and forward-compatibleptxassembly for each kernel. both cubin and ptx are generated for a certain target compute capability. a cubin generated for a certain compute capability is supported to run on any gpu with the same major revision and same or higher minor revision of compute capability. for example, a cubin generated for compute capability 8.6 is supported to run on a gpu with compute capability 8.9; however, a cubin generated for compute capability 8.9 isnotsupported to run on a gpu with compute capability 8.6, and a cubin generated with compute capability 8.x isnotsupported to run on a gpu with compute capability 9.0. kernels can also be compiled to a ptx form. at the application load time, ptx is compiled to cubin and the cubin is used for kernel execution. unlike cubin, ptx is forward-compatible. meaning ptx is supported to run on any gpu with compute capability higher than the compute capability assumed for generation of that ptx. for example, ptx code generated for compute capability 8.x is supported to run on compute capability 8.x or any higher revision (major or minor), including compute capability 9.x. therefore, although it is optional,it is recommended that all applications should include ptx of the kernels to ensure forward-compatibility.to read more about cubin and ptx compatibilities seecompilation with nvccfrom thecuda c++ programming guide. when a cuda application launches a kernel on a gpu, the cuda runtime determines the compute capability of the gpu in the system and uses this information to find the best matching cubin or ptx version of the kernel. if a cubin compatible with that gpu is present in the binary, the cubin is used as-is for execution. otherwise, the cuda runtime first generates compatible cubin by jit-compiling1the ptx and then the cubin is used for the execution. if neither compatible cubin nor ptx is available, kernel launch results in a failure. application binaries that include ptx version of kernels should work as-is on the nvidia ada architecture based gpus. in such cases, rebuilding the application is not required. however, application binaries that do not include ptx (only include cubins) need to be rebuilt to run on the nvidia ada architecture based gpus. to know more about building compatible applications, readbuilding applications with the nvidia ada gpu architecture support.",0
1.3.compatibility between ampere and ada,#compatibility-between-ampere-and-ada,"1.3.compatibility between ampere and ada the nvidia ada architecture is based on amperes instruction set architectureisa8.0, extending it with new instructions. as a consequence, any binary that runs on ampere will be able to run on ada (forward compatibility), but an ada binary will not be able to run on ampere.",6
1.4.verifying ada compatibility for existing applications,#verifying-ada-compatibility-for-existing-applications,1.4.verifying ada compatibility for existing applications the first step towards making a cuda application compatible with the nvidia ada gpu architecture is to check if the application binary already contains compatible gpu code (at least the ptx). the following sections explain how to accomplish this for an already built cuda application.,0
1.4.1.applications built using cuda toolkit 10.2 or earlier,#applications-built-using-cuda-toolkit-10-2-or-earlier,"1.4.1.applications built using cuda toolkit 10.2 or earlier cuda applications built using cuda toolkit versions 2.1 through 10.2 are compatible with nvidia ada architecture based gpus as long as they are built to include ptx versions of their kernels. this can be tested by forcing the ptx to jit-compile at application load time with following the steps: withcuda_force_ptx_jit=1, gpu binary code embedded in an application binary is ignored. instead ptx code for each kernel is jit-compiled to produce gpu binary code. an application fails to execute if it does not include ptx. this means the application is not compatible with the nvidia ada gpu architecture and needs to be rebuilt for compatibility. on the other hand, if the application works properly with this environment variable set, then the application is compatible with the nvidia ada gpu architecture.",0
1.4.2.applications built using cuda toolkit 11.0 through 11.7,#applications-built-using-cuda-toolkit-11-0-through-11-7,"1.4.2.applications built using cuda toolkit 11.0 through 11.7 cuda applications built using cuda toolkit 11.0 through 11.7 are compatible with the nvidia ada gpu architecture as long as they are built to include kernels in ampere-native cubin (seecompatibility between ampere and ada) or ptx format (seeapplications built using cuda toolkit 10.2 or earlier), or both.",0
1.4.3.applications built using cuda toolkit 11.8,#applications-built-using-cuda-toolkit-11-8,"1.4.3.applications built using cuda toolkit 11.8 cuda applications built using cuda toolkit 11.8 are compatible with the nvidia ada gpu architecture as long as they are built to include kernels in ampere-native or ada-native cubin (seecompatibility between ampere and ada), or ptx format (seeapplications built using cuda toolkit 10.2 or earlier), or both.",0
1.5.building applications with the nvidia ada gpu architecture support,#building-applications-with-the-nvidia-ada-gpu-architecture-support,"1.5.building applications with the nvidia ada gpu architecture support depending on the version of the cuda toolkit used for building the application, it can be built to include ptx and/or native cubin for the nvidia ada gpu architecture. although it is sufficient to just include ptx, including native cubin also has the following advantages:",0
1.5.1.building applications using cuda toolkit 10.x or earlier,#building-applications-using-cuda-toolkit-10-x-or-earlier,"1.5.1.building applications using cuda toolkit 10.x or earlier thenvcccompiler included with versions 10.x (10.0, 10.1 and 10.2) of the cuda toolkit can generate cubins native to the volta and turing architectures (compute capability 7.x). when using cuda toolkit 10.x, to ensure thatnvccwill generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate-gencode=parameters on thenvcccommand line as shown in the examples below. windows mac/linux alternatively, the simplifiednvcccommand-line option-arch=sm_xxcan be used. it is a shorthand equivalent to the following more explicit-gencode=command-line options used above.-arch=sm_xxexpands to the following: however, while the-arch=sm_xxcommand-line option does result in inclusion of a ptx back-end target binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple-arch=options on the samenvcccommand line, which is why the examples above use-gencode=explicitly. for cuda toolkits prior to 10.0, one or more of the-gencodeoptions will need to be removed according to the architectures supported by the specific toolkit version (for example, cuda toolkit 9.x supports architectures up to _60 and _61). the final-gencodeto generate ptx would also need to be updated. for further information and examples, see the documentation for the specific cuda toolkit version.",0
1.5.2.building applications using cuda toolkit 11.0 through 11.7,#building-applications-using-cuda-toolkit-11-0-through-11-7,"1.5.2.building applications using cuda toolkit 11.0 through 11.7 thenvcccompiler included with versions 11.0 through 11.7 of the cuda toolkit can generate cubins native to the ampere architecture (compute capability 8.0 and 8.6). when using cuda toolkit 11.0 through 11.7, to ensure thatnvccwill generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate-gencode=parameters on thenvcccommand line as shown in the examples below. windows mac/linux alternatively, the simplifiednvcccommand-line option-arch=sm_xxcan be used. it is a shorthand equivalent to the following more explicit-gencode=command-line options used above.-arch=sm_xxexpands to the following: however, while the-arch=sm_xxcommand-line option does result in inclusion of a ptx back-end target binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple-arch=options on the samenvcccommand line, which is why the examples above use-gencode=explicitly. for cuda toolkits prior to 11.0, one or more of the-gencodeoptions need to be removed according to the architectures supported by the specific toolkit version (for example, cuda toolkit 10.x supports architectures up to _72 and _75). the final-gencodeto generate ptx also needs to be updated. for further information and examples, see the documentation for the specific cuda toolkit version.",0
1.5.3.building applications using cuda toolkit 11.8,#building-applications-using-cuda-toolkit-11-8,"1.5.3.building applications using cuda toolkit 11.8 with version 11.8 of the cuda toolkit,nvcccan generate cubin native to the nvidia ada gpu architecture (compute capability 8.9). when using cuda toolkit 11.8, to ensure thatnvccwill generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate-gencode=parameters on thenvcccommand line as shown in the examples below. windows mac/linux",0
7.1.2.__device__,#device,7.1.2.__device__ the__device__execution space specifier declares a function that is: the__global__and__device__execution space specifiers cannot be used together.,6
7.35.cuda_memcpy2d_v2 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__MEMCPY2D__v2.html#structCUDA__MEMCPY2D__v2,7.35.cuda_memcpy2d_v2 struct reference,8
6.35.data types used by cuda runtime,https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1gaf8b3ba752727d996074a71ee997ce68,6.35.data types used by cuda runtime,0
2.7.6.cublas<t>symm(),#cublas-t-symm,"2.7.6.cublas<t>symm() this function supports the64-bit integer interface. this function performs the symmetric matrix-matrix multiplication \(c = \left\{ \begin{matrix}
{\alpha ab + \beta c} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_left}$}} \\
{\alpha ba + \beta c} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_right}$}} \\
\end{matrix} \right.\) where\(a\)is a symmetric matrix stored in lower or upper mode,\(b\)and\(c\)are\(m \times n\)matrices, and\(\alpha\)and\(\beta\)are scalars. the possible error values returned by this function and their meanings are listed below. for references please refer to: ssymm,dsymm,csymm,zsymm",3
2.7.7.cublas<t>syrk(),#cublas-t-syrk,"2.7.7.cublas<t>syrk() this function supports the64-bit integer interface. this function performs the symmetric rank-\(k\)update \(c = \alpha\text{op}(a)\text{op}(a)^{t} + \beta c\) where\(\alpha\)and\(\beta\)are scalars,\(c\)is a symmetric matrix stored in lower or upper mode, and\(a\)is a matrix with dimensions\(\text{op}(a)\)\(n \times k\). also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
\end{matrix} \right.\) the possible error values returned by this function and their meanings are listed below. for references please refer to: ssyrk,dsyrk,csyrk,zsyrk",3
2.7.8.cublas<t>syr2k(),#cublas-t-syr2k,"2.7.8.cublas<t>syr2k() this function supports the64-bit integer interface. this function performs the symmetric rank-\(2k\)update \(c = \alpha(\text{op}(a)\text{op}(b)^{t} + \text{op}(b)\text{op}(a)^{t}) + \beta c\) where\(\alpha\)and\(\beta\)are scalars,\(c\)is a symmetric matrix stored in lower or upper mode, and\(a\)and\(b\)are matrices with dimensions\(\text{op}(a)\)\(n \times k\)and\(\text{op}(b)\)\(n \times k\), respectively. also, for matrix\(a\)and\(b\) \(\text{op(}a\text{) and op(}b\text{)} = \left\{ \begin{matrix}
{a\text{ and }b} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_n}$}} \\
{a^{t}\text{ and }b^{t}} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_t}$}} \\
\end{matrix} \right.\) the possible error values returned by this function and their meanings are listed below. for references please refer to: ssyr2k,dsyr2k,csyr2k,zsyr2k",3
7.1.3.__host__,#host,"7.1.3.__host__ the__host__execution space specifier declares a function that is: it is equivalent to declare a function with only the__host__execution space specifier or to declare it without any of the__host__,__device__, or__global__execution space specifier; in either case the function is compiled for the host only. the__global__and__host__execution space specifiers cannot be used together. the__device__and__host__execution space specifiers can be used together however, in which case the function is compiled for both the host and the device. the__cuda_arch__macro introduced inapplication compatibilitycan be used to differentiate code paths between host and device:",6
7.1.4.undefined behavior,#undefined-behavior,7.1.4.undefined behavior a cross-execution space call has undefined behavior when:,6
7.1.5.__noinline__ and __forceinline__,#noinline-and-forceinline,"7.1.5.__noinline__ and __forceinline__ the compiler inlines any__device__function when deemed appropriate. the__noinline__function qualifier can be used as a hint for the compiler not to inline the function if possible. the__forceinline__function qualifier can be used to force the compiler to inline the function. the__noinline__and__forceinline__function qualifiers cannot be used together, and neither function qualifier can be applied to an inline function.",6
7.1.6.__inline_hint__,#inline-hint,"7.1.6.__inline_hint__ the__inline_hint__qualifier enables more aggressive inlining in the compiler. unlike__forceinline__, it does not imply that the function is inline. it can be used to improve inlining across modules when using lto. neither the__noinline__nor the__forceinline__function qualifier can be used with the__inline_hint__function qualifier.",6
7.41.cudalaunchconfig_t struct reference,https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaLaunchConfig__t.html#structcudaLaunchConfig__t,7.41.cudalaunchconfig_t struct reference,8
[data types used by cuda runtime],group__CUDART__TYPES.html,[data types used by cuda runtime],0
7.30.cudagraphinstantiateparams struct reference,https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaGraphInstantiateParams.html#structcudaGraphInstantiateParams,7.30.cudagraphinstantiateparams struct reference,8
7.2.variable memory space specifiers,#variable-memory-space-specifiers,"7.2.variable memory space specifiers variable memory space specifiers denote the memory location on the device of a variable. an automatic variable declared in device code without any of the__device__,__shared__and__constant__memory space specifiers described in this section generally resides in a register. however in some cases the compiler might choose to place it in local memory, which can have adverse performance consequences as detailed indevice memory accesses.",6
7.2.1.__device__,#device-variable-specifier,"7.2.1.__device__ the__device__memory space specifier declares a variable that resides on the device. at most one of the other memory space specifiers defined in the next three sections may be used together with__device__to further denote which memory space the variable belongs to. if none of them is present, the variable:",6
7.2.2.__constant__,#constant,"7.2.2.__constant__ the__constant__memory space specifier, optionally used together with__device__, declares a variable that: the behavior of modifying a constant from the host while there is a concurrent grid that access that constant at any point of this grids lifetime is undefined.",6
7.2.3.__shared__,#shared,"7.2.3.__shared__ the__shared__memory space specifier, optionally used together with__device__, declares a variable that: when declaring a variable in shared memory as an external array such as the size of the array is determined at launch time (seeexecution configuration). all variables declared in this fashion, start at the same address in memory, so that the layout of the variables in the array must be explicitly managed through offsets. for example, if one wants the equivalent of in dynamically allocated shared memory, one could declare and initialize the arrays the following way: note that pointers need to be aligned to the type they point to, so the following code, for example, does not work since array1 is not aligned to 4 bytes. alignment requirements for the built-in vector types are listed intable 5.",6
7.2.4.__grid_constant__,#grid-constant,"7.2.4.__grid_constant__ the__grid_constant__annotation for compute architectures greater or equal to 7.0 annotates aconst-qualified__global__function parameter of non-reference type that: requirements: if the address of a__global__function parameter is taken, the compiler will ordinarily make a copy of the kernel parameter in thread local memory and use the address of the copy, to partially support c++ semantics, which allow each thread to modify its own local copy of function parameters. annotating a__global__function parameter with__grid_constant__ensures that the compiler will not create a copy of the kernel parameter in thread local memory, but will instead use the generic address of the parameter itself. avoiding the local copy may result in improved performance.",6
7.2.5.__managed__,#managed,"7.2.5.__managed__ the__managed__memory space specifier, optionally used together with__device__, declares a variable that: see__managed__ memory space specifierfor more details.",6
7.2.6.__restrict__,#restrict,"7.2.6.__restrict__ nvccsupports restricted pointers via the__restrict__keyword. restricted pointers were introduced in c99 to alleviate the aliasing problem that exists in c-type languages, and which inhibits all kind of optimization from code re-ordering to common sub-expression elimination. here is an example subject to the aliasing issue, where use of restricted pointer can help the compiler to reduce the number of instructions: in c-type languages, the pointersa,b, andcmay be aliased, so any write throughccould modify elements ofaorb. this means that to guarantee functional correctness, the compiler cannot loada[0]andb[0]into registers, multiply them, and store the result to bothc[0]andc[1], because the results would differ from the abstract execution model if, say,a[0]is really the same location asc[0]. so the compiler cannot take advantage of the common sub-expression. likewise, the compiler cannot just reorder the computation ofc[4]into the proximity of the computation ofc[0]andc[1]because the preceding write toc[3]could change the inputs to the computation ofc[4]. by makinga,b, andcrestricted pointers, the programmer asserts to the compiler that the pointers are in fact not aliased, which in this case means writes throughcwould never overwrite elements ofaorb. this changes the function prototype as follows: note that all pointer arguments need to be made restricted for the compiler optimizer to derive any benefit. with the__restrict__keywords added, the compiler can now reorder and do common sub-expression elimination at will, while retaining functionality identical with the abstract execution model: the effects here are a reduced number of memory accesses and reduced number of computations. this is balanced by an increase in register pressure due to cached loads and common sub-expressions. since register pressure is a critical issue in many cuda codes, use of restricted pointers can have negative performance impact on cuda code, due to reduced occupancy.",6
7.3.built-in vector types,#built-in-vector-types,7.3.built-in vector types,6
6.24.vdpau interoperability,https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__VDPAU.html#group__CUDART__VDPAU,6.24.vdpau interoperability,9
"7.3.1.char, short, int, long, longlong, float, double",#char-short-int-long-longlong-float-double,"7.3.1.char, short, int, long, longlong, float, double these are vector types derived from the basic integer and floating-point types. they are structures and the 1st, 2nd, 3rd, and 4th components are accessible through the fieldsx,y,z, andw, respectively. they all come with a constructor function of the formmake_<typename>; for example, which creates a vector of typeint2with value(x,y). the alignment requirements of the vector types are detailed in thefollowing table.",6
7.3.2.dim3,#dim3,"7.3.2.dim3 this type is an integer vector type based onuint3that is used to specify dimensions. when defining a variable of typedim3, any component left unspecified is initialized to 1.",6
7.4.built-in variables,#built-in-variables,7.4.built-in variables built-in variables specify the grid and block dimensions and the block and thread indices. they are only valid within functions that are executed on the device.,6
7.4.1.griddim,#griddim,7.4.1.griddim this variable is of typedim3(seedim3) and contains the dimensions of the grid.,6
7.4.2.blockidx,#blockidx,"7.4.2.blockidx this variable is of typeuint3(seechar, short, int, long, longlong, float, double) and contains the block index within the grid.",6
7.4.3.blockdim,#blockdim,7.4.3.blockdim this variable is of typedim3(seedim3) and contains the dimensions of the block.,6
6.5.stream management,https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html#group__CUDART__STREAM_1gfda584f1788ca983cb21c5f4d2033a62,6.5.stream management,9
2.7.9.cublas<t>syrkx(),#cublas-t-syrkx,"2.7.9.cublas<t>syrkx() this function supports the64-bit integer interface. this function performs a variation of the symmetric rank-\(k\)update \(c = \alpha\text{op}(a)\text{op}(b)^{t} + \beta c\) where\(\alpha\)and\(\beta\)are scalars,\(c\)is a symmetric matrix stored in lower or upper mode, and\(a\)and\(b\)are matrices with dimensions\(\text{op}(a)\)\(n \times k\)and\(\text{op}(b)\)\(n \times k\), respectively. also, for matrices\(a\)and\(b\) \(\text{op(}a\text{) and op(}b\text{)} = \left\{ \begin{matrix}
{a\text{ and }b} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_n}$}} \\
{a^{t}\text{ and }b^{t}} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_t}$}} \\
\end{matrix} \right.\) this routine can be used when b is in such way that the result is guaranteed to be symmetric. a usual example is when the matrix b is a scaled form of the matrix a: this is equivalent to b being the product of the matrix a and a diagonal matrix. for an efficient computation of the product of a regular matrix with a diagonal matrix, refer to the routinecublas<t>dgmm. the possible error values returned by this function and their meanings are listed below. for references please refer to: ssyrk,dsyrk,csyrk,zsyrkand ssyr2k,dsyr2k,csyr2k,zsyr2k",3
1.5.4.independent thread scheduling compatibility,#independent-thread-scheduling-compatibility,"1.5.4.independent thread scheduling compatibility nvidia gpus since volta architecture haveindependent thread schedulingamong threads in a warp. if the developer made assumptions about warp-synchronicity2, this feature can alter the set of threads participating in the executed code compared to previous architectures. please seecompute capability 7.xin thecuda c++ programming guidefor details and corrective actions. to aid migration to the nvidia ada gpu architecture, developers can opt-in to the pascal scheduling model with the following combination of compiler options.",5
7.45.cudamemallocnodeparams struct reference,https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaMemAllocNodeParams.html#structcudaMemAllocNodeParams,7.45.cudamemallocnodeparams struct reference,8
2.7.10.cublas<t>trmm(),#cublas-t-trmm,"2.7.10.cublas<t>trmm() this function supports the64-bit integer interface. this function performs the triangular matrix-matrix multiplication \(c = \left\{ \begin{matrix}
{\alpha\text{op}(a)b} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_left}$}} \\
{\alpha b\text{op}(a)} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_right}$}} \\
\end{matrix} \right.\) where\(a\)is a triangular matrix stored in lower or upper mode with or without the main diagonal,\(b\)and\(c\)are\(m \times n\)matrix, and\(\alpha\)is a scalar. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) notice that in order to achieve better parallelism cublas differs from the blas api only for this routine. the blas api assumes an in-place implementation (with results written back to b), while the cublas api assumes an out-of-place implementation (with results written into c). the application can obtain the in-place functionality of blas in the cublas api by passing the address of the matrix b in place of the matrix c. no other overlapping in the input parameters is supported. the possible error values returned by this function and their meanings are listed below. for references please refer to: strmm,dtrmm,ctrmm,ztrmm",3
6.2.error handling,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__ERROR.html#group__CUDA__ERROR,6.2.error handling,9
6.39.1.direct3d 9 interoperability [deprecated],https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__D3D9__DEPRECATED.html#group__CUDA__D3D9__DEPRECATED,6.39.1.direct3d 9 interoperability [deprecated],9
7.4.4.threadidx,#threadidx,"7.4.4.threadidx this variable is of typeuint3(seechar, short, int, long, longlong, float, double) and contains the thread index within the block.",6
7.4.5.warpsize,#warpsize,7.4.5.warpsize this variable is of typeintand contains the warp size in threads (seesimt architecturefor the definition of a warp).,6
7.5.memory fence functions,#memory-fence-functions,"7.5.memory fence functions the cuda programming model assumes a device with a weakly-ordered memory model, that is the order in which a cuda thread writes data to shared memory, global memory, page-locked host memory, or the memory of a peer device is not necessarily the order in which the data is observed being written by another cuda or host thread. it is undefined behavior for two threads to read from or write to the same memory location without synchronization. in the following example, thread 1 executeswritexy(), while thread 2 executesreadxy(). the two threads read and write from the same memory locationsxandysimultaneously. any data-race is undefined behavior, and has no defined semantics. the resulting values foraandbcan be anything. memory fence functions can be used to enforce asequentially-consistentordering on memory accesses. the memory fence functions differ in thescopein which the orderings are enforced but they are independent of the accessed memory space (shared memory, global memory, page-locked host memory, and the memory of a peer device). is equivalent tocuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope_block)and ensures that: is equivalent tocuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope_device)and ensures that no writes to all memory made by the calling thread after the call to__threadfence()are observed by any thread in the device as occurring before any write to all memory made by the calling thread before the call to__threadfence(). is equivalent tocuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope_system)and ensures that all writes to all memory made by the calling thread before the call to__threadfence_system()are observed by all threads in the device, host threads, and all threads in peer devices as occurring before all writes to all memory made by the calling thread after the call to__threadfence_system(). __threadfence_system()is only supported by devices of compute capability 2.x and higher. in the previous code sample, we can insert fences in the codes as follows: for this code, the following outcomes can be observed: the fourth outcome is not possible, because the first write must be visible before the second write. if thread 1 and 2 belong to the same block, it is enough to use__threadfence_block(). if thread 1 and 2 do not belong to the same block,__threadfence()must be used if they are cuda threads from the same device and__threadfence_system()must be used if they are cuda threads from two different devices. a common use case is when threads consume some data produced by other threads as illustrated by the following code sample of a kernel that computes the sum of an array of n numbers in one call. each block first sums a subset of the array and stores the result in global memory. when all blocks are done, the last block done reads each of these partial sums from global memory and sums them to obtain the final result. in order to determine which block is finished last, each block atomically increments a counter to signal that it is done with computing and storing its partial sum (seeatomic functionsabout atomic functions). the last block is the one that receives the counter value equal togriddim.x-1. if no fence is placed between storing the partial sum and incrementing the counter, the counter might increment before the partial sum is stored and therefore, might reachgriddim.x-1and let the last block start reading partial sums before they have been actually updated in memory. memory fence functions only affect the ordering of memory operations by a thread; they do not, by themselves, ensure that these memory operations are visible to other threads (like__syncthreads()does for threads within a block (seesynchronization functions)). in the code sample below, the visibility of memory operations on theresultvariable is ensured by declaring it as volatile (seevolatile qualifier).",5
7.6.synchronization functions,#synchronization-functions,"7.6.synchronization functions waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to__syncthreads()are visible to all threads in the block. __syncthreads()is used to coordinate communication between the threads of the same block. when some threads within a block access the same addresses in shared or global memory, there are potential read-after-write, write-after-read, or write-after-write hazards for some of these memory accesses. these data hazards can be avoided by synchronizing threads in-between these accesses. __syncthreads()is allowed in conditional code but only if the conditional evaluates identically across the entire thread block, otherwise the code execution is likely to hang or produce unintended side effects. devices of compute capability 2.x and higher support three variations of__syncthreads()described below. is identical to__syncthreads()with the additional feature that it evaluates predicate for all threads of the block and returns the number of threads for which predicate evaluates to non-zero. is identical to__syncthreads()with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for all of them. is identical to__syncthreads()with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for any of them. will cause the executing thread to wait until all warp lanes named in mask have executed a__syncwarp()(with the same mask) before resuming execution. each calling thread must have its own bit set in the mask and all non-exited threads named in mask must execute a corresponding__syncwarp()with the same mask, or the result is undefined. executing__syncwarp()guarantees memory ordering among threads participating in the barrier. thus, threads within a warp that wish to communicate via memory can store to memory, execute__syncwarp(), and then safely read values stored by other threads in the warp.",5
7.7.mathematical functions,#mathematical-functions,7.7.mathematical functions the reference manual lists all c/c++ standard library mathematical functions that are supported in device code and all intrinsic functions that are only supported in device code. mathematical functionsprovides accuracy information for some of these functions when relevant.,6
7.8.texture functions,#texture-functions,7.8.texture functions texture objects are described intexture object api texture fetching is described intexture fetching.,6
7.8.1.texture object api,#texture-object-api-appendix,7.8.1.texture object api,9
7.9.surface functions,#surface-functions,"7.9.surface functions surface functions are only supported by devices of compute capability 2.0 and higher. surface objects are described in described insurface object api in the sections below,boundarymodespecifies the boundary mode, that is how out-of-range surface coordinates are handled; it is equal to eithercudaboundarymodeclamp, in which case out-of-range coordinates are clamped to the valid range, orcudaboundarymodezero, in which case out-of-range reads return zero and out-of-range writes are ignored, orcudaboundarymodetrap, in which case out-of-range accesses cause the kernel execution to fail.",6
7.9.1.surface object api,#surface-object-api-appendix,7.9.1.surface object api,9
8.2.installation,#installation,8.2.installation,9
7.8.cuda_array_memory_requirements_v1 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__ARRAY__MEMORY__REQUIREMENTS__v1.html,7.8.cuda_array_memory_requirements_v1 struct reference,8
7.10.read-only data cache load function,#read-only-data-cache-load-function,"7.10.read-only data cache load function the read-only data cache load function is only supported by devices of compute capability 5.0 and higher. returns the data of typetlocated at addressaddress, wheretischar,signedchar,short,int,long,longlongunsignedchar,unsignedshort,unsignedint,unsignedlong,unsignedlonglong,char2,char4,short2,short4,int2,int4,longlong2uchar2,uchar4,ushort2,ushort4,uint2,uint4,ulonglong2float,float2,float4,double, ordouble2. with thecuda_fp16.hheader included,tcan be__halfor__half2. similarly, with thecuda_bf16.hheader included,tcan also be__nv_bfloat16or__nv_bfloat162. the operation is cached in the read-only data cache (seeglobal memory).",6
7.11.load functions using cache hints,#load-functions-using-cache-hints,"7.11.load functions using cache hints these load functions are only supported by devices of compute capability 5.0 and higher. returns the data of typetlocated at addressaddress, wheretischar,signedchar,short,int,long,longlongunsignedchar,unsignedshort,unsignedint,unsignedlong,unsignedlonglong,char2,char4,short2,short4,int2,int4,longlong2uchar2,uchar4,ushort2,ushort4,uint2,uint4,ulonglong2float,float2,float4,double, ordouble2. with thecuda_fp16.hheader included,tcan be__halfor__half2. similarly, with thecuda_bf16.hheader included,tcan also be__nv_bfloat16or__nv_bfloat162. the operation is using the corresponding cache operator (seeptx isa)",6
7.12.store functions using cache hints,#store-functions-using-cache-hints,"7.12.store functions using cache hints these store functions are only supported by devices of compute capability 5.0 and higher. stores thevalueargument of typetto the location at addressaddress, wheretischar,signedchar,short,int,long,longlongunsignedchar,unsignedshort,unsignedint,unsignedlong,unsignedlonglong,char2,char4,short2,short4,int2,int4,longlong2uchar2,uchar4,ushort2,ushort4,uint2,uint4,ulonglong2float,float2,float4,double, ordouble2. with thecuda_fp16.hheader included,tcan be__halfor__half2. similarly, with thecuda_bf16.hheader included,tcan also be__nv_bfloat16or__nv_bfloat162. the operation is using the corresponding cache operator (seeptx isa)",6
7.13.time function,#time-function,"7.13.time function when executed in device code, returns the value of a per-multiprocessor counter that is incremented every clock cycle. sampling this counter at the beginning and at the end of a kernel, taking the difference of the two samples, and recording the result per thread provides a measure for each thread of the number of clock cycles taken by the device to completely execute the thread, but not of the number of clock cycles the device actually spent executing thread instructions. the former number is greater than the latter since threads are time sliced.",5
7.14.atomic functions,#atomic-functions,"7.14.atomic functions an atomic function performs a read-modify-write atomic operation on one 32-bit, 64-bit, or 128-bit word residing in global or shared memory. in the case offloat2orfloat4, the read-modify-write operation is performed on each element of the vector residing in global memory. for example,atomicadd()reads a word at some address in global or shared memory, adds a number to it, and writes the result back to the same address. atomic functions can only be used in device functions. the atomic functions described in this section have orderingcuda::memory_order_relaxedand are only atomic at a particularscope: in the following example both the cpu and the gpu atomically update an integer value at addressaddr: note that any atomic operation can be implemented based onatomiccas()(compare and swap). for example,atomicadd()for double-precision floating-point numbers is not available on devices with compute capability lower than 6.0 but it can be implemented as follows: there are system-wide and block-wide variants of the following device-wide atomic apis, with the following exceptions:",5
7.14.1.arithmetic functions,#arithmetic-functions,7.14.1.arithmetic functions,6
7.14.2.bitwise functions,#bitwise-functions,7.14.2.bitwise functions,6
7.15.address space predicate functions,#address-space-predicate-functions,7.15.address space predicate functions the functions described in this section have unspecified behavior if the argument is a null pointer.,6
7.15.1.__isglobal(),#isglobal,"7.15.1.__isglobal() returns 1 ifptrcontains the generic address of an object in global memory space, otherwise returns 0.",6
7.15.2.__isshared(),#isshared,"7.15.2.__isshared() returns 1 ifptrcontains the generic address of an object in shared memory space, otherwise returns 0.",6
7.15.3.__isconstant(),#isconstant,"7.15.3.__isconstant() returns 1 ifptrcontains the generic address of an object in constant memory space, otherwise returns 0.",6
7.15.4.__isgridconstant(),#isgridconstant,"7.15.4.__isgridconstant() returns 1 ifptrcontains the generic address of a kernel parameter annotated with__grid_constant__, otherwise returns 0. only supported for compute architectures greater than or equal to 7.x or later.",6
7.15.5.__islocal(),#islocal,"7.15.5.__islocal() returns 1 ifptrcontains the generic address of an object in local memory space, otherwise returns 0.",6
7.4.cuctxcigparam struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUctxCigParam.html#structCUctxCigParam,7.4.cuctxcigparam struct reference,8
7.6.cuda_array3d_descriptor_v2 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__ARRAY3D__DESCRIPTOR__v2.html#structCUDA__ARRAY3D__DESCRIPTOR__v2,7.6.cuda_array3d_descriptor_v2 struct reference,8
7.60.cumemaccessdesc_v1 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUmemAccessDesc__v1.html#structCUmemAccessDesc__v1,7.60.cumemaccessdesc_v1 struct reference,8
[direct3d 9 interoperability],group__CUDA__D3D9.html#group__CUDA__D3D9,[direct3d 9 interoperability],9
8.3.disabling nouveau,#disabling-nouveau,"8.3.disabling nouveau to install the display driver, the nouveau drivers must first be disabled. each distribution of linux has a different method for disabling nouveau. the nouveau drivers are loaded if the following command prints anything:",9
7.11.cudaeglplanedesc struct reference,https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaEglPlaneDesc.html#structcudaEglPlaneDesc,7.11.cudaeglplanedesc struct reference,8
7.9.cudadeviceprop struct reference,https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html#structcudaDeviceProp_19a63114766c4d2309f00403c1bf056c8,7.9.cudadeviceprop struct reference,8
6.17.opengl interoperability [deprecated],https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__OPENGL__DEPRECATED.html#group__CUDART__OPENGL__DEPRECATED,6.17.opengl interoperability [deprecated],9
7.56.culaunchattribute struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUlaunchAttribute.html#structCUlaunchAttribute,7.56.culaunchattribute struct reference,8
7.16.address space conversion functions,#address-space-conversion-functions,7.16.address space conversion functions,6
7.16.1.__cvta_generic_to_global(),#cvta-generic-to-global,7.16.1.__cvta_generic_to_global() returns the result of executing theptxcvta.to.globalinstruction on the generic address denoted byptr.,6
7.16.2.__cvta_generic_to_shared(),#cvta-generic-to-shared,7.16.2.__cvta_generic_to_shared() returns the result of executing theptxcvta.to.sharedinstruction on the generic address denoted byptr.,6
7.16.3.__cvta_generic_to_constant(),#cvta-generic-to-constant,7.16.3.__cvta_generic_to_constant() returns the result of executing theptxcvta.to.constinstruction on the generic address denoted byptr.,6
6.5.device management,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__DEVICE.html#group__CUDA__DEVICE,6.5.device management,9
7.16.4.__cvta_generic_to_local(),#cvta-generic-to-local,7.16.4.__cvta_generic_to_local() returns the result of executing theptxcvta.to.localinstruction on the generic address denoted byptr.,6
7.33.cudahostnodeparams struct reference,https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaHostNodeParams.html#structcudaHostNodeParams,7.33.cudahostnodeparams struct reference,8
7.26.cudaexternalsemaphorewaitparams_v1 struct reference,https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaExternalSemaphoreWaitParams__v1.html#structcudaExternalSemaphoreWaitParams__v1,7.26.cudaexternalsemaphorewaitparams_v1 struct reference,8
2.7.11.cublas<t>trsm(),#cublas-t-trsm,"2.7.11.cublas<t>trsm() this function supports the64-bit integer interface. this function solves the triangular linear system with multiple right-hand-sides \(\left\{ \begin{matrix}
{\text{op}(a)x = \alpha b} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_left}$}} \\
{x\text{op}(a) = \alpha b} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_right}$}} \\
\end{matrix} \right.\) where\(a\)is a triangular matrix stored in lower or upper mode with or without the main diagonal,\(x\)and\(b\)are\(m \times n\)matrices, and\(\alpha\)is a scalar. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) the solution\(x\)overwrites the right-hand-sides\(b\)on exit. no test for singularity or near-singularity is included in this function. the possible error values returned by this function and their meanings are listed below. for references please refer to: strsm,dtrsm,ctrsm,ztrsm",3
7.45.cudevprop_v1 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUdevprop__v1.html#structCUdevprop__v1,7.45.cudevprop_v1 struct reference,8
7.64.cumempoolprops_v1 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUmemPoolProps__v1.html#structCUmemPoolProps__v1_10f9278cc88653f1eee70ab6a7a2ad7f3,7.64.cumempoolprops_v1 struct reference,8
6.8.context management,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CTX.html#group__CUDA__CTX_1g27a365aebb0eb548166309f58a1e8b8e,6.8.context management,9
8.3.1.fedora,#runfile-nouveau-fedora,8.3.1.fedora,9
8.3.2.rhel / rocky and kylinos,#rhel-rocky-and-kylinos,8.3.2.rhel / rocky and kylinos,9
8.3.3.opensuse,#runfile-nouveau-suse,8.3.3.opensuse,9
8.3.4.sles,#runfile-nouveau-sles,8.3.4.sles no actions to disable nouveau are required as nouveau is not installed on sles.,9
8.3.5.wsl,#runfile-nouveau-wsl,8.3.5.wsl no actions to disable nouveau are required as nouveau is not installed on wsl.,9
8.3.6.ubuntu,#runfile-nouveau-ubuntu,8.3.6.ubuntu,9
8.3.7.debian,#runfile-nouveau-debian,8.3.7.debian,9
8.4.device node verification,#device-node-verification,"8.4.device node verification check that the device files/dev/nvidia*exist and have the correct (0666) file permissions. these files are used by the cuda driver to communicate with the kernel-mode portion of the nvidia driver. applications that use the nvidia driver, such as a cuda application or the x server (if any), will normally automatically create these files if they are missing using the setuidnvidia-modprobetool that is bundled with the nvidia driver. however, some systems disallow setuid binaries, so if these files do not exist, you can create them manually by using a startup script such as the one below:",0
8.5.advanced options,#advanced-options,8.5.advanced options,9
8.6.uninstallation,#uninstallation,"8.6.uninstallation to uninstall the cuda toolkit, run the uninstallation script provided in the bin directory of the toolkit. by default, it is located in/usr/local/cuda-12.4/bin: to uninstall the nvidia driver, runnvidia-uninstall: to enable the nouveau drivers, remove the blacklist file created in thedisabling nouveausection, and regenerate the kernel initramfs/initrd again as described in that section.",0
9.conda installation,#conda-installation,9.conda installation this section describes the installation and configuration of cuda when using the conda installer. the conda packages are available athttps://anaconda.org/nvidia.,0
9.1.conda overview,#conda-overview,9.1.conda overview the conda installation installs the cuda toolkit. the installation steps are listed below.,0
9.2.installing cuda using conda,#installing-cuda-using-conda,"9.2.installing cuda using conda to perform a basic install of all cuda toolkit components using conda, run the following command:",0
9.3.uninstalling cuda using conda,#uninstalling-cuda-using-conda,"9.3.uninstalling cuda using conda to uninstall the cuda toolkit using conda, run the following command:",0
9.4.installing previous cuda releases,#installing-previous-cuda-releases,"9.4.installing previous cuda releases all conda packages released under a specific cuda version are labeled with that release version. to install a previous version, include that label in theinstallcommand such as:",0
9.5.upgrading from cudatoolkit package,#upgrading-from-cudatoolkit-package,"9.5.upgrading from cudatoolkit package if you had previously installed cuda using thecudatoolkitpackage and want to maintain a similar install footprint, you can limit your installation to the following packages:",0
10.pip wheels,#pip-wheels,"10.pip wheels nvidia provides python wheels for installing cuda through pip, primarily for using cuda with python. these packages are intended for runtime use and do not currently include developer tools (these can be installed separately). please note that with this installation method, cuda installation environment is managed via pip and additional care must be taken to set up your host environment to use cuda outside the pip environment. prerequisites to install wheels, you must first install thenvidia-pyindexpackage, which is required in order to set up your pip installation to fetch additional python modules from the nvidia ngc pypi repo. if your pip and setuptools python modules are not up-to-date, then use the following command to upgrade these python modules. if these python modules are out-of-date then the commands which follow later in this section may fail. you should now be able to install thenvidia-pyindexmodule. if your project is using arequirements.txtfile, then you can add the following line to yourrequirements.txtfile as an alternative to installing thenvidia-pyindexpackage: procedure install the cuda runtime package: optionally, install additional packages as listed below using the following command: metapackages the following metapackages will install the latest version of the named component on linux for the indicated cuda version. cu12 should be read as cuda12. these metapackages install the following packages:",0
7.16.5.__cvta_global_to_generic(),#cvta-global-to-generic,7.16.5.__cvta_global_to_generic() returns the generic pointer obtained by executing theptxcvta.globalinstruction on the value provided byrawbits.,6
7.16.6.__cvta_shared_to_generic(),#cvta-shared-to-generic,7.16.6.__cvta_shared_to_generic() returns the generic pointer obtained by executing theptxcvta.sharedinstruction on the value provided byrawbits.,6
7.16.7.__cvta_constant_to_generic(),#cvta-constant-to-generic,7.16.7.__cvta_constant_to_generic() returns the generic pointer obtained by executing theptxcvta.constinstruction on the value provided byrawbits.,6
7.16.8.__cvta_local_to_generic(),#cvta-local-to-generic,7.16.8.__cvta_local_to_generic() returns the generic pointer obtained by executing theptxcvta.localinstruction on the value provided byrawbits.,6
7.17.alloca function,#alloca-function,7.17.alloca function,6
7.17.1.synopsis,#synopsis,7.17.1.synopsis,9
7.17.2.description,#description,"7.17.2.description thealloca()function allocatessizebytes of memory in the stack frame of the caller. the returned value is a pointer to allocated memory, the beginning of the memory is 16 bytes aligned when the function is invoked from device code. the allocated memory is automatically freed when the caller toalloca()is returned. it is supported with compute capability 5.2 or higher.",6
7.17.3.example,#example,7.17.3.example,9
7.18.compiler optimization hint functions,#compiler-optimization-hint-functions,7.18.compiler optimization hint functions the functions described in this section can be used to provide additional information to the compiler optimizer.,6
7.18.1.__builtin_assume_aligned(),#builtin-assume-aligned,"7.18.1.__builtin_assume_aligned() allows the compiler to assume that the argument pointer is aligned to at leastalignbytes, and returns the argument pointer. example: three parameter version: allows the compiler to assume that(char*)exp-offsetis aligned to at leastalignbytes, and returns the argument pointer. example:",6
7.18.2.__builtin_assume(),#builtin-assume,"7.18.2.__builtin_assume() allows the compiler to assume that the boolean argument is true. if the argument is not true at run time, then the behavior is undefined. note that if the argument has side effects, the behavior is unspecified. example:",6
7.18.3.__assume(),#assume,"7.18.3.__assume() allows the compiler to assume that the boolean argument is true. if the argument is not true at run time, then the behavior is undefined. note that if the argument has side effects, the behavior is unspecified. example:",6
7.18.4.__builtin_expect(),#builtin-expect,"7.18.4.__builtin_expect() indicates to the compiler that it is expected thatexp==c, and returns the value ofexp. typically used to indicate branch prediction information to the compiler. example:",6
7.18.5.__builtin_unreachable(),#builtin-unreachable,7.18.5.__builtin_unreachable() indicates to the compiler that control flow never reaches the point where this function is being called from. the program has undefined behavior if the control flow does actually reach this point at run time. example:,6
7.18.6.restrictions,#restrictions,"7.18.6.restrictions __assume()is only supported when usingcl.exehost compiler. the other functions are supported on all platforms, subject to the following restrictions:",6
2.7.12.cublas<t>trsmbatched(),#cublas-t-trsmbatched,"2.7.12.cublas<t>trsmbatched() this function supports the64-bit integer interface. this function solves an array of triangular linear systems with multiple right-hand-sides \(\left\{ \begin{matrix}
{\text{op}(a\lbrack i\rbrack)x\lbrack i\rbrack = \alpha b\lbrack i\rbrack} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_left}$}} \\
{x\lbrack i\rbrack\text{op}(a\lbrack i\rbrack) = \alpha b\lbrack i\rbrack} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_right}$}} \\
\end{matrix} \right.\) where\(a\lbrack i\rbrack\)is a triangular matrix stored in lower or upper mode with or without the main diagonal,\(x\lbrack i\rbrack\)and\(b\lbrack i\rbrack\)are\(m \times n\)matrices, and\(\alpha\)is a scalar. also, for matrix\(a\) \(\text{op}(a\lbrack i\rbrack) = \left\{ \begin{matrix}
{a\lbrack i\rbrack} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
{a^{t}\lbrack i\rbrack} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
{a^{h}\lbrack i\rbrack} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) the solution\(x\lbrack i\rbrack\)overwrites the right-hand-sides\(b\lbrack i\rbrack\)on exit. no test for singularity or near-singularity is included in this function. this function works for any sizes but is intended to be used for matrices of small sizes where the launch overhead is a significant factor. for bigger sizes, it might be advantageous to callbatchcounttimes the regularcublas<t>trsmwithin a set of cuda streams. the current implementation is limited to devices with compute capability above or equal 2.0. the possible error values returned by this function and their meanings are listed below. for references please refer to: strsm,dtrsm,ctrsm,ztrsm",3
7.11.cuda_child_graph_node_params struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__CHILD__GRAPH__NODE__PARAMS.html#structCUDA__CHILD__GRAPH__NODE__PARAMS,7.11.cuda_child_graph_node_params struct reference,8
2.7.13.cublas<t>hemm(),#cublas-t-hemm,"2.7.13.cublas<t>hemm() this function supports the64-bit integer interface. this function performs the hermitian matrix-matrix multiplication \(c = \left\{ \begin{matrix}
{\alpha ab + \beta c} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_left}$}} \\
{\alpha ba + \beta c} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_right}$}} \\
\end{matrix} \right.\) where\(a\)is a hermitian matrix stored in lower or upper mode,\(b\)and\(c\)are\(m \times n\)matrices, and\(\alpha\)and\(\beta\)are scalars. the possible error values returned by this function and their meanings are listed below. for references please refer to: chemm,zhemm",3
6.16.multicast object management,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MULTICAST.html#group__CUDA__MULTICAST,6.16.multicast object management,9
6.8.execution control,https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EXECUTION.html#group__CUDART__EXECUTION_1ge194af462d927583bed3acf60d450218,6.8.execution control,9
6.18.direct3d 9 interoperability,https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__D3D9.html#group__CUDART__D3D9_1gab5efa8a8882a6e0ee99717a434730b0,6.18.direct3d 9 interoperability,9
9.deprecated list,https://docs.nvidia.com/cuda/cuda-driver-api/deprecated.html#deprecated__deprecated_1_deprecated000061,9.deprecated list,9
7.19.warp vote functions,#warp-vote-functions,"7.19.warp vote functions deprecation notice:__any,__all, and__ballothave been deprecated in cuda 9.0 for all devices. removal notice: when targeting devices with compute capability 7.x or higher,__any,__all, and__ballotare no longer available and their sync variants should be used instead. the warp vote functions allow the threads of a givenwarpto perform a reduction-and-broadcast operation. these functions take as input an integerpredicatefrom each thread in the warp and compare those values with zero. the results of the comparisons are combined (reduced) across theactivethreads of the warp in one of the following ways, broadcasting a single return value to each participating thread: for__all_sync,__any_sync, and__ballot_sync, a mask must be passed that specifies the threads participating in the call. a bit, representing the threads lane id, must be set for each participating thread to ensure they are properly converged before the intrinsic is executed by the hardware. each calling thread must have its own bit set in the mask and all non-exited threads named in mask must execute the same intrinsic with the same mask, or the result is undefined. these intrinsics do not imply a memory barrier. they do not guarantee any memory ordering.",5
7.20.warp match functions,#warp-match-functions,7.20.warp match functions __match_any_syncand__match_all_syncperform a broadcast-and-compare operation of a variable between threads within awarp. supported by devices of compute capability 7.x or higher.,5
7.20.1.synopsis,#synopsis-match,"7.20.1.synopsis tcan beint,unsignedint,long,unsignedlong,longlong,unsignedlonglong,floatordouble.",6
7.20.2.description,#warp-description-match,"7.20.2.description the__match_sync()intrinsics permit a broadcast-and-compare of a valuevalueacross threads in a warp after synchronizing threads named inmask. the new*_syncmatch intrinsics take in a mask indicating the threads participating in the call. a bit, representing the threads lane id, must be set for each participating thread to ensure they are properly converged before the intrinsic is executed by the hardware. each calling thread must have its own bit set in the mask and all non-exited threads named in mask must execute the same intrinsic with the same mask, or the result is undefined. these intrinsics do not imply a memory barrier. they do not guarantee any memory ordering.",5
6.modules,https://docs.nvidia.com/cuda/cuda-driver-api/modules.html#modules,6.modules,9
2.7.14.cublas<t>herk(),#cublas-t-herk,"2.7.14.cublas<t>herk() this function supports the64-bit integer interface. this function performs the hermitian rank-\(k\)update \(c = \alpha\text{op}(a)\text{op}(a)^{h} + \beta c\) where\(\alpha\)and\(\beta\)are scalars,\(c\)is a hermitian matrix stored in lower or upper mode, and\(a\)is a matrix with dimensions\(\text{op}(a)\)\(n \times k\). also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) the possible error values returned by this function and their meanings are listed below. for references please refer to: cherk,zherk",3
6.25.occupancy,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__OCCUPANCY.html#group__CUDA__OCCUPANCY,6.25.occupancy,9
11.tarball and zip archive deliverables,#tarball-and-zip-archive-deliverables,"11.tarball and zip archive deliverables in an effort to meet the needs of a growing customer base requiring alternative installer packaging formats, as well as a means of input into community ci/cd systems, tarball and zip archives are available for each component. these tarball and zip archives, known as binary archives, are provided athttps://developer.download.nvidia.com/compute/cuda/redist/. these component .tar.xz and .zip binary archives do not replace existing packages such as .deb, .rpm, runfile, conda, etc. and are not meant for general consumption, as they are not installers. however this standardized approach will replace existing .txz archives. for each release, a json manifest is provided such asredistrib_11.4.2.json, which corresponds to the cuda 11.4.2 release label (cuda 11.4 update 2) which includes the release date, the name of each component, license name, relative url for each platform and checksums. package maintainers are advised to check the provided license for each component prior to redistribution. instructions for developers using cmake and bazel build systems are provided in the next sections.",0
11.1.parsing redistrib json,#parsing-redistrib-json,"11.1.parsing redistrib json the following example of a json manifest contains keys for each component: name, license, version, and a platform array which includes relative_path, sha256, md5, and size (bytes) for each archive. a json schema is provided athttps://developer.download.nvidia.com/compute/redist/redistrib-v2.schema.json. a sample script that parses these json manifests is available ongithub:",9
11.2.importing tarballs into cmake,#importing-tarballs-into-cmake,"11.2.importing tarballs into cmake the recommended module for importing these tarballs into the cmake build system is viafindcudatoolkit(3.17 and newer). the path to the extraction location can be specified with thecudatoolkit_rootenvironmental variable. for examplecmakelists.txtand commands, seecmake/1_findcudatoolkit/. for older versions of cmake, theexternalproject_addmodule is an alternative method. for examplecmakelists.txtfile and commands, seecmake/2_externalproject/.",9
11.3.importing tarballs into bazel,#importing-tarballs-into-bazel,"11.3.importing tarballs into bazel the recommended method of importing these tarballs into the bazel build system is usinghttp_archiveandpkg_tar. for an example, seebazel/1_pkg_tar/.",9
12.cuda cross-platform environment,#cuda-cross-platform-environment,"12.cuda cross-platform environment cross development for arm64-sbsa is supported on ubuntu 20.04, ubuntu 22.04, rhel 8, rhel 9, and sles 15. cross development for arm64-jetson is only supported on ubuntu 20.04 we recommend selecting a host development environment that matches the supported cross-target environment. this selection helps prevent possible host/target incompatibilities, such as gcc or glibc version mismatches.",0
12.1.cuda cross-platform installation,#cuda-cross-platform-installation,"12.1.cuda cross-platform installation some of the following steps may have already been performed as part of thenative ubuntu installation. such steps can safely be skipped. these steps should be performed on the x86_64 host system, rather than the target system. to install the native cuda toolkit on the target system, refer to the nativeubuntuinstallation section.",0
12.2.cuda cross-platform samples,#cuda-cross-platform-samples,"12.2.cuda cross-platform samples cuda samples are now located inhttps://github.com/nvidia/cuda-samples, which includes instructions for obtaining, building, and running the samples.",0
13.post-installation actions,#post-installation-actions,"13.post-installation actions the post-installation actions must be manually performed. these actions are split into mandatory, recommended, and optional sections.",9
13.1.mandatory actions,#mandatory-actions,13.1.mandatory actions some actions must be taken after the installation before the cuda toolkit and driver can be used.,0
13.1.1.environment setup,#environment-setup,"13.1.1.environment setup thepathvariable needs to includeexportpath=/usr/local/cuda-12.4/bin${path:+:${path}}. nsight compute has moved to/opt/nvidia/nsight-compute/only in rpm/deb installation method. when using.runinstaller it is still located under/usr/local/cuda-12.4/. to add this path to thepathvariable: in addition, when using the runfile installation method, theld_library_pathvariable needs to contain/usr/local/cuda-12.4/lib64on a 64-bit system, or/usr/local/cuda-12.4/libon a 32-bit system note that the above paths change when using a custom install path with the runfile installation method.",0
13.2.recommended actions,#recommended-actions,13.2.recommended actions other actions are recommended to verify the integrity of the installation.,9
13.2.1.install persistence daemon,#install-persistence-daemon,"13.2.1.install persistence daemon nvidia is providing a user-space daemon on linux to support persistence of driver state across cuda job runs. the daemon approach provides a more elegant and robust solution to this problem than persistence mode. for more details on the nvidia persistence daemon, see the documentationhere. the nvidia persistence daemon can be started as the root user by running: this command should be run on boot. consult your linux distributions init documentation for details on how to automate this.",0
13.2.2.install writable samples,#install-writable-samples,"13.2.2.install writable samples cuda samples are now located inhttps://github.com/nvidia/cuda-samples, which includes instructions for obtaining, building, and running the samples.",0
13.2.3.verify the installation,#verify-the-installation,"13.2.3.verify the installation before continuing, it is important to verify that the cuda toolkit can find and communicate correctly with the cuda-capable hardware. to do this, you need to compile and run some of the sample programs, located inhttps://github.com/nvidia/cuda-samples.",0
13.2.4.install nsight eclipse plugins,#install-nsight-eclipse-plugins,"13.2.4.install nsight eclipse plugins to install nsight eclipse plugins, an installation script is provided: refer tonsight eclipse plugins installation guidefor more details.",9
7.34.cuda_mem_free_node_params struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__MEM__FREE__NODE__PARAMS.html#structCUDA__MEM__FREE__NODE__PARAMS,7.34.cuda_mem_free_node_params struct reference,8
6.10.module management,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MODULE.html#group__CUDA__MODULE,6.10.module management,9
7.22.cuda_external_semaphore_handle_desc_v1 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__EXTERNAL__SEMAPHORE__HANDLE__DESC__v1.html#structCUDA__EXTERNAL__SEMAPHORE__HANDLE__DESC__v1,7.22.cuda_external_semaphore_handle_desc_v1 struct reference,8
6.28.texture object management,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TEXOBJECT.html#group__CUDA__TEXOBJECT,6.28.texture object management,9
6.11.module management [deprecated],https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MODULE__DEPRECATED.html#group__CUDA__MODULE__DEPRECATED,6.11.module management [deprecated],9
7.28.cuda_kernel_node_params_v1 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__KERNEL__NODE__PARAMS__v1.html#structCUDA__KERNEL__NODE__PARAMS__v1,7.28.cuda_kernel_node_params_v1 struct reference,8
6.36.profiler control [deprecated],https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__PROFILER__DEPRECATED.html#group__CUDA__PROFILER__DEPRECATED,6.36.profiler control [deprecated],9
6.13.stream ordered memory allocator,https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY__POOLS.html#group__CUDART__MEMORY__POOLS,6.13.stream ordered memory allocator,5
7.29.cudagraphexecupdateresultinfo struct reference,https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaGraphExecUpdateResultInfo.html#structcudaGraphExecUpdateResultInfo,7.29.cudagraphexecupdateresultinfo struct reference,8
7.21.warp reduce functions,#warp-reduce-functions,"7.21.warp reduce functions the__reduce_sync(unsignedmask,tvalue)intrinsics perform a reduction operation on the data provided invalueafter synchronizing threads named inmask. t can be unsigned or signed for {add, min, max} and unsigned only for {and, or, xor} operations. supported by devices of compute capability 8.x or higher.",1
7.21.1.synopsis,#warp-reduce-synopsis,7.21.1.synopsis,9
7.21.2.description,#warp-reduce-description,"7.21.2.description themaskindicates the threads participating in the call. a bit, representing the threads lane id, must be set for each participating thread to ensure they are properly converged before the intrinsic is executed by the hardware. each calling thread must have its own bit set in the mask and all non-exited threads named in mask must execute the same intrinsic with the same mask, or the result is undefined. these intrinsics do not imply a memory barrier. they do not guarantee any memory ordering.",5
7.22.warp shuffle functions,#warp-shuffle-functions,"7.22.warp shuffle functions __shfl_sync,__shfl_up_sync,__shfl_down_sync, and__shfl_xor_syncexchange a variable between threads within awarp. supported by devices of compute capability 5.0 or higher. deprecation notice:__shfl,__shfl_up,__shfl_down, and__shfl_xorhave been deprecated in cuda 9.0 for all devices. removal notice: when targeting devices with compute capability 7.x or higher,__shfl,__shfl_up,__shfl_down, and__shfl_xorare no longer available and their sync variants should be used instead.",5
7.22.1.synopsis,#warp-shuffle-synopsis,"7.22.1.synopsis tcan beint,unsignedint,long,unsignedlong,longlong,unsignedlonglong,floatordouble. with thecuda_fp16.hheader included,tcan also be__halfor__half2. similarly, with thecuda_bf16.hheader included,tcan also be__nv_bfloat16or__nv_bfloat162.",6
5.rules for version mixing,https://docs.nvidia.com/cuda/cuda-driver-api/version-mixing-rules.html#version-mixing-rules,5.rules for version mixing,9
7.37.cuda_memcpy3d_v2 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__MEMCPY3D__v2.html#structCUDA__MEMCPY3D__v2,7.37.cuda_memcpy3d_v2 struct reference,8
7.65.cumempoolptrexportdata_v1 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUmemPoolPtrExportData__v1.html#structCUmemPoolPtrExportData__v1,7.65.cumempoolptrexportdata_v1 struct reference,8
2.7.15.cublas<t>her2k(),#cublas-t-her2k,"2.7.15.cublas<t>her2k() this function supports the64-bit integer interface. this function performs the hermitian rank-\(2k\)update \(c = \alpha\text{op}(a)\text{op}(b)^{h} + \overset{}{\alpha}\text{op}(b)\text{op}(a)^{h} + \beta c\) where\(\alpha\)and\(\beta\)are scalars,\(c\)is a hermitian matrix stored in lower or upper mode, and\(a\)and\(b\)are matrices with dimensions\(\text{op}(a)\)\(n \times k\)and\(\text{op}(b)\)\(n \times k\), respectively. also, for matrix\(a\)and\(b\) \(\text{op(}a\text{) and op(}b\text{)} = \left\{ \begin{matrix}
{a\text{ and }b} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_n}$}} \\
{a^{h}\text{ and }b^{h}} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) the possible error values returned by this function and their meanings are listed below. for references please refer to: cher2k,zher2k",3
2.7.16.cublas<t>herkx(),#cublas-t-herkx,"2.7.16.cublas<t>herkx() this function supports the64-bit integer interface. this function performs a variation of the hermitian rank-\(k\)update \(c = \alpha\text{op}(a)\text{op}(b)^{h} + \beta c\) where\(\alpha\)and\(\beta\)are scalars,\(c\)is a hermitian matrix stored in lower or upper mode, and\(a\)and\(b\)are matrices with dimensions\(\text{op}(a)\)\(n \times k\)and\(\text{op}(b)\)\(n \times k\), respectively. also, for matrix\(a\)and\(b\) \(\text{op(}a\text{) and op(}b\text{)} = \left\{ \begin{matrix}
{a\text{ and }b} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_n}$}} \\
{a^{h}\text{ and }b^{h}} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) this routine can be used when the matrix b is in such way that the result is guaranteed to be hermitian. an usual example is when the matrix b is a scaled form of the matrix a: this is equivalent to b being the product of the matrix a and a diagonal matrix. for an efficient computation of the product of a regular matrix with a diagonal matrix, refer to the routinecublas<t>dgmm. the possible error values returned by this function and their meanings are listed below. for references please refer to: cherk,zherkand cher2k,zher2k",3
6.14.virtual memory management,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__VA.html#group__CUDA__VA,6.14.virtual memory management,5
13.2.5.local repo removal,#local-repo-removal,13.2.5.local repo removal removal of the local repo installer is recommended after installation ofcuda sdk. ubuntu and debian fedora rhel 9 / rocky linux 9 and rhel 8 / rocky linux 8 opensuse 15 and sles 15 removal of the local repo installer is recommended after installation ofnvida driver. ubuntu and debian fedora rhel 9 / rocky linux 9 and rhel 8 / rocky linux 8 opensuse 15 and sles 15,9
13.3.optional actions,#optional-actions,"13.3.optional actions other options are not necessary to use the cuda toolkit, but are available to provide additional features.",0
13.3.1.install third-party libraries,#install-third-party-libraries,"13.3.1.install third-party libraries some cuda samples use third-party libraries which may not be installed by default on your system. these samples attempt to detect any required libraries when building. if a library is not detected, it waives itself and warns you which library is missing. to build and run these samples, you must install the missing libraries. in cases where these dependencies are not installed, follow the instructions below. rhel 8 / rocky linux 8 rhel 9 / rocky linux 9 kylinos 10 fedora sles opensuse ubuntu debian",0
13.3.2.install the source code for cuda-gdb,#install-the-source-code-for-cuda-gdb,"13.3.2.install the source code for cuda-gdb thecuda-gdbsource must be explicitly selected for installation with the runfile installation method. during the installation, in the component selection page, expand the component cuda tools 12.4 and selectcuda-gdb-srcfor installation. it is unchecked by default. to obtain a copy of the source code forcuda-gdbusing the rpm and debian installation methods, thecuda-gdb-srcpackage must be installed. the source code is installed as a tarball in the/usr/local/cuda-12.4/extrasdirectory.",0
13.3.3.select the active version of cuda,#select-the-active-version-of-cuda,"13.3.3.select the active version of cuda for applications that rely on the symlinks/usr/local/cudaand/usr/local/cuda-major, you may wish to change to a different installed version of cuda using the provided alternatives. to show the active version of cuda and all available versions: to show the active minor version of a given major cuda release: to update the active version of cuda:",0
14.advanced setup,#advanced-setup,14.advanced setup below is information on some advanced setup scenarios which are not covered in the basic instructions above.,9
15.frequently asked questions,#frequently-asked-questions,15.frequently asked questions,9
15.1.how do i install the toolkit in a different location?,#how-do-i-install-the-toolkit-in-a-different-location,"15.1.how do i install the toolkit in a different location? the runfile installation asks where you wish to install the toolkit during an interactive install. if installing using a non-interactive install, you can use the--toolkitpathparameter to change the install location: the rpm and deb packages cannot be installed to a custom install location directly using the package managers. see the install cuda to a specific directory using the package manager installation method scenario in theadvanced setupsection for more information.",0
15.2.why do i see nvcc: no such file or directory when i try to build a cuda application?,#why-do-i-see-nvcc-no-such-file-or-directory-when-i-try-to-build-a-cuda-application,"15.2.why do i see nvcc: no such file or directory when i try to build a cuda application? your path environment variable is not set up correctly. ensure that your path includes the bin directory where you installed the toolkit, usually/usr/local/cuda-12.4/bin.",0
15.3.why do i see error while loading shared libraries: <lib name>: cannot open shared object file: no such file or directory when i try to run a cuda application that uses a cuda library?,#faq3,"15.3.why do i see error while loading shared libraries: <lib name>: cannot open shared object file: no such file or directory when i try to run a cuda application that uses a cuda library? your ld_library_path environment variable is not set up correctly. ensure that your ld_library_path includes the lib and/or lib64 directory where you installed the toolkit, usually/usr/local/cuda-12.4/lib{,64}:",0
15.4.why do i see multiple 404 not found errors when updating my repository meta-data on ubuntu?,#why-do-i-see-multiple-404-not-found-errors-when-updating-my-repository-meta-data-on-ubuntu,"15.4.why do i see multiple 404 not found errors when updating my repository meta-data on ubuntu? these errors occur after adding a foreign architecture because apt is attempting to query for each architecture within each repository listed in the systems sources.list file. repositories that do not host packages for the newly added architecture will present this error. while noisy, the error itself does no harm. please see theadvanced setupsection for details on how to modify yoursources.listfile to prevent these errors.",9
15.5.how can i tell x to ignore a gpu for compute-only use?,#how-can-i-tell-x-to-ignore-a-gpu-for-compute-only-use,"15.5.how can i tell x to ignore a gpu for compute-only use? to make sure x doesnt use a certain gpu for display, you need to specify whichothergpu to use for display. for more information, please refer to the use a specific gpu for rendering the display scenario in theadvanced setupsection.",0
15.6.why doesnt the cuda-repo package install the cuda toolkit and drivers?,#why-doesn-t-the-cuda-repo-package-install-the-cuda-toolkit-and-drivers,"15.6.why doesnt the cuda-repo package install the cuda toolkit and drivers? when using rpm or deb, the downloaded package is a repository package. such a package only informs the package manager where to find the actual installation packages, but will not install them. see thepackage manager installationsection for more details.",0
15.7.how do i get cuda to work on a laptop with an igpu and a dgpu running ubuntu14.04?,#how-do-i-get-cuda-to-work-on-a-laptop-with-an-igpu-and-a-dgpu-running-ubuntu14-04,"15.7.how do i get cuda to work on a laptop with an igpu and a dgpu running ubuntu14.04? after installing cuda, set the driver value for theinteldevice in/etc/x11/xorg.confto modesetting as shown below: to prevent ubuntu from reverting the change in xorg.conf, edit/etc/default/grubto add nogpumanager to grub_cmdline_linux_default. run the following command to update grub before rebooting:",0
"15.8.what do i do if the display does not load, or cuda does not work, after performing a system update?",#what-do-i-do-if-the-display-does-not-load-or-cuda-does-not-work-after-performing-a-system-update,"15.8.what do i do if the display does not load, or cuda does not work, after performing a system update? system updates may include an updated linux kernel. in many cases, a new linux kernel will be installed without properly updating the required linux kernel headers and development packages. to ensure the cuda driver continues to work when performing a system update, rerun the commands in thekernel headers and development packagessection. additionally, on fedora, the akmods framework will sometimes fail to correctly rebuild the nvidia kernel module packages when a new linux kernel is installed. when this happens, it is usually sufficient to invoke akmods manually and regenerate the module mapping files by running the following commands in a virtual console, and then rebooting: you can reach a virtual console by hittingctrl+alt+f2at the same time.",0
7.31.cuda_launch_params_v1 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__LAUNCH__PARAMS__v1.html#structCUDA__LAUNCH__PARAMS__v1,7.31.cuda_launch_params_v1 struct reference,8
7.26.cuda_host_node_params_v1 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__HOST__NODE__PARAMS__v1.html#structCUDA__HOST__NODE__PARAMS__v1,7.26.cuda_host_node_params_v1 struct reference,8
7.22.2.description,#warp-shuffle-description,"7.22.2.description the__shfl_sync()intrinsics permit exchanging of a variable between threads within a warp without use of shared memory. the exchange occurs simultaneously for allactivethreads within the warp (and named inmask), moving 4 or 8 bytes of data per thread depending on the type. threads within a warp are referred to aslanes, and may have an index between 0 andwarpsize-1(inclusive). four source-lane addressing modes are supported: threads may only read data from another thread which is actively participating in the__shfl_sync()command. if the target thread isinactive, the retrieved value is undefined. all of the__shfl_sync()intrinsics take an optionalwidthparameter which alters the behavior of the intrinsic.widthmust have a value which is a power of two in the range [1, warpsize] (i.e., 1, 2, 4, 8, 16 or 32). results are undefined for other values. __shfl_sync()returns the value ofvarheld by the thread whose id is given bysrclane. if width is less thanwarpsizethen each subsection of the warp behaves as a separate entity with a starting logical lane id of 0. ifsrclaneis outside the range[0:width-1], the value returned corresponds to the value of var held by thesrclanemodulowidth(i.e. within the same subsection). __shfl_up_sync()calculates a source lane id by subtractingdeltafrom the callers lane id. the value ofvarheld by the resulting lane id is returned: in effect,varis shifted up the warp bydeltalanes. if width is less thanwarpsizethen each subsection of the warp behaves as a separate entity with a starting logical lane id of 0. the source lane index will not wrap around the value ofwidth, so effectively the lowerdeltalanes will be unchanged. __shfl_down_sync()calculates a source lane id by addingdeltato the callers lane id. the value ofvarheld by the resulting lane id is returned: this has the effect of shiftingvardown the warp bydeltalanes. if width is less thanwarpsizethen each subsection of the warp behaves as a separate entity with a starting logical lane id of 0. as for__shfl_up_sync(), the id number of the source lane will not wrap around the value of width and so the upperdeltalanes will remain unchanged. __shfl_xor_sync()calculates a source line id by performing a bitwise xor of the callers lane id withlanemask: the value ofvarheld by the resulting lane id is returned. ifwidthis less thanwarpsizethen each group ofwidthconsecutive threads are able to access elements from earlier groups of threads, however if they attempt to access elements from later groups of threads their own value ofvarwill be returned. this mode implements a butterfly addressing pattern such as is used in tree reduction and broadcast. the new*_syncshfl intrinsics take in a mask indicating the threads participating in the call. a bit, representing the threads lane id, must be set for each participating thread to ensure they are properly converged before the intrinsic is executed by the hardware. each calling thread must have its own bit set in the mask and all non-exited threads named in mask must execute the same intrinsic with the same mask, or the result is undefined. threads may only read data from another thread which is actively participating in the__shfl_sync()command. if the target thread is inactive, the retrieved value is undefined. these intrinsics do not imply a memory barrier. they do not guarantee any memory ordering.",5
7.22.3.examples,#examples,7.22.3.examples,9
7.23.nanosleep function,#nanosleep-function,7.23.nanosleep function,6
7.23.1.synopsis,#nanosleep-synopsis,7.23.1.synopsis,9
7.23.2.description,#nanosleep-description,7.23.2.description __nanosleep(ns)suspends the thread for a sleep duration of approximatelynsnanoseconds.  the maximum sleep duration is approximately 1 millisecond. it is supported with compute capability 7.0 or higher.,6
6.24.graph management,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__GRAPH.html,6.24.graph management,9
7.23.3.example,#nanosleep-example,7.23.3.example the following code implements a mutex with exponential back-off.,6
7.24.warp matrix functions,#warp-matrix-functions,"7.24.warp matrix functions c++ warp matrix operations leverage tensor cores to accelerate matrix problems of the formd=a*b+c. these operations are supported on mixed-precision floating point data for devices of compute capability 7.0 or higher. this requires co-operation from all threads in awarp. in addition, these operations are allowed in conditional code only if the condition evaluates identically across the entirewarp, otherwise the code execution is likely to hang.",1
15.9.how do i install a cuda driver with a version less than 367 using a network repo?,#how-do-i-install-a-cuda-driver-with-a-version-less-than-367-using-a-network-repo,"15.9.how do i install a cuda driver with a version less than 367 using a network repo? to install a cuda driver at a version earlier than 367 using a network repo, the required packages will need to be explicitly installed at the desired version. for example, to install 352.99, instead of installing the cuda-drivers metapackage at version 352.99, you will need to install all required packages of cuda-drivers at version 352.99.",0
15.10.how do i install an older cuda version using a network repo?,#how-do-i-install-an-older-cuda-version-using-a-network-repo,"15.10.how do i install an older cuda version using a network repo? depending on your system configuration, you may not be able to install old versions of cuda using the cuda metapackage. in order to install a specific version of cuda, you may need to specify all of the packages that would normally be installed by the cuda metapackage at the version you want to install. if you are using yum to install certain packages at an older version, the dependencies may not resolve as expected. in this case you may need to pass --setopt=obsoletes=0 to yum to allow an install of packages which are obsoleted at a later version than you are trying to install.",0
15.11.why does the installation on suse install the mesa-dri-nouveau dependency?,#why-does-the-installation-on-suse-install-the-mesa-dri-nouveau-dependency,"15.11.why does the installation on suse install the mesa-dri-nouveau dependency? this dependency comes from the suse repositories and shouldnt affect the use of the nvidia driver or the cuda toolkit. to disable this dependency, you can lock that package with the following command:",0
15.12.how do i handle errors were encountered while processing: glx-diversions?,#how-do-i-handle-errors-were-encountered-while-processing-glx-diversions,15.12.how do i handle errors were encountered while processing: glx-diversions? this sometimes occurs when trying to uninstall cuda after a clean .deb installation. run the following commands: then re-run the commands fromremoving cuda toolkit and driver.,0
2.8.blas-like extension,#blas-like-extension,2.8.blas-like extension this section describes the blas-extension functions that perform matrix-matrix operations.,3
2.8.1.cublas<t>geam(),#cublas-t-geam,"2.8.1.cublas<t>geam() this function supports the64-bit integer interface. this function performs the matrix-matrix addition/transposition \(c = \alpha\text{op}(a) + \beta\text{op}(b)\) where\(\alpha\)and\(\beta\)are scalars, and\(a\),\(b\)and\(c\)are matrices stored in column-major format with dimensions\(\text{op}(a)\)\(m \times n\),\(\text{op}(b)\)\(m \times n\)and\(c\)\(m \times n\), respectively. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) and\(\text{op}(b)\)is defined similarly for matrix\(b\). the operation is out-of-place if c does not overlap a or b. the in-place mode supports the following two operations, \(c = \alpha\text{*}c + \beta\text{op}(b)\) \(c = \alpha\text{op}(a) + \beta\text{*}c\) for in-place mode, ifc=a,ldc=ldaandtransa=cublas_op_n. ifc=b,ldc=ldbandtransb=cublas_op_n. if the user does not meet above requirements,cublas_status_invalid_valueis returned. the operation includes the following special cases: the user can reset matrix c to zero by setting*alpha=*beta=0. the user can transpose matrix a by setting*alpha=1and*beta=0. the possible error values returned by this function and their meanings are listed below.",3
7.50.cudamemfreenodeparams struct reference,https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaMemFreeNodeParams.html#structcudaMemFreeNodeParams,7.50.cudamemfreenodeparams struct reference,8
6.4.error handling,https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__ERROR.html,6.4.error handling,9
7.24.1.description,#wmma-description,"7.24.1.description all following functions and types are defined in the namespacenvcuda::wmma. sub-byte operations are considered preview, i.e. the data structures and apis for them are subject to change and may not be compatible with future releases. this extra functionality is defined in thenvcuda::wmma::experimentalnamespace. only certain combinations of template arguments are allowed. the first template parameter specifies how the fragment will participate in the matrix operation. acceptable values foruseare: ifsatf(saturate to finite value) mode istrue, the following additional numerical properties apply for the destination accumulator: because the map of matrix elements into each threadsfragmentis unspecified, individual matrix elements must be accessed from memory (shared or global) after callingstore_matrix_sync. in the special case where all threads in the warp will apply an element-wise operation uniformly to all fragment elements, direct element access can be implemented using the followingfragmentclass members. as an example, the following code scales anaccumulatormatrix tile by half.",1
7.24.2.alternate floating point,#alternate-floating-point,7.24.2.alternate floating point tensor cores support alternate types of floating point operations on devices with compute capability 8.0 and higher.,6
7.24.3.double precision,#double-precision,"7.24.3.double precision tensor cores support double-precision floating point operations on devices with compute capability 8.0 and higher. to use this new functionality, afragmentwith thedoubletype must be used. themma_syncoperation will be performed with the .rn (rounds to nearest even) rounding modifier.",6
6.29.surface object management,https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__SURFOBJECT.html#group__CUDA__SURFOBJECT,6.29.surface object management,9
6.30.graph management,https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__GRAPH.html#group__CUDART__GRAPH_1ga351557d4d9ecab23d56395599b0e069,6.30.graph management,9
7.39.cudalaunchattribute struct reference,https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaLaunchAttribute.html#structcudaLaunchAttribute,7.39.cudalaunchattribute struct reference,8
16.additional considerations,#additional-considerations,"16.additional considerations now that you have cuda-capable hardware and the nvidia cuda toolkit installed, you can examine and enjoy the numerous included programs. to begin using cuda to accelerate the performance of your own applications, consult the cuda c++ programming guide, located in/usr/local/cuda-12.4/doc. a number of helpful development tools are included in the cuda toolkit to assist you as you develop your cuda programs, such as nvidiansight eclipse edition, nvidia visual profiler, cuda-gdb, and cuda-memcheck. for technical support on programming questions, consult and participate in the developer forums athttps://forums.developer.nvidia.com/c/accelerated-computing/cuda/206.",0
17.switching between driver module flavors,#switching-between-driver-module-flavors,"17.switching between driver module flavors use the following steps to switch between the nvidia driver legacy and open module flavors on your system. fedora, rhel 9 / rocky linux 9, rhel 8 / rocky linux 8 to switch between legacy and open: uninstall, then reinstall. kylin os to switch between legacy and open: uninstall, then reinstall. ubuntu to switch from legacy to open: to switch from open to legacy: debian to switch from legacy to open: to switch from open to legacy: opensuse to switch from legacy to open: to switch from open to legacy: sles to switch from legacy to open: to switch from open to legacy:",0
18.removing cuda toolkit and driver,#removing-cuda-toolkit-and-driver,18.removing cuda toolkit and driver follow the below steps to properly uninstall the cuda toolkit and nvidia drivers from your system. these steps will ensure that the uninstallation will be clean. kylinos 10 to remove cuda toolkit: to remove nvidia drivers: to reset the module stream: rhel 9 / rocky linux 9 to remove cuda toolkit: to remove nvidia drivers: to reset the module stream: rhel 8 / rocky linux 8 to remove cuda toolkit: to remove nvidia drivers: to reset the module stream: fedora to remove cuda toolkit: to remove nvidia drivers: to reset the module stream: to remove 3rd party nvidia drivers: opensuse / sles to remove cuda toolkit: to remove nvidia drivers: ubuntu and debian to remove cuda toolkit: to remove nvidia drivers: to clean up the uninstall:,0
19.notices,#notices,19.notices,9
19.1.notice,#notice,"19.1.notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation (nvidia) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material (defined below), code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer (terms of sale). nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion and/or use of nvidia products in such equipment or applications and therefore such inclusion and/or use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions and/or requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the nvidia product in any manner that is contrary to this document or (ii) customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding third-party products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents (together and separately, materials) are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product.",4
19.2.opencl,#opencl,19.2.opencl opencl is a trademark of apple inc. used under license to the khronos group inc.,4
19.3.trademarks,#trademarks,19.3.trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.,4
20.copyright,#copyright,20.copyright  2009-2024 nvidia corporation & affiliates. all rights reserved. this product includes software developed by the syncro soft srl (http://www.sync.ro/).,4
7.14.cuda_event_wait_node_params struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__EVENT__WAIT__NODE__PARAMS.html#structCUDA__EVENT__WAIT__NODE__PARAMS,7.14.cuda_event_wait_node_params struct reference,8
7.27.cudafuncattributes struct reference,https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaFuncAttributes.html#structcudaFuncAttributes,7.27.cudafuncattributes struct reference,8
7.37.cudakernelnodeparams struct reference,https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaKernelNodeParams.html#structcudaKernelNodeParams,7.37.cudakernelnodeparams struct reference,8
6.11.memory management,https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ge8d5c17670f16ac4fc8fcb4181cb490c,6.11.memory management,5
6.16.opengl interoperability,https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__OPENGL.html,6.16.opengl interoperability,9
7.24.4.sub-byte operations,#sub-byte-operations,"7.24.4.sub-byte operations sub-byte wmma operations provide a way to access the low-precision capabilities of tensor cores. they are considered a preview feature i.e. the data structures and apis for them are subject to change and may not be compatible with future releases. this functionality is available via thenvcuda::wmma::experimentalnamespace: for 4 bit precision, the apis available remain the same, but you must specifyexperimental::precision::u4orexperimental::precision::s4as the fragment data type. since the elements of the fragment are packed together,num_storage_elementswill be smaller thannum_elementsfor that fragment. thenum_elementsvariable for a sub-byte fragment, hence returns the number of elements of sub-byte typeelement_type<t>. this is true for single bit precision as well, in which case, the mapping fromelement_type<t>tostorage_element_type<t>is as follows: the allowed layouts for sub-byte fragments is alwaysrow_majorformatrix_aandcol_majorformatrix_b. for sub-byte operations the value ofldminload_matrix_syncshould be a multiple of 32 for element typeexperimental::precision::u4andexperimental::precision::s4or a multiple of 128 for element typeexperimental::precision::b1(i.e., multiple of 16 bytes in both cases).",6
7.24.5.restrictions,#wmma-restrictions,"7.24.5.restrictions the special format required by tensor cores may be different for each major and minor device architecture. this is further complicated by threads holding only a fragment (opaque architecture-specific abi data structure) of the overall matrix, with the developer not allowed to make assumptions on how the individual parameters are mapped to the registers participating in the matrix multiply-accumulate. since fragments are architecture-specific, it is unsafe to pass them from function a to function b if the functions have been compiled for different link-compatible architectures and linked together into the same device executable. in this case, the size and layout of the fragment will be specific to one architecture and using wmma apis in the other will lead to incorrect results or potentially, corruption. an example of two link-compatible architectures, where the layout of the fragment differs, is sm_70 and sm_75. this undefined behavior might also be undetectable at compilation time and by tools at runtime, so extra care is needed to make sure the layout of the fragments is consistent. this linking hazard is most likely to appear when linking with a legacy library that is both built for a different link-compatible architecture and expecting to be passed a wmma fragment. note that in the case of weak linkages (for example, a cuda c++ inline function), the linker may choose any available function definition which may result in implicit passes between compilation units. to avoid these sorts of problems, the matrix should always be stored out to memory for transit through external interfaces (e.g.wmma::store_matrix_sync(dst,);) and then it can be safely passed tobar()as a pointer type [e.g.float*dst]. note that since sm_70 can run on sm_75, the above example sm_75 code can be changed to sm_70 and correctly work on sm_75. however, it is recommended to have sm_75 native code in your application when linking with other sm_75 separately compiled binaries.",5
7.24.6.element types and matrix sizes,#element-types-and-matrix-sizes,"7.24.6.element types and matrix sizes tensor cores support a variety of element types and matrix sizes. the following table presents the various combinations ofmatrix_a,matrix_bandaccumulatormatrix supported: alternate floating point support: double precision support: experimental support for sub-byte operations:",1
7.24.7.example,#wmma-example,7.24.7.example the following code implements a 16x16x16 matrix multiplication in a single warp.,1
7.25.dpx,#dpx,"7.25.dpx dpx is a set of functions that enable finding min and max values, as well as fused addition and min/max, for up to three 16 and 32-bit signed or unsigned integer parameters, with optional relu (clamping to zero): these instructions are hardware-accelerated on devices with compute capability 9 and higher, and software emulation on older devices. full api can be found incuda math api documentation. dpx is exceptionally useful when implementing dynamic programming algorithms, such as smith-waterman or needlemanwunsch in genomics and floyd-warshall in route optimization.",6
7.25.1.examples,#dpx-example,"7.25.1.examples max value of three signed 32-bit integers, with relu min value of the sum of two 32-bit signed integers, another 32-bit signed integer and a zero (relu) min value of two unsigned 32-bit integers and determining which value is smaller max values of three pairs of unsigned 16-bit integers",6
7.26.asynchronous barrier,#asynchronous-barrier,"7.26.asynchronous barrier the nvidia c++ standard library introduces a gpu implementation ofstd::barrier. along with the implementation ofstd::barrierthe library provides extensions that allow users to specify the scope of barrier objects. the barrier api scopes are documented underthread scopes. devices of compute capability 8.0 or higher provide hardware acceleration for barrier operations and integration of these barriers with thememcpy_asyncfeature. on devices with compute capability below 8.0 but starting 7.0, these barriers are available without hardware acceleration. nvcuda::experimental::awbarrieris deprecated in favor ofcuda::barrier.",5
2.8.2.cublas<t>dgmm(),#id10,"2.8.2.cublas<t>dgmm() this function supports the64-bit integer interface. this function performs the matrix-matrix multiplication \(c = \left\{ \begin{matrix}
{a \times diag(x)} & {\text{if }\textsf{mode == $\mathrm{cublas\_side\_right}$}} \\
{diag(x) \times a} & {\text{if }\textsf{mode == $\mathrm{cublas\_side\_left}$}} \\
\end{matrix} \right.\) where\(a\)and\(c\)are matrices stored in column-major format with dimensions\(m \times n\).\(x\)is a vector of size\(n\)ifmode==cublas_side_rightand of size\(m\)ifmode==cublas_side_left.\(x\)is gathered from one-dimensional array x with strideincx. the absolute value ofincxis the stride and the sign ofincxis direction of the stride. ifincxis positive, then we forward x from the first element. otherwise, we backward x from the last element. the formula of x is \(x\lbrack j\rbrack = \left\{ \begin{matrix}
{x\lbrack j \times incx\rbrack} & {\text{if }incx \geq 0} \\
{x\lbrack(\chi - 1) \times |incx| - j \times |incx|\rbrack} & {\text{if }incx < 0} \\
\end{matrix} \right.\) where\(\chi = m\)ifmode==cublas_side_leftand\(\chi = n\)ifmode==cublas_side_right. example 1: if the user wants to perform\(diag(diag(b)) \times a\), then\(incx = ldb + 1\)where\(ldb\)is leading dimension of matrixb, either row-major or column-major. example 2: if the user wants to perform\(\alpha \times a\), then there are two choices, eithercublas<t>geam()with*beta=0andtransa==cublas_op_norcublas<t>dgmm()withincx=0andx[0]=alpha. the operation is out-of-place. the in-place only works iflda=ldc. the possible error values returned by this function and their meanings are listed below.",3
2.8.3.cublas<t>getrfbatched(),#cublas-t-getrfbatched,"2.8.3.cublas<t>getrfbatched() aarrayis an array of pointers to matrices stored in column-major format with dimensionsnxnand leading dimensionlda. this function performs the lu factorization of eachaarray[i]for i = 0, ,batchsize-1by the following equation \(\text{p}\text{*}{aarray}\lbrack i\rbrack = l\text{*}u\) wherepis a permutation matrix which represents partial pivoting with row interchanges.lis a lower triangular matrix with unit diagonal anduis an upper triangular matrix. formallypis written by a product of permutation matricespj, forj=1,2,...,n, sayp=p1*p2*p3*....*pn.pjis a permutation matrix which interchanges two rows of vector x when performingpj*x.pjcan be constructed byjelement ofpivotarray[i]by the following matlab code landuare written back to original matrixa, and diagonal elements oflare discarded. thelanducan be constructed by the following matlab code if matrixa(=aarray[i])is singular, getrf still works and the value ofinfo(=infoarray[i])reports first row index that lu factorization cannot proceed. if info isk,u(k,k)is zero. the equationp*a=l*ustill holds, howeverlandureconstruction needs different matlab code as follows: this function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. cublas<t>getrfbatched supports non-pivot lu factorization ifpivotarrayis null. cublas<t>getrfbatched supports arbitrary dimension. cublas<t>getrfbatched only supports compute capability 2.0 or above. the possible error values returned by this function and their meanings are listed below. for references please refer to: sgeqrf,dgeqrf,cgeqrf,zgeqrf",1
2.8.4.cublas<t>getrsbatched(),#cublas-t-getrsbatched,"2.8.4.cublas<t>getrsbatched() this function solves an array of systems of linear equations of the form: \(\text{op}(a\lbrack i \rbrack) x\lbrack i\rbrack = b\lbrack i\rbrack\) where\(a\lbrack i\rbrack\)is a matrix which has been lu factorized with pivoting,\(x\lbrack i\rbrack\)and\(b\lbrack i\rbrack\)are\(n \times {nrhs}\)matrices. also, for matrix\(a\) \(\text{op}(a\lbrack i\rbrack) = \left\{ \begin{matrix}
{a\lbrack i\rbrack} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_n}$}} \\
{a^{t}\lbrack i\rbrack} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_t}$}} \\
{a^{h}\lbrack i\rbrack} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) this function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. cublas<t>getrsbatchedsupports non-pivot lu factorization ifdevipivis null. cublas<t>getrsbatchedsupports arbitrary dimension. cublas<t>getrsbatchedonly supports compute capability 2.0 or above. the possible error values returned by this function and their meanings are listed below. for references please refer to: sgeqrs,dgeqrs,cgeqrs,zgeqrs",3
2.8.5.cublas<t>getribatched(),#cublas-t-getribatched,"2.8.5.cublas<t>getribatched() aarrayandcarrayare arrays of pointers to matrices stored in column-major format with dimensionsn*nand leading dimensionldaandldcrespectively. this function performs the inversion of matricesa[i]for i = 0, ,batchsize-1. prior to calling cublas<t>getribatched, the matrixa[i]must be factorized first using the routine cublas<t>getrfbatched. after the call of cublas<t>getrfbatched, the matrix pointing byaarray[i]will contain the lu factors of the matrixa[i]and the vector pointing by(pivotarray+i)will contain the pivoting sequence. following the lu factorization, cublas<t>getribatched uses forward and backward triangular solvers to complete inversion of matricesa[i]for i = 0, ,batchsize-1. the inversion is out-of-place, so memory space of carray[i] cannot overlap memory space of array[i]. typically all parameters in cublas<t>getrfbatched would be passed into cublas<t>getribatched. for example, the user can check singularity from either cublas<t>getrfbatched or cublas<t>getribatched. this function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. if cublas<t>getrfbatched is performed by non-pivoting,pivotarrayof cublas<t>getribatched should be null. cublas<t>getribatched supports arbitrary dimension. cublas<t>getribatched only supports compute capability 2.0 or above. the possible error values returned by this function and their meanings are listed below.",3
2.8.6.cublas<t>matinvbatched(),#cublas-t-matinvbatched,"2.8.6.cublas<t>matinvbatched() aandainvare arrays of pointers to matrices stored in column-major format with dimensionsn*nand leading dimensionldaandlda_invrespectively. this function performs the inversion of matricesa[i]for i = 0, ,batchsize-1. this function is a short cut ofcublas<t>getrfbatchedpluscublas<t>getribatched. however it doesnt work ifnis greater than 32. if not, the user has to go throughcublas<t>getrfbatchedandcublas<t>getribatched. if the matrixa[i]is singular, theninfo[i]reports singularity, the same ascublas<t>getrfbatched. the possible error values returned by this function and their meanings are listed below.",3
6.6.event management,https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1gf4fcb74343aa689f4159791967868446,6.6.event management,9
7.40.cuda_memset_node_params_v2 struct reference,https://docs.nvidia.com/cuda/cuda-driver-api/structCUDA__MEMSET__NODE__PARAMS__v2.html#structCUDA__MEMSET__NODE__PARAMS__v2,7.40.cuda_memset_node_params_v2 struct reference,8
7.26.1.simple synchronization pattern,#simple-synchronization-pattern,"7.26.1.simple synchronization pattern without the arrive/wait barrier, synchronization is achieved using__syncthreads()(to synchronize all threads in a block) orgroup.sync()when usingcooperative groups. threads are blocked at the synchronization point (block.sync()) until all threads have reached the synchronization point. in addition, memory updates that happened before the synchronization point are guaranteed to be visible to all threads in the block after the synchronization point, i.e., equivalent toatomic_thread_fence(memory_order_seq_cst,thread_scope_block)as well as thesync. this pattern has three stages:",5
7.26.2.temporal splitting and five stages of synchronization,#temporal-splitting-and-five-stages-of-synchronization,"7.26.2.temporal splitting and five stages of synchronization the temporally-split synchronization pattern with thestd::barrieris as follows. in this pattern, the synchronization point (block.sync()) is split into an arrive point (bar.arrive()) and a wait point (bar.wait(std::move(token))). a thread begins participating in acuda::barrierwith its first call tobar.arrive(). when a thread callsbar.wait(std::move(token))it will be blocked until participating threads have completedbar.arrive()the expected number of times as specified by the expected arrival count argument passed toinit(). memory updates that happen before participating threads call tobar.arrive()are guaranteed to be visible to participating threads after their call tobar.wait(std::move(token)). note that the call tobar.arrive()does not block a thread, it can proceed with other work that does not depend upon memory updates that happen before other participating threads call tobar.arrive(). thearrive and then waitpattern has five stages which may be iteratively repeated:",5
"7.26.3.bootstrap initialization, expected arrival count, and participation",#bootstrap-initialization-expected-arrival-count-and-participation,"7.26.3.bootstrap initialization, expected arrival count, and participation initialization must happen before any thread begins participating in acuda::barrier. before any thread can participate incuda::barrier, the barrier must be initialized usinginit()with anexpected arrival count,block.size()in this example. initialization must happen before any thread callsbar.arrive(). this poses a bootstrapping challenge in that threads must synchronize before participating in thecuda::barrier, but threads are creating acuda::barrierin order to synchronize. in this example, threads that will participate are part of a cooperative group and useblock.sync()to bootstrap initialization. in this example a whole thread block is participating in initialization, hence__syncthreads()could also be used. the second parameter ofinit()is theexpected arrival count, i.e., the number of timesbar.arrive()will be called by participating threads before a participating thread is unblocked from its call tobar.wait(std::move(token)). in the prior example thecuda::barrieris initialized with the number of threads in the thread block i.e.,cooperative_groups::this_thread_block().size(), and all threads within the thread block participate in the barrier. acuda::barrieris flexible in specifying how threads participate (split arrive/wait) and which threads participate. in contrastthis_thread_block.sync()from cooperative groups or__syncthreads()is applicable to whole-thread-block and__syncwarp(mask)is a specified subset of a warp. if the intention of the user is to synchronize a full thread block or a full warp we recommend using__syncthreads()and__syncwarp(mask)respectively for performance reasons.",5
2.8.7.cublas<t>geqrfbatched(),#cublas-t-geqrfbatched,"2.8.7.cublas<t>geqrfbatched() aarrayis an array of pointers to matrices stored in column-major format with dimensionsmxnand leading dimensionlda.tauarrayis an array of pointers to vectors of dimension of at leastmax(1,min(m,n). this function performs the qr factorization of eachaarray[i]fori=0,...,batchsize-1using householder reflections. each matrixq[i]is represented as a product of elementary reflectors and is stored in the lower part of eachaarray[i]as follows : each h[j][i] has the form wheretau[j]is a real scalar, andvis a real vector withv(1:i-1)=0andv(i)=1;v(i+1:m)is stored on exit inaarray[j][i+1:m,i], andtauintauarray[j][i]. this function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. cublas<t>geqrfbatched supports arbitrary dimension. cublas<t>geqrfbatched only supports compute capability 2.0 or above. the possible error values returned by this function and their meanings are listed below. for references please refer to: sgeqrf,dgeqrf,cgeqrf,zgeqrf",3
2.8.8.cublas<t>gelsbatched(),#cublas-t-gelsbatched,"2.8.8.cublas<t>gelsbatched() aarrayis an array of pointers to matrices stored in column-major format.carrayis an array of pointers to matrices stored in column-major format. this function find the least squares solution of a batch of overdetermined systems: it solves the least squares problem described as follows : on exit, eachaarray[i]is overwritten with their qr factorization and eachcarray[i]is overwritten with the least square solution cublas<t>gelsbatched supports only the non-transpose operation and only solves over-determined systems (m >= n). cublas<t>gelsbatched only supports compute capability 2.0 or above. this function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. the possible error values returned by this function and their meanings are listed below. for references please refer to: sgels,dgels,cgels,zgels",3
2.8.9.cublas<t>tpttr(),#cublas-t-tpttr,"2.8.9.cublas<t>tpttr() this function performs the conversion from the triangular packed format to the triangular format ifuplo==cublas_fill_mode_lowerthen the elements ofapare copied into the lower triangular part of the triangular matrixaand the upper part ofais left untouched. ifuplo==cublas_fill_mode_upperthen the elements ofapare copied into the upper triangular part of the triangular matrixaand the lower part ofais left untouched. the possible error values returned by this function and their meanings are listed below. for references please refer to: stpttr,dtpttr,ctpttr,ztpttr",3
"7.26.4.a barriers phase: arrival, countdown, completion, and reset",#a-barrier-s-phase-arrival-countdown-completion-and-reset,"7.26.4.a barriers phase: arrival, countdown, completion, and reset acuda::barriercounts down from the expected arrival count to zero as participating threads callbar.arrive(). when the countdown reaches zero, acuda::barrieris complete for the current phase. when the last call tobar.arrive()causes the countdown to reach zero, the countdown is automatically and atomically reset. the reset assigns the countdown to the expected arrival count, and moves thecuda::barrierto the next phase. atokenobject of classcuda::barrier::arrival_token, as returned fromtoken=bar.arrive(), is associated with the current phase of the barrier. a call tobar.wait(std::move(token))blocks the calling thread while thecuda::barrieris in the current phase, i.e., while the phase associated with the token matches the phase of thecuda::barrier. if the phase is advanced (because the countdown reaches zero) before the call tobar.wait(std::move(token))then the thread does not block; if the phase is advanced while the thread is blocked inbar.wait(std::move(token)), the thread is unblocked. it is essential to know when a reset could or could not occur, especially in non-trivial arrive/wait synchronization patterns. for simple arrive/wait synchronization patterns, compliance with these usage rules is straightforward.",5
7.26.5.spatial partitioning (also known as warp specialization),#spatial-partitioning-also-known-as-warp-specialization,"7.26.5.spatial partitioning (also known as warp specialization) a thread block can be spatially partitioned such that warps are specialized to perform independent computations. spatial partitioning is used in a producer or consumer pattern, where one subset of threads produces data that is concurrently consumed by the other (disjoint) subset of threads. a producer/consumer spatial partitioning pattern requires two one sided synchronizations to manage a data buffer between the producer and consumer. producer threads wait for consumer threads to signal that the buffer is ready to be filled; however, consumer threads do not wait for this signal. consumer threads wait for producer threads to signal that the buffer is filled; however, producer threads do not wait for this signal. for full producer/consumer concurrency this pattern has (at least) double buffering where each buffer requires twocuda::barriers. in this example the first warp is specialized as the producer and the remaining warps are specialized as the consumer. all producer and consumer threads participate (callbar.arrive()orbar.arrive_and_wait()) in each of the fourcuda::barriers so the expected arrival counts are equal toblock.size(). a producer thread waits for the consumer threads to signal that the shared memory buffer can be filled. in order to wait for acuda::barriera producer thread must first arrive on thatready[i%2].arrive()to get a token and thenready[i%2].wait(token)with that token. for simplicityready[i%2].arrive_and_wait()combines these operations. producer threads compute and fill the ready buffer, they then signal that the buffer is filled by arriving on the filled barrier,filled[i%2].arrive(). a producer thread does not wait at this point, instead it waits until the next iterations buffer (double buffering) is ready to be filled. a consumer thread begins by signaling that both buffers are ready to be filled. a consumer thread does not wait at this point, instead it waits for this iterations buffer to be filled,filled[i%2].arrive_and_wait(). after the consumer threads consume the buffer they signal that the buffer is ready to be filled again,ready[i%2].arrive(), and then wait for the next iterations buffer to be filled.",5
2.8.10.cublas<t>trttp(),#cublas-t-trttp,"2.8.10.cublas<t>trttp() this function performs the conversion from the triangular format to the triangular packed format ifuplo==cublas_fill_mode_lowerthen the lower triangular part of the triangular matrixais copied into the arrayap. ifuplo==cublas_fill_mode_upperthen then the upper triangular part of the triangular matrixais copied into the arrayap. the possible error values returned by this function and their meanings are listed below. for references please refer to: strttp,dtrttp,ctrttp,ztrttp",3
7.26.6.early exit (dropping out of participation),#early-exit-dropping-out-of-participation,"7.26.6.early exit (dropping out of participation) when a thread that is participating in a sequence of synchronizations must exit early from that sequence, that thread must explicitly drop out of participation before exiting. the remaining participating threads can proceed normally with subsequentcuda::barrierarrive and wait operations. this operation arrives on thecuda::barrierto fulfill the participating threads obligation to arrive in thecurrentphase, and then decrements the expected arrival count for thenextphase so that this thread is no longer expected to arrive on the barrier.",5
2.8.11.cublas<t>gemmex(),#cublas-t-gemmex,"2.8.11.cublas<t>gemmex() this function supports the64-bit integer interface. this function is an extension ofcublas<t>gemm. in this function the input matrices and output matrices can have a lower precision but the computation is still done in the type<t>. for example, in the typefloatforcublassgemmex()and in the typecucomplexforcublascgemmex(). \(c = \alpha\text{op}(a)\text{op}(b) + \beta c\) where\(\alpha\)and\(\beta\)are scalars, and\(a\),\(b\)and\(c\)are matrices stored in column-major format with dimensions\(\text{op}(a)\)\(m \times k\),\(\text{op}(b)\)\(k \times n\)and\(c\)\(m \times n\), respectively. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) and\(\text{op}(b)\)is defined similarly for matrix\(b\). the matrix types combinations supported forcublassgemmex()are listed below: the matrix types combinations supported forcublascgemmex()are listed below : the possible error values returned by this function and their meanings are listed below. for references please refer to: sgemm for more information about the numerical behavior of some gemm algorithms, refer to thegemm algorithms numerical behaviorsection.",3
2.8.12.cublasgemmex(),#cublasgemmex,"2.8.12.cublasgemmex() this function supports the64-bit integer interface. this function is an extension ofcublas<t>gemmthat allows the user to individually specify the data types for each of the a, b and c matrices, the precision of computation and the gemm algorithm to be run. supported combinations of arguments are listed further down in this section. this function is only supported on devices with compute capability 5.0 or later. \(c = \alpha\text{op}(a)\text{op}(b) + \beta c\) where\(\alpha\)and\(\beta\)are scalars, and\(a\),\(b\)and\(c\)are matrices stored in column-major format with dimensions\(\text{op}(a)\)\(m \times k\),\(\text{op}(b)\)\(k \times n\)and\(c\)\(m \times n\), respectively. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) and\(\text{op}(b)\)is defined similarly for matrix\(b\). cublasgemmex()supports the following compute type, scale type, atype/btype, and ctype: the possible error values returned by this function and their meanings are listed in the following table. starting with release 11.2, using the typed functions instead of the extension functions (cublas**ex()) helps in reducing the binary size when linking to static cublas library. also refer to:sgemm. for more information about the numerical behavior of some gemm algorithms, refer to thegemm algorithms numerical behaviorsection.",3
2.8.13.cublasgemmbatchedex(),#cublasgemmbatchedex,"2.8.13.cublasgemmbatchedex() this function supports the64-bit integer interface. this function is an extension ofcublas<t>gemmbatchedthat performs the matrix-matrix multiplication of a batch of matrices and allows the user to individually specify the data types for each of the a, b and c matrix arrays, the precision of computation and the gemm algorithm to be run. likecublas<t>gemmbatched, the batch is considered to be uniform, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective a, b and c matrices. the address of the input matrices and the output matrix of each instance of the batch are read from arrays of pointers passed to the function by the caller. supported combinations of arguments are listed further down in this section. \(c\lbrack i\rbrack = \alpha\text{op}(a\lbrack i\rbrack)\text{op}(b\lbrack i\rbrack) + \beta c\lbrack i\rbrack,\text{ for i } \in \lbrack 0,batchcount - 1\rbrack\) where\(\alpha\)and\(\beta\)are scalars, and\(a\),\(b\)and\(c\)are arrays of pointers to matrices stored in column-major format with dimensions\(\text{op}(a\lbrack i\rbrack)\)\(m \times k\),\(\text{op}(b\lbrack i\rbrack)\)\(k \times n\)and\(c\lbrack i\rbrack\)\(m \times n\), respectively. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) and\(\text{op}(b\lbrack i\rbrack)\)is defined similarly for matrix\(b\lbrack i\rbrack\). on certain problem sizes, it might be advantageous to make multiple calls tocublas<t>gemmin different cuda streams, rather than use this api. cublasgemmbatchedex()supports the following compute type, scale type, atype/btype, and ctype: ifatypeiscuda_r_16forcuda_r_16bf, orcomputetypeis any of thefastoptions, or when math mode oralgoenable fast math modes, pointers (not the pointer arrays) placed in the gpu memory must be properly aligned to avoid misaligned memory access errors. ideally all pointers are aligned to at least 16 bytes. otherwise it is recommended that they meet the following rule: the possible error values returned by this function and their meanings are listed below. also refer to:sgemm.",3
2.8.14.cublasgemmstridedbatchedex(),#cublasgemmstridedbatchedex,"2.8.14.cublasgemmstridedbatchedex() this function supports the64-bit integer interface. this function is an extension ofcublas<t>gemmstridedbatchedthat performs the matrix-matrix multiplication of a batch of matrices and allows the user to individually specify the data types for each of the a, b and c matrices, the precision of computation and the gemm algorithm to be run. likecublas<t>gemmstridedbatched, the batch is considered to be uniform, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective a, b and c matrices. input matrices a, b and output matrix c for each instance of the batch are located at fixed offsets in number of elements from their locations in the previous instance. pointers to a, b and c matrices for the first instance are passed to the function by the user along with the offsets in number of elements - stridea, strideb and stridec that determine the locations of input and output matrices in future instances. \(c + i*{stridec} = \alpha\text{op}(a + i*{stridea})\text{op}(b + i*{strideb}) + \beta(c + i*{stridec}),\text{ for i } \in \lbrack 0,batchcount - 1\rbrack\) where\(\alpha\)and\(\beta\)are scalars, and\(a\),\(b\)and\(c\)are arrays of pointers to matrices stored in column-major format with dimensions\(\text{op}(a\lbrack i\rbrack)\)\(m \times k\),\(\text{op}(b\lbrack i\rbrack)\)\(k \times n\)and\(c\lbrack i\rbrack\)\(m \times n\), respectively. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) and\(\text{op}(b\lbrack i\rbrack)\)is defined similarly for matrix\(b\lbrack i\rbrack\). on certain problem sizes, it might be advantageous to make multiple calls tocublas<t>gemmin different cuda streams, rather than use this api. cublasgemmstridedbatchedex()supports the following compute type, scale type, atype/btype, and ctype: the possible error values returned by this function and their meanings are listed below. also refer to:sgemm.",3
2.8.15.cublasgemmgroupedbatchedex(),#cublasgemmgroupedbatchedex,"2.8.15.cublasgemmgroupedbatchedex() this function supports the64-bit integer interface. this function performs the matrix-matrix multiplication on groups of matrices. a given group is considered to be uniform, i.e. all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective a, b and c matrices. however, the dimensions, leading dimensions, transpositions, and scaling factors (alpha, beta) may vary between groups. the address of the input matrices and the output matrix of each instance of the batch are read from arrays of pointers passed to the function by the caller.  this is functionally equivalent to the following: where\(\text{$\mathrm{alpha\_array}$}\)and\(\text{$\mathrm{beta\_array}$}\)are arrays of scaling factors, and\(\text{aarray}\),\(\text{barray}\)and\(\text{carray}\)are arrays of pointers to matrices stored in column-major format.  for a given index,\(\text{idx}\), that is part of group\(i\), the dimensions are: for matrix\(a[\text{idx}]\)in group\(i\) \(\text{op}(a[\text{idx}]) = \left\{ \begin{matrix}
a[\text{idx}] & {\text{if }\textsf{$\mathrm{transa\_array}\lbrack i\rbrack$ == $\mathrm{cublas\_op\_n}$}} \\
a[\text{idx}]^{t} & {\text{if }\textsf{$\mathrm{transa\_array}\lbrack i\rbrack$ == $\mathrm{cublas\_op\_t}$}} \\
a[\text{idx}]^{h} & {\text{if }\textsf{$\mathrm{transa\_array}\lbrack i\rbrack$ == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) and\(\text{op}(b[\text{idx}])\)is defined similarly for matrix\(b[\text{idx}]\)in group\(i\). on certain problem sizes, it might be advantageous to make multiple calls tocublasgemmbatchedex()in different cuda streams, rather than use this api. cublasgemmgroupedbatchedex()supports the following compute type, scale type, atype/btype, and ctype: ifatypeiscuda_r_16forcuda_r_16bfor if thecomputetypeis any of thefastoptions, pointers (not the pointer arrays) placed in the gpu memory must be properly aligned to avoid misaligned memory access errors. ideally all pointers are aligned to at least 16 bytes. otherwise it is required that they meet the following rule: the possible error values returned by this function and their meanings are listed below.",3
2.8.16.cublascsyrkex(),#cublascsyrkex,"2.8.16.cublascsyrkex() this function supports the64-bit integer interface. this function is an extension ofcublascsyrk()where the input matrix and output matrix can have a lower precision but the computation is still done in the typecucomplex this function performs the symmetric rank-\(k\)update \(c = \alpha\text{op}(a)\text{op}(a)^{t} + \beta c\) where\(\alpha\)and\(\beta\)are scalars,\(c\)is a symmetric matrix stored in lower or upper mode, and\(a\)is a matrix with dimensions\(\text{op}(a)\)\(n \times k\). also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
\end{matrix} \right.\) the matrix types combinations supported forcublascsyrkex()are listed below: the possible error values returned by this function and their meanings are listed below. for references please refer to: ssyrk,dsyrk,csyrk,zsyrk",3
7.26.7.completion function,#completion-function,"7.26.7.completion function thecompletionfunctionofcuda::barrier<scope,completionfunction>is executed once per phase, after the last threadarrivesand before any thread is unblocked from thewait. memory operations performed by the threads that arrived at thebarrierduring the phase are visible to the thread executing thecompletionfunction, and all memory operations performed within thecompletionfunctionare visible to all threads waiting at thebarrieronce they are unblocked from thewait.",5
7.26.8.memory barrier primitives interface,#memory-barrier-primitives-interface,7.26.8.memory barrier primitives interface memory barrier primitives are c-like interfaces tocuda::barrierfunctionality. these primitives are available through including the<cuda_awbarrier_primitives.h>header.,6
7.27.asynchronous data copies,#asynchronous-data-copies,7.27.asynchronous data copies cuda 11 introduces asynchronous data operations withmemcpy_asyncapi to allow device code to explicitly manage the asynchronous copying of data. thememcpy_asyncfeature enables cuda kernels to overlap computation with data movement.,5
2.8.17.cublascsyrk3mex(),#cublascsyrk3mex,"2.8.17.cublascsyrk3mex() this function supports the64-bit integer interface. this function is an extension ofcublascsyrk()where the input matrix and output matrix can have a lower precision but the computation is still done in the typecucomplex. this routine is implemented using the gauss complexity reduction algorithm which can lead to an increase in performance up to 25% this function performs the symmetric rank-\(k\)update \(c = \alpha\text{op}(a)\text{op}(a)^{t} + \beta c\) where\(\alpha\)and\(\beta\)are scalars,\(c\)is a symmetric matrix stored in lower or upper mode, and\(a\)is a matrix with dimensions\(\text{op}(a)\)\(n \times k\). also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
\end{matrix} \right.\) the matrix types combinations supported forcublascsyrk3mex()are listed below : the possible error values returned by this function and their meanings are listed below. for references please refer to: ssyrk,dsyrk,csyrk,zsyrk",3
2.8.18.cublascherkex(),#cublascherkex,"2.8.18.cublascherkex() this function supports the64-bit integer interface. this function is an extension ofcublascherk()where the input matrix and output matrix can have a lower precision but the computation is still done in the typecucomplex this function performs the hermitian rank-\(k\)update \(c = \alpha\text{op}(a)\text{op}(a)^{h} + \beta c\) where\(\alpha\)and\(\beta\)are scalars,\(c\)is a hermitian matrix stored in lower or upper mode, and\(a\)is a matrix with dimensions\(\text{op}(a)\)\(n \times k\). also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) the matrix types combinations supported forcublascherkex()are listed in the following table: the possible error values returned by this function and their meanings are listed below. for references please refer to: cherk",3
7.27.1.memcpy_asyncapi,#memcpy-async-api,"7.27.1.memcpy_asyncapi thememcpy_asyncapis are provided in thecuda/barrier,cuda/pipeline, andcooperative_groups/memcpy_async.hheader files. thecuda::memcpy_asyncapis work withcuda::barrierandcuda::pipelinesynchronization primitives, while thecooperative_groups::memcpy_asyncsynchronizes usingcoopertive_groups::wait. these apis have very similar semantics: copy objects fromsrctodstas-if performed by another thread which, on completion of the copy, can be synchronized throughcuda::pipeline,cuda::barrier, orcooperative_groups::wait. the complete api documentation of thecuda::memcpy_asyncoverloads forcuda::barrierandcuda::pipelineis provided in thelibcudacxx apidocumentation along with some examples. the api documentation ofcooperative_groups::memcpy_asyncis provided in thecooperative groupssection of the documentation. thememcpy_asyncapis that usecuda::barrierandcuda::pipelinerequire compute capability 7.0 or higher. on devices with compute capability 8.0 or higher,memcpy_asyncoperations from global to shared memory can benefit from hardware acceleration.",5
2.8.19.cublascherk3mex(),#cublascherk3mex,"2.8.19.cublascherk3mex() this function supports the64-bit integer interface. this function is an extension ofcublascherk()where the input matrix and output matrix can have a lower precision but the computation is still done in the typecucomplex. this routine is implemented using the gauss complexity reduction algorithm which can lead to an increase in performance up to 25% this function performs the hermitian rank-\(k\)update \(c = \alpha\text{op}(a)\text{op}(a)^{h} + \beta c\) where\(\alpha\)and\(\beta\)are scalars,\(c\)is a hermitian matrix stored in lower or upper mode, and\(a\)is a matrix with dimensions\(\text{op}(a)\)\(n \times k\). also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) the matrix types combinations supported forcublascherk3mex()are listed in the following table: the possible error values returned by this function and their meanings are listed below. for references please refer to: cherk",3
7.27.2.copy and compute pattern - staging data through shared memory,#copy-and-compute-pattern-staging-data-through-shared-memory,7.27.2.copy and compute pattern - staging data through shared memory cuda applications often employ acopy and computepattern that: the following sections illustrate how this pattern can be expressed without and with thememcpy_asyncfeature:,5
7.27.3.withoutmemcpy_async,#without-memcpy-async,"7.27.3.withoutmemcpy_async withoutmemcpy_async, thecopyphase of thecopy and computepattern is expressed asshared[local_idx]=global[global_idx]. this global to shared memory copy is expanded to a read from global memory into a register, followed by a write to shared memory from the register. when this pattern occurs within an iterative algorithm, each thread block needs to synchronize after theshared[local_idx]=global[global_idx]assignment, to ensure all writes to shared memory have completed before the compute phase can begin. the thread block also needs to synchronize again after the compute phase, to prevent overwriting shared memory before all threads have completed their computations. this pattern is illustrated in the following code snippet.",5
7.27.4.withmemcpy_async,#with-memcpy-async,"7.27.4.withmemcpy_async withmemcpy_async, the assignment of shared memory from global memory is replaced with an asynchronous copy operation fromcooperative groups thecooperative_groups::memcpy_asyncapi copiessizeof(int)*block.size()bytes from global memory starting atglobal_in+batch_idxto theshareddata. this operation happens as-if performed by another thread, which synchronizes with the current threads call tocooperative_groups::waitafter the copy has completed. until the copy operation completes, modifying the global data or reading or writing the shared data introduces a data race. on devices with compute capability 8.0 or higher,memcpy_asynctransfers from global to shared memory can benefit from hardware acceleration, which avoids transfering the data through an intermediate register.",5
7.27.5.asynchronous data copies usingcuda::barrier,#asynchronous-data-copies-using-cuda-barrier,"7.27.5.asynchronous data copies usingcuda::barrier thecuda::memcpy_asyncoverload forcuda::barrierenables synchronizing asynchronous data transfers using abarrier. this overloads executes the copy operation as-if performed by another thread bound to the barrier by: incrementing the expected count of the current phase on creation, and decrementing it on completion of the copy operation, such that the phase of thebarrierwill only advance when all threads participating in the barrier have arrived, and allmemcpy_asyncbound to the current phase of the barrier have completed. the following example uses a block-widebarrier, where all block threads participate, and swaps the wait operation with a barrierarrive_and_wait, while providing the same functionality as the previous example:",5
2.8.20.cublasnrm2ex(),#cublasnrm2ex,"2.8.20.cublasnrm2ex() this function supports the64-bit integer interface. this function is an api generalization of the routinecublas<t>nrm2where input data, output data and compute type can be specified independently. this function computes the euclidean norm of the vectorx. the code uses a multiphase model of accumulation to avoid intermediate underflow and overflow, with the result being equivalent to\(\sqrt{\sum_{i = 1}^{n}\left( {\mathbf{x}\lbrack j\rbrack \times \mathbf{x}\lbrack j\rbrack} \right)}\)where\(j = 1 + \left( {i - 1} \right)*\text{incx}\)in exact arithmetic. notice that the last equation reflects 1-based indexing used for compatibility with fortran. the datatypes combinations currently supported forcublasnrm2ex()are listed below : the possible error values returned by this function and their meanings are listed below. for references please refer to: snrm2,dnrm2,scnrm2,dznrm2",3
2.8.21.cublasaxpyex(),#cublasaxpyex,"2.8.21.cublasaxpyex() this function supports the64-bit integer interface. this function is an api generalization of the routinecublas<t>axpywhere input data, output data and compute type can be specified independently. this function multiplies the vectorxby the scalar\(\alpha\)and adds it to the vectoryoverwriting the latest vector with the result. hence, the performed operation is\(\mathbf{y}\lbrack j\rbrack = \alpha \times \mathbf{x}\lbrack k\rbrack + \mathbf{y}\lbrack j\rbrack\)for\(i = 1,\ldots,n\),\(k = 1 + \left( {i - 1} \right)*\text{incx}\)and\(j = 1 + \left( {i - 1} \right)*\text{incy}\). notice that the last two equations reflect 1-based indexing used for compatibility with fortran. the datatypes combinations currently supported forcublasaxpyex()are listed in the following table: the possible error values returned by this function and their meanings are listed below. for references please refer to: saxpy,daxpy,caxpy,zaxpy",3
2.8.22.cublasdotex(),#cublasdotex,"2.8.22.cublasdotex() these functions support the64-bit integer interface. these functions are an api generalization of the routinescublas<t>dotandcublas<t>dotcwhere input data, output data and compute type can be specified independently. note:cublas<t>dotcis dot product conjugated,cublas<t>dotuis dot product unconjugated. this function computes the dot product of vectorsxandy. hence, the result is\(\sum_{i = 1}^{n}\left( {\mathbf{x}\lbrack k\rbrack \times \mathbf{y}\lbrack j\rbrack} \right)\)where\(k = 1 + \left( {i - 1} \right)*\text{incx}\)and\(j = 1 + \left( {i - 1} \right)*\text{incy}\). notice that in the first equation the conjugate of the element of vector x should be used if the function name ends in character c and that the last two equations reflect 1-based indexing used for compatibility with fortran. the datatypes combinations currently supported forcublasdotex()andcublasdotcex()are listed below: the possible error values returned by this function and their meanings are listed in the following table: for references please refer to: sdot,ddot,cdotu,cdotc,zdotu,zdotc",3
2.8.23.cublasrotex(),#cublasrotex,"2.8.23.cublasrotex() this function supports the64-bit integer interface. this function is an extension to the routinecublas<t>rotwhere input data, output data, cosine/sine type, and compute type can be specified independently. this function applies givens rotation matrix (i.e., rotation in the x,y plane counter-clockwise by angle defined by cos(alpha)=c, sin(alpha)=s): \(g = \begin{pmatrix}
c & s \\
{- s} & c \\
\end{pmatrix}\) to vectorsxandy. hence, the result is\(\mathbf{x}\lbrack k\rbrack = c \times \mathbf{x}\lbrack k\rbrack + s \times \mathbf{y}\lbrack j\rbrack\)and\(\mathbf{y}\lbrack j\rbrack = - s \times \mathbf{x}\lbrack k\rbrack + c \times \mathbf{y}\lbrack j\rbrack\)where\(k = 1 + \left( {i - 1} \right)*\text{incx}\)and\(j = 1 + \left( {i - 1} \right)*\text{incy}\). notice that the last two equations reflect 1-based indexing used for compatibility with fortran. the datatypes combinations currently supported forcublasrotex()are listed below : the possible error values returned by this function and their meanings are listed below. for references please refer to: srot,drot,crot,csrot,zrot,zdrot",3
2.8.24.cublasscalex(),#cublasscalex,"2.8.24.cublasscalex() this function supports the64-bit integer interface. this function scales the vectorxby the scalar\(\alpha\)and overwrites it with the result. hence, the performed operation is\(\mathbf{x}\lbrack j\rbrack = \alpha \times \mathbf{x}\lbrack j\rbrack\)for\(i = 1,\ldots,n\)and\(j = 1 + \left( {i - 1} \right)*\text{incx}\). notice that the last two equations reflect 1-based indexing used for compatibility with fortran. the datatypes combinations currently supported forcublasscalex()are listed below : the possible error values returned by this function and their meanings are listed below. for references please refer to: sscal,dscal,csscal,cscal,zdscal,zscal",3
3.using the cublaslt api,#using-the-cublaslt-api,3.using the cublaslt api,3
7.27.6.performance guidance formemcpy_async,#performance-guidance-for-memcpy-async,"7.27.6.performance guidance formemcpy_async for compute capability 8.x, the pipeline mechanism is shared among cuda threads in the same cuda warp. this sharing causes batches ofmemcpy_asyncto be entangled within a warp, which can impact performance under certain circumstances. this section highlights the warp-entanglement effect oncommit,wait, andarriveoperations. please refer to thepipeline interfaceand thepipeline primitives interfacefor an overview of the individual operations.",5
3.1.general description,#id41,"3.1.general description the cublaslt library is a new lightweight library dedicated to general matrix-to-matrix multiply (gemm) operations with a new flexible api. this new library adds flexibility in matrix data layouts, input types, compute types, and also in choosing the algorithmic implementations and heuristics through parameter programmability. once a set of options for the intended gemm operation are identified by the user, these options can be used repeatedly for different inputs. this is analogous to how cufft and fftw first create a plan and reuse for same size and type ffts with different input data.",3
3.1.1.problem size limitations,#problem-size-limitations,"3.1.1.problem size limitations there are inherent problem size limitations that are a result of limitations in cuda grid dimensions.  for example, many kernels do not support batch sizes greater than 65535 due to a limitation on thezdimension of a grid.  there are similar restriction on the m and n values for a given problem. in cases where a problem cannot be run by a single kernel, cublaslt will attempt to decompose the problem into multiple sub-problems and solve it by running the kernel on each sub-problem. to overcome these limitations, a user may want to partition the problem themself, launch kernels for each sub-problem, and compute any necessary reductions to combine the results.",5
3.1.2.heuristics cache,#heuristics-cache,"3.1.2.heuristics cache cublaslt uses heuristics to pick the most suitable matmul kernel for execution based on the problem sizes, gpu configuration, and other parameters. this requires performing some computations on the host cpu, which could take tens of microseconds. to overcome this overhead, it is recommended to query the heuristics once usingcublasltmatmulalgogetheuristic()and then reuse the result for subsequent computations usingcublasltmatmul(). for the cases where querying heuristics once and then reusing them is not feasible, cublaslt implements a heuristics cache that maps matmul problems to kernels previously selected by heuristics. the heuristics cache uses an lru-like eviction policy and is thread-safe. the user can control the heuristics cache capacity with thecublaslt_heuristics_cache_capacityenvironment variable or with thecublasltheuristicscachesetcapacity()function which has higher precedence. the capacity is measured in number of entries and might be rounded up to the nearest multiple of some factor for performance reasons. each entry takes about 360 bytes but is subject to change. the default capacity is 8192 entries. see also:cublasltheuristicscachegetcapacity(),cublasltheuristicscachesetcapacity().",3
3.1.3.cublaslt logging,#cublaslt-logging,"3.1.3.cublaslt logging cublaslt logging mechanism can be enabled by setting the following environment variables before launching the target application: ifcublaslt_log_fileis not defined, the log messages are printed to stdout. another option is to use the experimental cublaslt logging api. see: cublasltloggersetcallback(),cublasltloggersetfile(),cublasltloggeropenfile(),cublasltloggersetlevel(),cublasltloggersetmask(),cublasltloggerforcedisable()",3
7.28.asynchronous data copies usingcuda::pipeline,#asynchronous-data-copies-using-cuda-pipeline,"7.28.asynchronous data copies usingcuda::pipeline cuda provides thecuda::pipelinesynchronization object to manage and overlap asynchronous data movement with computation. the api documentation forcuda::pipelineis provided in thelibcudacxx api. a pipeline object is a double-ended n stage queue with aheadand atail, and is used to process work in a first-in first-out (fifo) order. the pipeline object has following member functions to manage the stages of the pipeline.",5
7.28.1.single-stage asynchronous data copies usingcuda::pipeline,#single-stage-asynchronous-data-copies-using-cuda-pipeline,"7.28.1.single-stage asynchronous data copies usingcuda::pipeline in previous examples we showed how to usecooperative_groupsandcuda::barrierto do asynchronous data transfers. in this section, we will use thecuda::pipelineapi with a single stage to schedule asynchronous copies. and later we will expand this example to show multi staged overlapped compute and copy.",5
3.1.4.8-bit floating point data types (fp8) usage,#bit-floating-point-data-types-fp8-usage,"3.1.4.8-bit floating point data types (fp8) usage fp8 was first introduced with ada and hopper gpus (compute capability 8.9 and above) and is designed to further accelerate matrix multiplications. there are two types of fp8 available: in order to maintain accurate fp8 matrix multiplications, we define native compute fp8 matrix multiplication as follows: where a, b, and c are input matrices, and scalea, scaleb, scalec, scaled, alpha, and beta are input scalars. this differs from the other matrix multiplication routines because of this addition of scaling factors for each matrix. the scalea, scaleb, and scalec are used for de-quantization, and scaled is used for quantization. note that all the scaling factors are applied multiplicatively. this means that sometimes it is necessary to use a scaling factor or its reciprocal depending on the context in which it is applied. for more information on fp8, seecublasltmatmul()andcublasltmatmuldescattributes_t. for fp8 matrix multiplications, epilogues and amaxd may be computed as follows: here aux is an auxiliary output of an epilogue function like gelu, scaleaux is an optional scaling factor that can be applied to aux, and amaxaux is the maximum absolute value in aux before scaling. for more information, see attributescublaslt_matmul_desc_amax_d_pointerandcublaslt_matmul_desc_epilogue_aux_amax_pointerincublasltmatmuldescattributes_t.",1
7.28.2.multi-stage asynchronous data copies usingcuda::pipeline,#multi-stage-asynchronous-data-copies-using-cuda-pipeline,"7.28.2.multi-stage asynchronous data copies usingcuda::pipeline in the previous examples withcooperative_groups::waitandcuda::barrier, the kernel threads immediately wait for the data transfer to shared memory to complete. this avoids data transfers from global memory into registers, but does nothidethe latency of thememcpy_asyncoperation by overlapping computation. for that we use the cudapipelinefeature in the following example. it provides a mechanism for managing a sequence ofmemcpy_asyncbatches, enabling cuda kernels to overlap memory transfers with computation. the following example implements a two-stage pipeline that overlaps data-transfer with computation. it: note that, for interoperability withcuda::pipeline,cuda::memcpy_asyncfrom thecuda/pipelineheader is used here. apipeline objectis a double-ended queue with aheadand atail, and is used to process work in a first-in first-out (fifo) order. producer threads commit work to the pipelines head, while consumer threads pull work from the pipelines tail. in the example above, all threads are both producer and consumer threads. the threads firstcommitmemcpy_asyncoperations to fetch thenextbatch while theywaiton thepreviousbatch ofmemcpy_asyncoperations to complete. cuda::pipeline_shared_state<scope,count>encapsulates the finite resources that allow a pipeline to process up tocountconcurrent stages. if all resources are in use,pipeline.producer_acquire()blocks producer threads until the resources of the next pipeline stage are released by consumer threads. this example can be written in a more concise manner by merging the prolog and epilog of the loop with the loop itself as follows: thepipeline<thread_scope_block>primitive used above is very flexible, and supports two features that our examples above are not using: any arbitrary subset of threads in the block can participate in thepipeline, and from the threads that participate, any subsets can be producers, consumers, or both. in the following example, threads with an even thread rank are producers, while other threads are consumers: there are some optimizations thatpipelineperforms, for example, when all threads are both producers and consumers, but in general, the cost of supporting all these features cannot be fully eliminated. for example,pipelinestores and uses a set of barriers in shared memory for synchronization, which is not really necessary if all threads in the block participate in the pipeline. for the particular case in which all threads in the block participate in thepipeline, we can do better thanpipeline<thread_scope_block>by using apipeline<thread_scope_thread>combined with__syncthreads(): if thecomputeoperation only reads shared memory written to by other threads in the same warp as the current thread,__syncwarp()suffices.",5
3.1.5.disabling cpu instructions,#disabling-cpu-instructions,"3.1.5.disabling cpu instructions as mentioned in theheuristics cachesection, cublaslt heuristics perform some compute-intensive operations on the host cpu.
to speed-up the operations, the implementation detects cpu capabilities and may use special instructions, such as advanced vector extensions (avx) on x86-64 cpus.
however, in some rare cases this might be not desirable. for instance, using advanced instructions may result in cpu running at a lower frequency, which would affect performance of the other host code. the user can optionally instruct the cublaslt library to not use some cpu instructions with thecublaslt_disable_cpu_instructions_maskenvironment variable or with thecublasltdisablecpuinstructionssetmask()function which has higher precedence.
the default mask is 0, meaning that there are no restrictions. please checkcublasltdisablecpuinstructionssetmask()for more information.",3
3.1.6.atomics synchronization,#atomics-synchronization,"3.1.6.atomics synchronization atomics synchronization allows optimizing matmul workloads by enablingcublasltmatmul()to have a producer or consumer relationship with another concurrently running kernel. this allows overlapping computation and communication with a finer granularity. conceptually, matmul is provided with an array containing 32-bit integer counters, and then: the array of counters are passed to matmuls via thecublaslt_matmul_desc_atomic_sync_in_counters_pointerandcublaslt_matmul_desc_atomic_sync_out_counters_pointercompute descriptor attributes for the consumer and producer modes respectively3. the arrays must have a sufficient number of elements for all the chunks. the number of chunks is controlled bycublaslt_matmul_desc_atomic_sync_num_chunks_d_rowsandcublaslt_matmul_desc_atomic_sync_num_chunks_d_colscompute descriptor attributes. both of these attributes must be set to a value greater than zero for the feature to be enabled. for the column-major layout, the number of chunks must satisfy: for row-major layout, m and n in tile size and cluster shape must be swapped. these restrictions mean that it is required to first query heuristic viacublasltmatmulalgogetheuristic()and inspect the result for tile and cluster shapes, and only then set the number of chunks. the pseudocode below shows the principles of operation: it should be noted that, in general, cuda programming model provides few kernel co-scheduling guarantees. thus, use of this feature requires careful orchestration of producer and consumer kernels launch order and resource availability, as it easy to create a deadlock situation. a deadlock may occur in the following cases (this is not an exhaustive list):",5
7.28.3.pipeline interface,#pipeline-interface,"7.28.3.pipeline interface the complete api documentation forcuda::memcpy_asyncis provided in thelibcudacxx apidocumentation along with some examples. thepipelineinterface requires for a c-like interface, when compiling without iso c++ 2011 compatibility, seepipeline primitives interface.",6
7.28.4.pipeline primitives interface,#pipeline-primitives-interface,"7.28.4.pipeline primitives interface pipeline primitives are a c-like interface formemcpy_asyncfunctionality. the pipeline primitives interface is available by including the<cuda_pipeline.h>header. when compiling without iso c++ 2011 compatibility, include the<cuda_pipeline_primitives.h>header.",6
7.29.asynchronous data copies using tensor memory access (tma),#asynchronous-data-copies-using-tensor-memory-access-tma,"7.29.asynchronous data copies using tensor memory access (tma) many applications require movement of large amounts of data from and to global
memory. often, the data is laid out in global memory as a multi-dimensional
array with non-sequential data acess patterns. to reduce global memory usage,
sub-tiles of such arrays are copied to shared memory before use in computations.
the loading and storing involves addreses-calculations that can be error-prone
and repetitive. to offload these computations, compute capability 9.0 introduces
tensor memory acces (tma). the primary goal of tma is to provide an efficient
data transfer mechanism from global memory to shared memory for
multi-dimensional arrays. naming. tensor memory access (tma) is a broad term used to market the
features described in this section. for the purpose of forward-compatibility and
to reduce discrepancies with the ptx isa, the text in this section refers to tma
operations as either bulk-asynchronous copies or bulk tensor asynchronous
copies, depending on the specific type of copy used. the term bulk is used to
contrast these operations with the asynchronous memory operations described in
the previous sections. dimensions. tma supports copying both one-dimensional and multi-dimensional
arrays (up to 5-dimensional). the programming model forbulk-asynchronous
copiesof one-dimensional contiguous arrays is different from the programming
model forbulk tensor asynchronous copiesof multi-dimensional arrays. to
perform a bulk tensor asynchronous copy of a multi-dimensional array, the
hardware requires atensor map.
this object describes the layout of the multi-dimensional array in global and
shared memory. a tensor map is created on the host using thecutensormapencode
api.
the tensor map is transferred from host to device as aconstkernel
parameter annotated with__grid_constant__, and can be used on the device to
copy a tile of data between shared and global memory. in contrast, performing a
bulk-asynchronous copy of a contiguous one-dimensional array does not require a
tensor map: it can be performed on-device with a pointer and size parameter. source and destination. the source and destination addresses of bulk-asynchronous copy
operations can be in shared or global memory. the operations can read data from global to
shared memory, write data from shared to global memory, and also copy from
shared memory todistributed shared memoryof another block in the same cluster.
in addition, when in a cluster, a bulk-asynchronous operation can be specified as being
multicast. in this case, data can be transferred from global memory to the
shared memory of multiple blocks within the cluster. the multicast feature is
optimized for target architecturesm_90aand may havesignificantly reduced performanceon
other targets. hence, it is advised to be used withcompute architecturesm_90a. asynchronous. data transfers using tma areasynchronous. this allows the initiating
thread to continue computing while the hardware asynchronously copies the data.whether the data transfer occurs asynchronously in practice is up to the hardware implementation and may change in the future.
there are severalcompletion mechanismsthat bulk-asynchronous operations can use to signal that they have completed.
when the operation reads from global to shared memory, any
thread in the block can wait for the data to be readable in shared memory by
waiting on ashared memory barrier. when the bulk-asynchronous
operation writes data from shared memory to global or distributed shared memory,
only the initiating thread can wait for the operation to have completed.
this is accomplished using abulk async-groupbased completion mechanism. a
table describing the completion mechanisms can be found below and in theptx isa.",5
7.29.1.using tma to transfer one-dimensional arrays,#using-tma-to-transfer-one-dimensional-arrays,"7.29.1.using tma to transfer one-dimensional arrays this section demonstrates how to write a simple kernel that read-modify-writes a
one-dimensional array using tma. this shows how to how to load and store data
using bulk-asynchronous copies, as well as how to synchronize threads of
execution with those copies. the code of the kernel is included below. some functionality requires inline ptx
assembly that is currently made available throughlibcu++.
the availability of these wrappers can be
checked with the following code: the kernel goes through the following stages: barrier initialization. the barrier is initialized with the number of
threads participating in the block. as a result, the barrier will flip only if
all threads have arrived on this barrier. shared memory barriers are described
in more detail inasynchronous data copies using cuda::barrier.
to make the initialized barrier visible to subsequent bulk-asynchronous copies, thefence.proxy.async.shared::ctainstruction is used. this instruction ensures that
subsequent bulk-asynchronous copy operations operate on the initialized barrier. tma read. the bulk-asynchronous copy instruction directs the
hardware to copy a large chunk of data into shared memory, and to update thetransaction countof the shared memory barrier after completing the read. in general, issuing as
few bulk copies with as big a size as possible results in the best performance.
because the copy can be performed asynchronously by the hardware, it is not
necessary to split the copy into smaller chunks. the thread that initiates the bulk-asynchronous copy operation arrives at the barrier
usingmbarrier.expect_tx. this is automatically performed bycuda::memcpy_async. this tells the barrier that the thread has
arrived and also how many bytes (tx / transactions) are expected to arrive. only
a single thread has to update the expected transaction count. if multiple
threads update the transaction count, the expected transaction will be the sum
of the updates. the barrier will only flip once all threads have arrivedandall bytes have arrived. once the barrier has flipped, the bytes are safe to read
from shared memory, both by the threads as well as by subsequent
bulk-asynchronous copies. more information about barrier transaction accounting
can be found in theptx isa. barrier wait. waiting for the barrier to flip is done usingmbarrier.try_wait. it can either return true, indicating that the wait is
over, or return false, which may mean that the wait timed out. the while loop
waits for completion, and retries on time-out. smem write and sync. the increment of the buffer values reads and writes to shared
memory. to make the writes visible to subsequent bulk-asynchronous copies, thefence.proxy.async.shared::ctainstruction is used. this orders the writes to
shared memory before subsequent reads from bulk-asynchronous copy operations,
which read through the async proxy. so each thread first orders the writes to
objects in shared memory in the async proxy via thefence.proxy.async.shared::cta, and these operations by all threads are
ordered before the async operation performed in thread 0 using__syncthreads(). tma write and sync. the write from shared to global memory is again
initiated by a single thread. the completion of the write is not tracked by a
shared memory barrier. instead, a thread-local mechanism is used. multiple
writes can be batched into a so-calledbulk async-group. afterwards, the
thread can wait for all operations in this group to have completed reading from
shared memory (as in the code above) or to have completed writing to global
memory, making the writes visible to the initiating thread. for more information,
refer to the ptx isa documentation ofcp.async.bulk.wait_group.
note that the bulk-asynchronous and non-bulk asynchronous copy instructions have
different async-groups: there exist bothcp.async.wait_groupandcp.async.bulk.wait_groupinstructions. the bulk-asynchronous instructions have specific alignment requirements on their source and
destination addresses. more information can be found in the table below.",5
3.2.cublaslt code examples,#cublaslt-code-examples,3.2.cublaslt code examples please visithttps://github.com/nvidia/cudalibrarysamples/tree/master/cublasltfor updated code examples.,3
3.3.cublaslt datatypes reference,#cublaslt-datatypes-reference,3.3.cublaslt datatypes reference,3
7.29.2.using tma to transfer multi-dimensional arrays,#using-tma-to-transfer-multi-dimensional-arrays,"7.29.2.using tma to transfer multi-dimensional arrays the primary difference between the one-dimensional and multi-dimensional case is
that a tensor map must be created on the host and passed to the cuda kernel.
this section describes how to create a tensor map using the cuda driver api, how
to pass it to device, and how to use it on device. driver api. a tensor map is created using thecutensormapencodetileddriver api. this api can be accessed by linking to the driver directly
(-lcuda) or by using thecudagetdriverentrypointapi. below, we show how to get a pointer to thecutensormapencodetiledapi.
for more information, refer todriver entry point access. creation. creating a tensor map requires many parameters. among
them are the base pointer to an array in global memory, the size of the array
(in number of elements), the stride from one row to the next (in bytes), the
size of the shared memory buffer (in number of elements). the code below creates
a tensor map to describe a two-dimensional row-major array of sizegmem_heightxgmem_width. note the order of the parameters: the fastest moving dimension
comes first. host-to-device transfer. a bulk tensor asynchronous operations require the
tensor map to be in immutable memory. this can be achieved by usingconstantmemory or by passing the tensor map as aconst__grid_constant__parameter
to a kernel. when passing the tensor map as a parameter, some versions of the
gcc c++ compiler issue the warning the abi for passing parameters with 64-byte
alignment has changed in gcc 4.6. this warning can be ignored. as an alternative to the__grid_constant__kernel parameter, a globalconstantvariable can be used. an example is included
below. the following example copies the tensor map to global device memory. using a
pointer to a tensor map in global device memory is undefined behavior and will
lead to silent and difficult to track down bugs. use. the kernel below loads a 2d tile of sizesmem_heightxsmem_widthfrom a larger 2d array. the top-left corner of the tile is indicated by the
indicesxandy. the tile is loaded into shared memory, modified, and
written back to global memory. negative indices and out of bounds. when part of the tile that is beingreadfrom global to shared memory is out of bounds, the shared memory that
corresponds to the out of bounds area is zero-filled. the top-left corner
indices of the tile may also be negative. whenwritingfrom shared to global
memory, parts of the tile may be out of bounds, but the top left corner cannot
have any negative indices. size and stride. the size of a tensor is the number of elements along one
dimension. all sizes must be greater than one. the stride is the number of bytes
between elements of the same dimension. for instance, a4 x 4matrix of
integers has sizes 4 and 4. since it has 4 bytes per element, the strides are 4
and 16 bytes. due to alignment requirements, a4 x 3row-major matrix of
integers must have strides of 4 and 16 bytes as well. each row is padded with 4
extra bytes to ensure that the start of the next row is aligned to 16 bytes. for
more information regarding alignment, refer to tablealignment requirements for multi-dimensional bulk tensor asynchronous copy operations in compute capability 9.0..",5
7.30.profiler counter function,#profiler-counter-function,"7.30.profiler counter function each multiprocessor has a set of sixteen hardware counters that an application can increment with a single instruction by calling the__prof_trigger()function. increments by one per warp the per-multiprocessor hardware counter of indexcounter. counters 8 to 15 are reserved and should not be used by applications. the value of counters 0, 1, , 7 can be obtained vianvprofbynvprof--eventsprof_trigger_0xwherexis 0, 1, , 7. all counters are reset before each kernel launch (note that when collecting counters, kernel launches are synchronous as mentioned inconcurrent execution between host and device).",5
7.31.assertion,#assertion,"7.31.assertion assertion is only supported by devices of compute capability 2.x and higher. stops the kernel execution ifexpressionis equal to zero. if the program is run within a debugger, this triggers a breakpoint and the debugger can be used to inspect the current state of the device. otherwise, each thread for whichexpressionis equal to zero prints a message tostderrafter synchronization with the host viacudadevicesynchronize(),cudastreamsynchronize(), orcudaeventsynchronize(). the format of this message is as follows: any subsequent host-side synchronization calls made for the same device will returncudaerrorassert. no more commands can be sent to this device untilcudadevicereset()is called to reinitialize the device. ifexpressionis different from zero, the kernel execution is unaffected. for example, the following program from source filetest.cu will output: assertions are for debugging purposes. they can affect performance and it is therefore recommended to disable them in production code. they can be disabled at compile time by defining thendebugpreprocessor macro before includingassert.h. note thatexpressionshould not be an expression with side effects (something like(++i>0), for example), otherwise disabling the assertion will affect the functionality of the code.",5
7.32.trap function,#trap-function,7.32.trap function a trap operation can be initiated by calling the__trap()function from any device thread. the execution of the kernel is aborted and an interrupt is raised in the host program.,6
7.33.breakpoint function,#breakpoint-function,7.33.breakpoint function execution of a kernel function can be suspended by calling the__brkpt()function from any device thread.,6
7.34.formatted output,#formatted-output,"7.34.formatted output formatted output is only supported by devices of compute capability 2.x and higher. prints formatted output from a kernel to a host-side output stream. the in-kernelprintf()function behaves in a similar way to the standard c-libraryprintf()function, and the user is referred to the host systems manual pages for a complete description ofprintf()behavior. in essence, the string passed in asformatis output to a stream on the host, with substitutions made from the argument list wherever a format specifier is encountered. supported format specifiers are listed below. theprintf()command is executed as any other device-side function: per-thread, and in the context of the calling thread. from a multi-threaded kernel, this means that a straightforward call toprintf()will be executed by every thread, using that threads data as specified. multiple versions of the output string will then appear at the host stream, once for each thread which encountered theprintf(). it is up to the programmer to limit the output to a single thread if only a single output string is desired (seeexamplesfor an illustrative example). unlike the c-standardprintf(), which returns the number of characters printed, cudasprintf()returns the number of arguments parsed. if no arguments follow the format string, 0 is returned. if the format string is null, -1 is returned. if an internal error occurs, -2 is returned.",6
7.34.1.format specifiers,#format-specifiers,"7.34.1.format specifiers as for standardprintf(), format specifiers take the form:%[flags][width][.precision][size]type the following fields are supported (see widely-available documentation for a complete description of all behaviors): note that cudasprintf()will accept any combination of flag, width, precision, size and type, whether or not overall they form a valid format specifier. in other words, %hd will be accepted and printf will expect a double-precision variable in the corresponding location in the argument list.",6
7.34.2.limitations,#limitations,"7.34.2.limitations final formatting of theprintf()output takes place on the host system. this means that the format string must be understood by the host-systems compiler and c library. every effort has been made to ensure that the format specifiers supported by cudas printf function form a universal subset from the most common host compilers, but exact behavior will be host-os-dependent. as described informat specifiers,printf()will acceptallcombinations of valid flags and types. this is because it cannot determine what will and will not be valid on the host system where the final output is formatted. the effect of this is that output may be undefined if the program emits a format string which contains invalid combinations. theprintf()command can accept at most 32 arguments in addition to the format string. additional arguments beyond this will be ignored, and the format specifier output as-is. owing to the differing size of thelongtype on 64-bit windows platforms (four bytes on 64-bit windows platforms, eight bytes on other 64-bit platforms), a kernel which is compiled on a non-windows 64-bit machine but then run on a win64 machine will see corrupted output for all format strings which include %ld. it is recommended that the compilation platform matches the execution platform to ensure safety. the output buffer forprintf()is set to a fixed size before kernel launch (seeassociated host-side api). it is circular and if more output is produced during kernel execution than can fit in the buffer, older output is overwritten. it is flushed only when one of these actions is performed: note that the buffer is not flushed automatically when the program exits. the user must callcudadevicereset()orcuctxdestroy()explicitly, as shown in the examples below. internallyprintf()uses a shared data structure and so it is possible that callingprintf()might change the order of execution of threads. in particular, a thread which callsprintf()might take a longer execution path than one which does not callprintf(), and that path length is dependent upon the parameters of theprintf(). note, however, that cuda makes no guarantees of thread execution order except at explicit__syncthreads()barriers, so it is impossible to tell whether execution order has been modified byprintf()or by other scheduling behavior in the hardware.",6
7.34.3.associated host-side api,#associated-host-side-api,7.34.3.associated host-side api the following api functions get and set the size of the buffer used to transfer theprintf()arguments and internal metadata to the host (default is 1 megabyte):,6
3.3.1.cublasltclustershape_t,#cublasltclustershape-t,"3.3.1.cublasltclustershape_t cublasltclustershape_tis an enumerated type used to configure thread block cluster dimensions. thread block clusters add an optional hierarchical level and are made up of thread blocks. similar to thread blocks, these can be one, two, or three-dimensional. see alsothread block clusters.",3
3.3.2.cublasltepilogue_t,#cublasltepilogue-t,3.3.2.cublasltepilogue_t thecublasltepilogue_tis an enum type to set the postprocessing options for the epilogue. notes:,3
3.3.3.cublaslthandle_t,#cublaslthandle-t,"3.3.3.cublaslthandle_t thecublaslthandle_ttype is a pointer type to an opaque structure holding the cublaslt  library context. usecublasltcreate()to initialize the cublaslt library context and return a handle to an opaque structure holding the cublaslt library context, and usecublasltdestroy()to destroy a previously created cublaslt library context descriptor and release the resources.",3
3.3.4.cublasltloggercallback_t,#cublasltloggercallback-t,3.3.4.cublasltloggercallback_t cublasltloggercallback_tis a callback function pointer type. a callback function can be set usingcublasltloggersetcallback(). parameters:,3
7.34.4.examples,#format-specifier-examples,"7.34.4.examples the following code sample: will output: notice how each thread encounters theprintf()command, so there are as many lines of output as there were threads launched in the grid. as expected, global values (i.e.,floatf) are common between all threads, and local values (i.e.,threadidx.x) are distinct per-thread. the following code sample: will output: self-evidently, theif()statement limits which threads will callprintf, so that only a single line of output is seen.",6
3.3.5.cublasltmatmulalgo_t,#cublasltmatmulalgo-t,3.3.5.cublasltmatmulalgo_t cublasltmatmulalgo_tis an opaque structure holding the description of the matrix multiplication algorithm. this structure can be trivially serialized and later restored for use with the same version of cublas library to save on selecting the right configuration again.,3
3.3.6.cublasltmatmulalgocapattributes_t,#cublasltmatmulalgocapattributes-t,3.3.6.cublasltmatmulalgocapattributes_t cublasltmatmulalgocapattributes_tenumerates matrix multiplication algorithm capability attributes that can be retrieved from an initializedcublasltmatmulalgo_tdescriptor usingcublasltmatmulalgocapgetattribute().,3
3.3.7.cublasltmatmulalgoconfigattributes_t,#cublasltmatmulalgoconfigattributes-t,"3.3.7.cublasltmatmulalgoconfigattributes_t cublasltmatmulalgoconfigattributes_tis an enumerated type that contains the configuration attributes for cublaslt matrix multiply algorithms. the configuration attributes are algorithm-specific, and can be set. the attributes configuration of a given algorithm should agree with its capability attributes. usecublasltmatmulalgoconfiggetattribute()andcublasltmatmulalgoconfigsetattribute()to get and set the attribute value of a matmul algorithm descriptor.",3
3.3.8.cublasltmatmuldesc_t,#cublasltmatmuldesc-t,3.3.8.cublasltmatmuldesc_t thecublasltmatmuldesc_tis a pointer to an opaque structure holding the description of the matrix multiplication operationcublasltmatmul(). a descriptor can be created by callingcublasltmatmuldesccreate()and destroyed by callingcublasltmatmuldescdestroy().,3
3.3.9.cublasltmatmuldescattributes_t,#cublasltmatmuldescattributes-t,3.3.9.cublasltmatmuldescattributes_t cublasltmatmuldescattributes_tis a descriptor structure containing the attributes that define the specifics of the matrix multiply operation. usecublasltmatmuldescgetattribute()andcublasltmatmuldescsetattribute()to get and set the attribute value of a matmul descriptor.,3
3.3.10.cublasltmatmulheuristicresult_t,#cublasltmatmulheuristicresult-t,3.3.10.cublasltmatmulheuristicresult_t cublasltmatmulheuristicresult_tis a descriptor that holds the configured matrix multiplication algorithm descriptor and its runtime properties.,3
3.3.11.cublasltmatmulinnershape_t,#cublasltmatmulinnershape-t,3.3.11.cublasltmatmulinnershape_t cublasltmatmulinnershape_tis an enumerated type used to configure various aspects of the internal kernel design. this does not impact the cuda grid size.,3
3.3.12.cublasltmatmulpreference_t,#cublasltmatmulpreference-t,3.3.12.cublasltmatmulpreference_t thecublasltmatmulpreference_tis a pointer to an opaque structure holding the description of the preferences forcublasltmatmulalgogetheuristic()configuration. usecublasltmatmulpreferencecreate()to create one instance of the descriptor andcublasltmatmulpreferencedestroy()to destroy a previously created descriptor and release the resources.,3
3.3.13.cublasltmatmulpreferenceattributes_t,#cublasltmatmulpreferenceattributes-t,3.3.13.cublasltmatmulpreferenceattributes_t cublasltmatmulpreferenceattributes_tis an enumerated type used to apply algorithm search preferences while fine-tuning the heuristic function. usecublasltmatmulpreferencegetattribute()andcublasltmatmulpreferencesetattribute()to get and set the attribute value of a matmul preference descriptor.,3
3.3.14.cublasltmatmulsearch_t,#cublasltmatmulsearch-t,3.3.14.cublasltmatmulsearch_t cublasltmatmulsearch_tis an enumerated type that contains the attributes for heuristics search type.,3
3.3.15.cublasltmatmultile_t,#cublasltmatmultile-t,3.3.15.cublasltmatmultile_t cublasltmatmultile_tis an enumerated type used to set the tile size inrowsxcolumns.see alsocutlass: fast linear algebra in cuda c++.,3
7.35.dynamic global memory allocation and operations,#dynamic-global-memory-allocation-and-operations,"7.35.dynamic global memory allocation and operations dynamic global memory allocation and operations are only supported by devices of compute capability 2.x and higher. allocate and free memory dynamically from a fixed-size heap in global memory. copysizebytes from the memory location pointed bysrcto the memory location pointed bydest. setsizebytes of memory block pointed byptrtovalue(interpreted as an unsigned char). the cuda in-kernelmalloc()function allocates at leastsizebytes from the device heap and returns a pointer to the allocated memory or null if insufficient memory exists to fulfill the request. the returned pointer is guaranteed to be aligned to a 16-byte boundary. the cuda in-kernel__nv_aligned_device_malloc()function allocates at leastsizebytes from the device heap and returns a pointer to the allocated memory or null if insufficient memory exists to fulfill the requested size or alignment. the address of the allocated memory will be a multiple ofalign.alignmust be a non-zero power of 2. the cuda in-kernelfree()function deallocates the memory pointed to byptr, which must have been returned by a previous call tomalloc()or__nv_aligned_device_malloc(). ifptris null, the call tofree()is ignored. repeated calls tofree()with the sameptrhas undefined behavior. the memory allocated by a given cuda thread viamalloc()or__nv_aligned_device_malloc()remains allocated for the lifetime of the cuda context, or until it is explicitly released by a call tofree(). it can be used by any other cuda threads even from subsequent kernel launches. any cuda thread may free memory allocated by another thread, but care should be taken to ensure that the same pointer is not freed more than once.",5
3.3.16.cublasltmatmulstages_t,#cublasltmatmulstages-t,3.3.16.cublasltmatmulstages_t cublasltmatmulstages_tis an enumerated type used to configure the size and number of shared memory buffers where input elements are staged. number of staging buffers defines kernels pipeline depth.,3
7.35.1.heap memory allocation,#heap-memory-allocation,"7.35.1.heap memory allocation the device memory heap has a fixed size that must be specified before any program usingmalloc(),__nv_aligned_device_malloc()orfree()is loaded into the context. a default heap of eight megabytes is allocated if any program usesmalloc()or__nv_aligned_device_malloc()without explicitly specifying the heap size. the following api functions get and set the heap size: the heap size granted will be at leastsizebytes.cuctxgetlimit()andcudadevicegetlimit()return the currently requested heap size. the actual memory allocation for the heap occurs when a module is loaded into the context, either explicitly via the cuda driver api (seemodule), or implicitly via the cuda runtime api (seecuda runtime). if the memory allocation fails, the module load will generate acuda_error_shared_object_init_failederror. heap size cannot be changed once a module load has occurred and it does not resize dynamically according to need. memory reserved for the device heap is in addition to memory allocated through host-side cuda api calls such ascudamalloc().",5
7.35.2.interoperability with host memory api,#interoperability-with-host-memory-api,"7.35.2.interoperability with host memory api memory allocated via devicemalloc()or__nv_aligned_device_malloc()cannot be freed using the runtime (i.e., by calling any of the free memory functions fromdevice memory). similarly, memory allocated via the runtime (i.e., by calling any of the memory allocation functions fromdevice memory) cannot be freed viafree(). in addition, memory allocated by a call tomalloc()or__nv_aligned_device_malloc()in device code cannot be used in any runtime or driver api calls (i.e. cudamemcpy, cudamemset, etc).",5
7.35.3.examples,#examples-per-thread,7.35.3.examples,9
7.36.execution configuration,#execution-configuration,"7.36.execution configuration any call to a__global__function must specify theexecution configurationfor that call. the execution configuration defines the dimension of the grid and blocks that will be used to execute the function on the device, as well as the associated stream (seecuda runtimefor a description of streams). the execution configuration is specified by inserting an expression of the form<<<dg,db,ns,s>>>between the function name and the parenthesized argument list, where: as an example, a function declared as must be called like this: the arguments to the execution configuration are evaluated before the actual function arguments. the function call will fail ifdgordbare greater than the maximum sizes allowed for the device as specified incompute capabilities, or ifnsis greater than the maximum amount of shared memory available on the device, minus the amount of shared memory required for static allocation. compute capability 9.0 and above allows users to specify compile time thread block cluster dimensions, so that the kernel can use the cluster hierarchy in cuda. compile time cluster dimension can be specified using__cluster_dims__([x,[y,[z]]]). the example below shows compile time cluster size of 2 in x dimension and 1 in y and z dimension. thread block cluster dimensions can also be specified at runtime and kernel with the cluster can be launched usingcudalaunchkernelexapi. the api takes a configuration arugument of typecudalaunchconfig_t, kernel function pointer and kernel arguments. runtime kernel configuration is shown in the example below.",5
7.37.launch bounds,#launch-bounds,"7.37.launch bounds as discussed in detail inmultiprocessor level, the fewer registers a kernel uses, the more threads and thread blocks are likely to reside on a multiprocessor, which can improve performance. therefore, the compiler uses heuristics to minimize register usage while keeping register spilling (seedevice memory accesses) and instruction count to a minimum. an application can optionally aid these heuristics by providing additional information to the compiler in the form of launch bounds that are specified using the__launch_bounds__()qualifier in the definition of a__global__function: if launch bounds are specified, the compiler first derives from them the upper limitlon the number of registers the kernel should use to ensure thatminblockspermultiprocessorblocks (or a single block ifminblockspermultiprocessoris not specified) ofmaxthreadsperblockthreads can reside on the multiprocessor (seehardware multithreadingfor the relationship between the number of registers used by a kernel and the number of registers allocated per block). the compiler then optimizes register usage in the following way: a kernel will fail to launch if it is executed with more threads per block than its launch boundmaxthreadsperblock. a kernel will fail to launch if it is executed with more thread blocks per cluster than its launch boundmaxblockspercluster. per thread resources required by a cuda kernel might limit the maximum block size in an unwanted way. in order to maintain forward compatibility to future hardware and toolkits and to ensure that at least one thread block can run on an sm, developers should include the single argument__launch_bounds__(maxthreadsperblock)which specifies the largest block size that the kernel will be launched with. failure to do so could lead to too many resources requested for launch errors. providing the two argument version of__launch_bounds__(maxthreadsperblock,minblockspermultiprocessor)can improve performance in some cases. the right value forminblockspermultiprocessorshould be determined using a detailed per kernel analysis. optimal launch bounds for a given kernel will usually differ across major architecture revisions. the sample code below shows how this is typically handled in device code using the__cuda_arch__macro introduced inapplication compatibility in the common case wheremykernelis invoked with the maximum number of threads per block (specified as the first parameter of__launch_bounds__()), it is tempting to usemy_kernel_max_threadsas the number of threads per block in the execution configuration: this will not work however since__cuda_arch__is undefined in host code as mentioned inapplication compatibility, somykernelwill launch with 256 threads per block even when__cuda_arch__is greater or equal to 200. instead the number of threads per block should be determined: register usage is reported by the--ptxas-options=-vcompiler option. the number of resident blocks can be derived from the occupancy reported by the cuda profiler (seedevice memory accessesfor a definition of occupancy). the__launch_bounds__()and__maxnreg__()qualifiers cannot be applied to the same kernel. register usage can also be controlled for all__global__functions in a file using themaxrregcountcompiler option. the value ofmaxrregcountis ignored for functions with launch bounds.",5
7.38.maximum number of registers per thread,#maximum-number-of-registers-per-thread,"7.38.maximum number of registers per thread to provide a mechanism for low-level performance tuning, cuda c++ provides the__maxnreg()__function qualifier to pass performance tuning information to the backend optimizing compiler. the__maxnreg__()qualifier specifies the maximum number of registers to be allocated to a single thread in a thread block. in the definition of a__global__function: the__launch_bounds__()and__maxnreg__()qualifiers cannot be applied to the same kernel. register usage can also be controlled for all__global__functions in a file using themaxrregcountcompiler option. the value ofmaxrregcountis ignored for functions with the__maxnreg__qualifier.",5
7.39.pragma unroll,#pragma-unroll,"7.39.pragma unroll by default, the compiler unrolls small loops with a known trip count. thepragmaunrolldirective however can be used to control unrolling of any given loop. it must be placed immediately before the loop and only applies to that loop. it is optionally followed by an integral constant expression (ice)13. if the ice is absent, the loop will be completely unrolled if its trip count is constant. if the ice evaluates to 1, the compiler will not unroll the loop. the pragma will be ignored if the ice evaluates to a non-positive integer or to an integer greater than the maximum value representable by theintdata type. examples:",6
7.40.simd video instructions,#simd-video-instructions,"7.40.simd video instructions ptx isa version 3.0 includes simd (single instruction, multiple data) video instructions which operate on pairs of 16-bit values and quads of 8-bit values. these are available on devices of compute capability 3.0. the simd video instructions are: ptx instructions, such as the simd video instructions, can be included in cuda programs by way of the assembler,asm(), statement. the basic syntax of anasm()statement is: an example of using thevabsdiff4ptx instruction is: this uses thevabsdiff4instruction to compute an integer quad byte simd sum of absolute differences. the absolute difference value is computed for each byte of the unsigned integers a and b in simd fashion. the optional accumulate operation (.add) is specified to sum these differences. refer to the document using inline ptx assembly in cuda for details on using the assembly statement in your code. refer to the ptx isa documentation (parallel thread execution isa version 3.0 for example) for details on the ptx instructions for the version of ptx that you are using.",7
7.41.diagnostic pragmas,#diagnostic-pragmas,"7.41.diagnostic pragmas the following pragmas may be used to control the error severity used when a given diagnostic message is issued. uses of these pragmas have the following form: the diagnostic affected is specified using an error number showed in a warning message. any diagnostic may be overridden to be an error, but only warnings may have their severity suppressed or be restored to a warning after being promoted to an error. thenv_diag_defaultpragma is used to return the severity of a diagnostic to the one that was in effect before any pragmas were issued (i.e., the normal severity of the message as modified by any command-line options). the following example suppresses the""declaredbutneverreferenced""warning on the declaration offoo: the following pragmas may be used to save and restore the current diagnostic pragma state: examples: note that the pragmas only affect the nvcc cuda frontend compiler; they have no effect on the host compiler. removal notice: the support of diagnostic pragmas withoutnv_prefix are removed from cuda 12.0, if the pragmas are inside the device code, warningunrecognizedpragmaindevicecodewill be emitted, otherwise they will be passed to the host compiler. if they are intended for cuda code, use the pragmas withnv_prefix instead.",6
8.cooperative groups,#cooperative-groups,8.cooperative groups,9
8.1.introduction,#introduction-cg,"8.1.introduction cooperative groups is an extension to the cuda programming model, introduced in cuda 9, for organizing groups of communicating threads. cooperative groups allows developers to express the granularity at which threads are communicating, helping them to express richer, more efficient parallel decompositions. historically, the cuda programming model has provided a single, simple construct for synchronizing cooperating threads: a barrier across all threads of a thread block, as implemented with the__syncthreads()intrinsic function. however, programmers would like to define and synchronize groups of threads at other granularities to enable greater performance, design flexibility, and software reuse in the form of collective group-wide function interfaces. in an effort to express broader patterns of parallel interaction, many performance-oriented programmers have resorted to writing their own ad hoc and unsafe primitives for synchronizing threads within a single warp, or across sets of thread blocks running on a single gpu. whilst the performance improvements achieved have often been valuable, this has resulted in an ever-growing collection of brittle code that is expensive to write, tune, and maintain over time and across gpu generations. cooperative groups addresses this by providing a safe and future-proof mechanism to enable performant code.",5
8.2.whats new in cooperative groups,#what-s-new-in-cooperative-groups,8.2.whats new in cooperative groups,9
8.2.1.cuda 12.2,#cuda-12-2,8.2.1.cuda 12.2,0
8.2.2.cuda 12.1,#cuda-12-1,8.2.2.cuda 12.1,0
8.2.3.cuda 12.0,#cuda-12-0,8.2.3.cuda 12.0,0
8.3.programming model concept,#programming-model-concept,"8.3.programming model concept the cooperative groups programming model describes synchronization patterns both within and across cuda thread blocks. it provides both the means for applications to define their own groups of threads, and the interfaces to synchronize them. it also provides new launch apis that enforce certain restrictions and therefore can guarantee the synchronization will work. these primitives enable new patterns of cooperative parallelism within cuda, including producer-consumer parallelism, opportunistic parallelism, and global synchronization across the entire grid. the cooperative groups programming model consists of the following elements: the main concept in cooperative groups is that of objects naming the set of threads that are part of it. this expression of groups as first-class program objects improves software composition, since collective functions can receive an explicit object representing the group of participating threads. this object also makes programmer intent explicit, which eliminates unsound architectural assumptions that result in brittle code, undesirable restrictions upon compiler optimizations, and better compatibility with new gpu generations. to write efficient code, its best to use specialized groups (going generic loses a lot of compile time optimizations), and pass these group objects by reference to functions that intend to use these threads in some cooperative fashion. cooperative groups requires cuda 9.0 or later. to use cooperative groups, include the header file: and use the cooperative groups namespace: the code can be compiled in a normal way using nvcc, however if you wish to use memcpy_async, reduce or scan functionality and your host compilers default dialect is not c++11 or higher, then you must add--std=c++11to the command line.",5
3.3.17.cublasltnumericalimplflags_t,#cublasltnumericalimplflags-t,3.3.17.cublasltnumericalimplflags_t cublasltnumericalimplflags_t: a set of bit-flags that can be specified to select implementation details that may affect numerical behavior of algorithms. flags below can be combined using the bit or operator |.,3
3.3.18.cublasltmatrixlayout_t,#cublasltmatrixlayout-t,3.3.18.cublasltmatrixlayout_t thecublasltmatrixlayout_tis a pointer to an opaque structure holding the description of a matrix layout. usecublasltmatrixlayoutcreate()to create one instance of the descriptor andcublasltmatrixlayoutdestroy()to destroy a previously created descriptor and release the resources.,3
3.3.19.cublasltmatrixlayoutattribute_t,#cublasltmatrixlayoutattribute-t,3.3.19.cublasltmatrixlayoutattribute_t cublasltmatrixlayoutattribute_tis a descriptor structure containing the attributes that define the details of the matrix operation. usecublasltmatrixlayoutgetattribute()andcublasltmatrixlayoutsetattribute()to get and set the attribute value of a matrix layout descriptor.,3
3.3.20.cublasltmatrixtransformdesc_t,#cublasltmatrixtransformdesc-t,3.3.20.cublasltmatrixtransformdesc_t thecublasltmatrixtransformdesc_tis a pointer to an opaque structure holding the description of a matrix transformation operation. usecublasltmatrixtransformdesccreate()to create one instance of the descriptor andcublasltmatrixtransformdescdestroy()to destroy a previously created descriptor and release the resources.,3
3.3.21.cublasltmatrixtransformdescattributes_t,#cublasltmatrixtransformdescattributes-t,3.3.21.cublasltmatrixtransformdescattributes_t cublasltmatrixtransformdescattributes_tis a descriptor structure containing the attributes that define the specifics of the matrix transform operation. usecublasltmatrixtransformdescgetattribute()andcublasltmatrixtransformdescsetattribute()to set the attribute value of a matrix transform descriptor.,3
3.3.22.cublasltorder_t,#cublasltorder-t,3.3.22.cublasltorder_t cublasltorder_tis an enumerated type used to indicate the data ordering of the matrix.,3
3.3.23.cublasltpointermode_t,#cublasltpointermode-t,3.3.23.cublasltpointermode_t cublasltpointermode_tis an enumerated type used to set the pointer mode for the scaling factorsalphaandbeta.,3
3.3.24.cublasltpointermodemask_t,#cublasltpointermodemask-t,3.3.24.cublasltpointermodemask_t cublasltpointermodemask_tis an enumerated type used to define and query the pointer mode capability.,3
3.3.25.cublasltreductionscheme_t,#cublasltreductionscheme-t,"3.3.25.cublasltreductionscheme_t cublasltreductionscheme_tis an enumerated type used to specify a reduction scheme for the portions of the dot-product calculated in parallel (i.e., split - k).",3
3.4.cublaslt api reference,#cublaslt-api-reference,3.4.cublaslt api reference,3
3.4.1.cublasltcreate(),#cublasltcreate,"3.4.1.cublasltcreate() this function initializes the cublaslt library and creates a handle to an opaque structure holding the cublaslt library context. it allocates light hardware resources on the host and device, and must be called prior to making any other cublaslt library calls. the cublaslt library context is tied to the current cuda device. to use the library on multiple devices, one cublaslt handle should be created for each device. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.",3
3.4.2.cublasltdestroy(),#cublasltdestroy,"3.4.2.cublasltdestroy() this function releases hardware resources used by the cublaslt library. this function is usually the last call with a particular handle to the cublaslt library. becausecublasltcreate()allocates some internal resources and the release of those resources by callingcublasltdestroy()will implicitly callcudadevicesynchronize(), it is recommended to minimize the number of times these functions are called. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.",3
3.4.3.cublasltdisablecpuinstructionssetmask(),#cublasltdisablecpuinstructionssetmask,"3.4.3.cublasltdisablecpuinstructionssetmask() instructs cublaslt library to not usecpu instructionsspecified by the flags in themask.
the function takes precedence over thecublaslt_disable_cpu_instructions_maskenvironment variable. parameters:mask the flags combined with bitwiseor(|)operator that specify which cpu instructions should not be used. supported flags: returns:the previous value of themask.",3
3.4.4.cublasltgetcudartversion(),#cublasltgetcudartversion,3.4.4.cublasltgetcudartversion() this function returns the version number of the cuda runtime library. parameters:none. returns:size_t- the version number of the cuda runtime library.,0
3.4.5.cublasltgetproperty(),#cublasltgetproperty,3.4.5.cublasltgetproperty() this function returns the value of the requested property by writing it to the memory location pointed to by the value parameter. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.6.cublasltgetstatusname(),#cublasltgetstatusname,3.4.6.cublasltgetstatusname() returns the string representation of a given status. parameters:cublasstatus_t- the status. returns:constchar*- the null-terminated string.,3
3.4.7.cublasltgetstatusstring(),#cublasltgetstatusstring,3.4.7.cublasltgetstatusstring() returns the description string for a given status. parameters:cublasstatus_t- the status. returns:constchar*- the null-terminated string.,3
3.4.8.cublasltheuristicscachegetcapacity(),#cublasltheuristicscachegetcapacity,3.4.8.cublasltheuristicscachegetcapacity() returns theheuristics cachecapacity. parameters: returns:,3
3.4.9.cublasltheuristicscachesetcapacity(),#cublasltheuristicscachesetcapacity,3.4.9.cublasltheuristicscachesetcapacity() sets theheuristics cachecapacity. set the capacity to 0 to disable the heuristics cache. this function takes precedence overcublaslt_heuristics_cache_capacityenvironment variable. parameters: returns:,3
3.4.10.cublasltgetversion(),#cublasltgetversion,3.4.10.cublasltgetversion() this function returns the version number of cublaslt library. parameters:none. returns:size_t- the version number of cublaslt library.,3
3.4.11.cublasltloggersetcallback(),#cublasltloggersetcallback,3.4.11.cublasltloggersetcallback() experimental: this function sets the logging callback function. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.12.cublasltloggersetfile(),#cublasltloggersetfile,"3.4.12.cublasltloggersetfile() experimental: this function sets the logging output file. note: once registered using this function call, the provided file handle must not be closed unless the function is called again to switch to a different file handle. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.",3
3.4.13.cublasltloggeropenfile(),#cublasltloggeropenfile,3.4.13.cublasltloggeropenfile() experimental: this function opens a logging output file in the given path. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.14.cublasltloggersetlevel(),#cublasltloggersetlevel,3.4.14.cublasltloggersetlevel() experimental: this function sets the value of the logging level. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.15.cublasltloggersetmask(),#cublasltloggersetmask,3.4.15.cublasltloggersetmask() experimental: this function sets the value of the logging mask. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.16.cublasltloggerforcedisable(),#cublasltloggerforcedisable,3.4.16.cublasltloggerforcedisable() experimental: this function disables logging for the entire run. returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.17.cublasltmatmul(),#cublasltmatmul,"3.4.17.cublasltmatmul() this function computes the matrix multiplication of matrices a and b to produce the output matrix d, according to the following operation: d=alpha*(a*b)+beta*(c), wherea,b, andcare input matrices, andalphaandbetaare input scalars. theworkspacepointer must be aligned to at least a multiple of 256 bytes.
the recommendations onworkspacesizeinbytesare the same as mentioned in thecublassetworkspace()section. datatypes supported: cublasltmatmul()supports the following computetype, scaletype, atype/btype, and ctype. footnotes can be found at the end of this section. to use imma kernels, one of the following sets of requirements, with the first being the preferred one, must be met: to use fp8 kernels, the following set of requirements must be satisfied: see the table below when using fp8 kernels: and finally, see below table when a,b,c,d are planar-complex matrices (cublaslt_matrix_layout_plane_offset!=0, seecublasltmatrixlayoutattribute_t) to make use of mixed precision tensor core acceleration. notes: parameters: returns: seecublasstatus_tfor a complete list of valid return codes.",3
3.4.18.cublasltmatmulalgocapgetattribute(),#cublasltmatmulalgocapgetattribute,"3.4.18.cublasltmatmulalgocapgetattribute() this function returns the value of the queried capability attribute for an initializedcublasltmatmulalgo_tdescriptor structure. the capability attribute value is retrieved from the enumerated typecublasltmatmulalgocapattributes_t. for example, to get list of supported tile ids: parameters: returns: seecublasstatus_tfor a complete list of valid return codes.",3
3.4.19.cublasltmatmulalgocheck(),#cublasltmatmulalgocheck,"3.4.19.cublasltmatmulalgocheck() this function performs the correctness check on the matrix multiply algorithm descriptor for the matrix multiply operationcublasltmatmul()function with the given input matrices a, b and c, and the output matrix d. it checks whether the descriptor is supported on the current device, and returns the result containing the required workspace and the calculated wave count. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.",3
3.4.20.cublasltmatmulalgoconfiggetattribute(),#cublasltmatmulalgoconfiggetattribute,3.4.20.cublasltmatmulalgoconfiggetattribute() this function returns the value of the queried configuration attribute for an initializedcublasltmatmulalgo_tdescriptor. the configuration attribute value is retrieved from the enumerated typecublasltmatmulalgoconfigattributes_t. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.21.cublasltmatmulalgoconfigsetattribute(),#cublasltmatmulalgoconfigsetattribute,3.4.21.cublasltmatmulalgoconfigsetattribute() this function sets the value of the specified configuration attribute for an initializedcublasltmatmulalgo_tdescriptor. the configuration attribute is an enumerant of the typecublasltmatmulalgoconfigattributes_t. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.22.cublasltmatmulalgogetheuristic(),#cublasltmatmulalgogetheuristic,"3.4.22.cublasltmatmulalgogetheuristic() this function retrieves the possible algorithms for the matrix multiply operationcublasltmatmul()function with the given input matrices a, b and c, and the output matrix d. the output is placed inheuristicresultsarray[]in the order of increasing estimated compute time. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.",3
3.4.23.cublasltmatmulalgogetids(),#cublasltmatmulalgogetids,"3.4.23.cublasltmatmulalgogetids() this function retrieves the ids of all the matrix multiply algorithms that are valid, and can potentially be run by thecublasltmatmul()function, for given types of the input matrices a, b and c, and of the output matrix d. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.",3
8.3.1.composition example,#composition-example,"8.3.1.composition example to illustrate the concept of groups, this example attempts to perform a block-wide sum reduction. previously, there were hidden constraints on the implementation when writing this code: all threads in the thread block must arrive at the__syncthreads()barrier, however, this constraint is hidden from the developer who might want to usesum(). with cooperative groups, a better way of writing this would be:",5
3.4.24.cublasltmatmulalgoinit(),#cublasltmatmulalgoinit,"3.4.24.cublasltmatmulalgoinit() this function initializes the matrix multiply algorithm structure for thecublasltmatmul(), for a specified matrix multiply algorithm and input matrices a, b and c, and the output matrix d. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.",3
3.4.25.cublasltmatmuldesccreate(),#cublasltmatmuldesccreate,3.4.25.cublasltmatmuldesccreate() this function creates a matrix multiply descriptor by allocating the memory needed to hold its opaque structure. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.26.cublasltmatmuldescinit(),#cublasltmatmuldescinit,3.4.26.cublasltmatmuldescinit() this function initializes a matrix multiply descriptor in a previously allocated one. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.27.cublasltmatmuldescdestroy(),#cublasltmatmuldescdestroy,3.4.27.cublasltmatmuldescdestroy() this function destroys a previously created matrix multiply descriptor object. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.28.cublasltmatmuldescgetattribute(),#cublasltmatmuldescgetattribute,3.4.28.cublasltmatmuldescgetattribute() this function returns the value of the queried attribute belonging to a previously created matrix multiply descriptor. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.29.cublasltmatmuldescsetattribute(),#cublasltmatmuldescsetattribute,3.4.29.cublasltmatmuldescsetattribute() this function sets the value of the specified attribute belonging to a previously created matrix multiply descriptor. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.30.cublasltmatmulpreferencecreate(),#cublasltmatmulpreferencecreate,3.4.30.cublasltmatmulpreferencecreate() this function creates a matrix multiply heuristic search preferences descriptor by allocating the memory needed to hold its opaque structure. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
8.4.group types,#group-types,8.4.group types,9
3.4.31.cublasltmatmulpreferenceinit(),#cublasltmatmulpreferenceinit,3.4.31.cublasltmatmulpreferenceinit() this function initializes a matrix multiply heuristic search preferences descriptor in a previously allocated one. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.32.cublasltmatmulpreferencedestroy(),#cublasltmatmulpreferencedestroy,3.4.32.cublasltmatmulpreferencedestroy() this function destroys a previously created matrix multiply preferences descriptor object. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.33.cublasltmatmulpreferencegetattribute(),#cublasltmatmulpreferencegetattribute,3.4.33.cublasltmatmulpreferencegetattribute() this function returns the value of the queried attribute belonging to a previously created matrix multiply heuristic search preferences descriptor. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.34.cublasltmatmulpreferencesetattribute(),#cublasltmatmulpreferencesetattribute,3.4.34.cublasltmatmulpreferencesetattribute() this function sets the value of the specified attribute belonging to a previously created matrix multiply preferences descriptor. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.35.cublasltmatrixlayoutcreate(),#cublasltmatrixlayoutcreate,3.4.35.cublasltmatrixlayoutcreate() this function creates a matrix layout descriptor by allocating the memory needed to hold its opaque structure. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.36.cublasltmatrixlayoutinit(),#cublasltmatrixlayoutinit,3.4.36.cublasltmatrixlayoutinit() this function initializes a matrix layout descriptor in a previously allocated one. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.37.cublasltmatrixlayoutdestroy(),#cublasltmatrixlayoutdestroy,3.4.37.cublasltmatrixlayoutdestroy() this function destroys a previously created matrix layout descriptor object. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.38.cublasltmatrixlayoutgetattribute(),#cublasltmatrixlayoutgetattribute,3.4.38.cublasltmatrixlayoutgetattribute() this function returns the value of the queried attribute belonging to the specified matrix layout descriptor. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.39.cublasltmatrixlayoutsetattribute(),#cublasltmatrixlayoutsetattribute,3.4.39.cublasltmatrixlayoutsetattribute() this function sets the value of the specified attribute belonging to a previously created matrix layout descriptor. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.40.cublasltmatrixtransform(),#cublasltmatrixtransform,"3.4.40.cublasltmatrixtransform() this function computes the matrix transformation operation on the input matrices a and b, to produce the output matrix c, according to the below operation: c=alpha*transformation(a)+beta*transformation(b), wherea,bare input matrices, andalphaandbetaare input scalars. the transformation operation is defined by thetransformdescpointer. this function can be used to change the memory order of data or to scale and shift the values. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.",3
3.4.41.cublasltmatrixtransformdesccreate(),#cublasltmatrixtransformdesccreate,3.4.41.cublasltmatrixtransformdesccreate() this function creates a matrix transform descriptor by allocating the memory needed to hold its opaque structure. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.42.cublasltmatrixtransformdescinit(),#cublasltmatrixtransformdescinit,3.4.42.cublasltmatrixtransformdescinit() this function initializes a matrix transform descriptor in a previously allocated one. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.43.cublasltmatrixtransformdescdestroy(),#cublasltmatrixtransformdescdestroy,3.4.43.cublasltmatrixtransformdescdestroy() this function destroys a previously created matrix transform descriptor object. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.44.cublasltmatrixtransformdescgetattribute(),#cublasltmatrixtransformdescgetattribute,3.4.44.cublasltmatrixtransformdescgetattribute() this function returns the value of the queried attribute belonging to a previously created matrix transform descriptor. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
3.4.45.cublasltmatrixtransformdescsetattribute(),#cublasltmatrixtransformdescsetattribute,3.4.45.cublasltmatrixtransformdescsetattribute() this function sets the value of the specified attribute belonging to a previously created matrix transform descriptor. parameters: returns: seecublasstatus_tfor a complete list of valid return codes.,3
8.4.1.implicit groups,#implicit-groups,"8.4.1.implicit groups implicit groups represent the launch configuration of the kernel. regardless of how your kernel is written, it always has a set number of threads, blocks and block dimensions, a single grid and grid dimensions. in addition, if the multi-device cooperative launch api is used, it can have multiple grids (single grid per device). these groups provide a starting point for decomposition into finer grained groups which are typically hw accelerated and are more specialized for the problem the developer is solving. although you can create an implicit group anywhere in the code, it is dangerous to do so. creating a handle for an implicit group is a collective operationall threads in the group must participate. if the group was created in a conditional branch that not all threads reach, this can lead to deadlocks or data corruption. for this reason, it is recommended that you create a handle for the implicit group upfront (as early as possible, before any branching has occurred) and use that handle throughout the kernel. group handles must be initialized at declaration time (there is no default constructor) for the same reason and copy-constructing them is discouraged.",5
8.4.2.explicit groups,#explicit-groups,8.4.2.explicit groups,9
8.5.group partitioning,#group-partitioning,8.5.group partitioning,9
4.using the cublasxt api,#using-the-cublasxt-api,4.using the cublasxt api,3
8.5.1.tiled_partition,#tiled-partition,"8.5.1.tiled_partition thetiled_partitionmethod is a collective operation that partitions the parent group into a one-dimensional, row-major, tiling of subgroups. a total of ((size(parent)/tilesz) subgroups will be created, therefore the parent group size must be evenly divisible by thesize. the allowed parent groups arethread_blockorthread_block_tile. the implementation may cause the calling thread to wait until all the members of the parent group have invoked the operation before resuming execution. functionality is limited to native hardware sizes, 1/2/4/8/16/32 and thecg::size(parent)must be greater than thesizeparameter. the templated version oftiled_partitionsupports 64/128/256/512 sizes as well, but some additional steps are required on compute capability 7.5 or lower, refer tothread block tilefor details. codegen requirements:compute capability 5.0 minimum, c++11 for sizes larger than 32 example: we can partition each of these groups into even smaller groups, each of size 4 threads: if, for instance, if we were to then include the following line of code: then the statement would be printed by every fourth thread in the block: the threads of rank 0 in eachtile4group, which correspond to those threads with ranks 0,4,8,12,etc. in theblockgroup.",5
8.5.2.labeled_partition,#labeled-partition,"8.5.2.labeled_partition thelabeled_partitionmethod is a collective operation that partitions the parent group into one-dimensional subgroups within which the threads are coalesced. the implementation will evaluate a condition label and assign threads that have the same value for label into the same group. labelcan be any integral type. the implementation may cause the calling thread to wait until all the members of the parent group have invoked the operation before resuming execution. note:this functionality is still being evaluated and may slightly change in the future. codegen requirements:compute capability 7.0 minimum, c++11",5
8.5.3.binary_partition,#binary-partition,"8.5.3.binary_partition thebinary_partition()method is a collective operation that partitions the parent group into one-dimensional subgroups within which the threads are coalesced. the implementation will evaluate a predicate and assign threads that have the same value into the same group. this is a specialized form oflabeled_partition(), where the label can only be 0 or 1. the implementation may cause the calling thread to wait until all the members of the parent group have invoked the operation before resuming execution. note:this functionality is still being evaluated and may slightly change in the future. codegen requirements:compute capability 7.0 minimum, c++11 example:",5
8.6.group collectives,#group-collectives,"8.6.group collectives cooperative groups library provides a set of collective operations that can be performed by a group of threads.
these operations require participation of all threads in the specified group in order to complete the operation.
all threads in the group need to pass the same values for corresponding arguments to each collective call, unless
different values are explicitly allowed in the argument description. otherwise the behavior of the call is undefined.",5
8.6.1.synchronization,#synchronization,8.6.1.synchronization,9
8.6.2.data transfer,#data-transfer,8.6.2.data transfer,9
8.6.3.data manipulation,#data-manipulation,8.6.3.data manipulation,9
8.6.4.execution control,#execution-control,8.6.4.execution control,9
8.7.grid synchronization,#grid-synchronization,"8.7.grid synchronization prior to the introduction of cooperative groups, the cuda programming model only allowed synchronization between thread blocks at a kernel completion boundary. the kernel boundary carries with it an implicit invalidation of state, and with it, potential performance implications. for example, in certain use cases, applications have a large number of small kernels, with each kernel representing a stage in a processing pipeline. the presence of these kernels is required by the current cuda programming model to ensure that the thread blocks operating on one pipeline stage have produced data before the thread block operating on the next pipeline stage is ready to consume it. in such cases, the ability to provide global inter thread block synchronization would allow the application to be restructured to have persistent thread blocks, which are able to synchronize on the device when a given stage is complete. to synchronize across the grid, from within a kernel, you would simply use thegrid.sync()function: and when launching the kernel it is necessary to use, instead of the<<<...>>>execution configuration syntax, thecudalaunchcooperativekernelcuda runtime launch api or thecudadriverequivalent. example: to guarantee co-residency of the thread blocks on the gpu, the number of blocks launched needs to be carefully considered. for example, as many blocks as there are sms can be launched as follows: alternatively, you can maximize the exposed parallelism by calculating how many blocks can fit simultaneously per-sm using the occupancy calculator as follows: it is good practice to first ensure the device supports cooperative launches by querying the device attributecudadevattrcooperativelaunch: which will setsupportscooplaunchto 1 if the property is supported on device 0. only devices with compute capability of 6.0 and higher are supported. in addition, you need to be running on either of these:",5
8.8.multi-device synchronization,#multi-device-synchronization,"8.8.multi-device synchronization in order to enable synchronization across multiple devices with cooperative groups, use of thecudalaunchcooperativekernelmultidevicecuda api is required. this, a significant departure from existing cuda apis, will allow a single host thread to launch a kernel across multiple devices. in addition to the constraints and guarantees made bycudalaunchcooperativekernel, this api has additional semantics: deprecation notice:cudalaunchcooperativekernelmultidevicehas been deprecated in cuda 11.3 for all devices. example of an alternative approach can be found in the multi device conjugate gradient sample. optimal performance in multi-device synchronization is achieved by enabling peer access viacuctxenablepeeraccessorcudadeviceenablepeeraccessfor all participating devices. the launch parameters should be defined using an array of structs (one per device), and launched withcudalaunchcooperativekernelmultidevice example: also, as with grid-wide synchronization, the resulting device code looks very similar: however, the code needs to be compiled in separate compilation by passing-rdc=trueto nvcc. it is good practice to first ensure the device supports multi-device cooperative launches by querying the device attributecudadevattrcooperativemultidevicelaunch: which will setsupportsmdcooplaunchto 1 if the property is supported on device 0. only devices with compute capability of 6.0 and higher are supported. in addition, you need to be running on the linux platform (without mps) or on current versions of windows with the device in tcc mode. see thecudalaunchcooperativekernelmultideviceapi documentation for more information.",5
9.cuda dynamic parallelism,#cuda-dynamic-parallelism,9.cuda dynamic parallelism,5
9.1.introduction,#introduction-cuda-dynamic-parallelism,9.1.introduction,9
9.1.1.overview,#overview,"9.1.1.overview dynamic parallelismis an extension to the cuda programming model enabling a cuda kernel to create and synchronize with new work directly on the gpu. the creation of parallelism dynamically at whichever point in a program that it is needed offers exciting capabilities. the ability to create work directly from the gpu can reduce the need to transfer execution control and data between host and device, as launch configuration decisions can now be made at runtime by threads executing on the device. additionally, data-dependent parallel work can be generated inline within a kernel at run-time, taking advantage of the gpus hardware schedulers and load balancers dynamically and adapting in response to data-driven decisions or workloads. algorithms and programming patterns that had previously required modifications to eliminate recursion, irregular loop structure, or other constructs that do not fit a flat, single-level of parallelism may more transparently be expressed. this document describes the extended capabilities of cuda which enable dynamic parallelism, including the modifications and additions to the cuda programming model necessary to take advantage of these, as well as guidelines and best practices for exploiting this added capacity. dynamic parallelism is only supported by devices of compute capability 3.5 and higher.",5
9.1.2.glossary,#glossary,9.1.2.glossary definitions for terms used in this guide.,9
9.2.execution environment and memory model,#execution-environment-and-memory-model,9.2.execution environment and memory model,5
9.2.1.execution environment,#execution-environment,"9.2.1.execution environment the cuda execution model is based on primitives of threads, thread blocks, and grids, with kernel functions defining the program executed by individual threads within a thread block and grid. when a kernel function is invoked the grids properties are described by an execution configuration, which has a special syntax in cuda. support for dynamic parallelism in cuda extends the ability to configure, launch, and implicitly synchronize upon new grids to threads that are running on the device.",5
9.2.2.memory model,#memory-model,"9.2.2.memory model parent and child grids share the same global and constant memory storage, but have distinct local and shared memory.",5
9.3.programming interface,#programming-interface-cdp,9.3.programming interface,9
9.3.1.cuda c++ reference,#cuda-c-reference,"9.3.1.cuda c++ reference this section describes changes and additions to the cuda c++ language extensions for supportingdynamic parallelism. the language interface and api available to cuda kernels using cuda c++ for dynamic parallelism, referred to as thedevice runtime, is substantially like that of the cuda runtime api available on the host. where possible the syntax and semantics of the cuda runtime api have been retained in order to facilitate ease of code reuse for routines that may run in either the host or device environments. as with all code in cuda c++, the apis and code outlined here is per-thread code. this enables each thread to make unique, dynamic decisions regarding what kernel or operation to execute next. there are no synchronization requirements between threads within a block to execute any of the provided device runtime apis, which enables the device runtime api functions to be called in arbitrarily divergent kernel code without deadlock.",5
9.3.2.device-side launch from ptx,#device-side-launch-from-ptx,9.3.2.device-side launch from ptx this section is for the programming language and compiler implementers who targetparallel thread execution(ptx) and plan to supportdynamic parallelismin their language. it provides the low-level details related to supporting kernel launches at the ptx level.,7
9.3.3.toolkit support for dynamic parallelism,#toolkit-support-for-dynamic-parallelism,9.3.3.toolkit support for dynamic parallelism,5
9.4.programming guidelines,#programming-guidelines,9.4.programming guidelines,9
9.4.1.basics,#basics,"9.4.1.basics the device runtime is a functional subset of the host runtime. api level device management, kernel launching, device memcpy, stream management, and event management are exposed from the device runtime. programming for the device runtime should be familiar to someone who already has experience with cuda. device runtime syntax and semantics are largely the same as that of the host api, with any exceptions detailed earlier in this document. the following example shows a simplehello worldprogram incorporating dynamic parallelism: this program may be built in a single step from the command line as follows:",5
9.4.2.performance,#performance,9.4.2.performance,9
9.4.3.implementation restrictions and limitations,#implementation-restrictions-and-limitations,"9.4.3.implementation restrictions and limitations dynamic parallelismguarantees all semantics described in this document, however, certain hardware and software resources are implementation-dependent and limit the scale, performance and other properties of a program which uses the device runtime.",5
4.1.general description,#id93,"4.1.general description the cublasxt api of cublas exposes a multi-gpu capable host interface: when using this api the application only needs to allocate the required matrices on the host memory space. additionally, the current implementation supports managed memory on linux with gpu devices that have compute capability 6.x or greater but treats it as host memory. managed memory is not supported on windows. there are no restriction on the sizes of the matrices as long as they can fit into the host memory. the cublasxt api takes care of allocating the memory across the designated gpus and dispatched the workload between them and finally retrieves the results back to the host. the cublasxt api supports only the compute-intensive blas3 routines (e.g matrix-matrix operations) where the pci transfers back and forth from the gpu can be amortized. the cublasxt api has its own header filecublasxt.h. starting with release 8.0, cublasxt api allows any of the matrices to be located on a gpu device. note : the cublasxt api is only supported on 64-bit platforms.",3
9.5.cdp2 vs cdp1,#cdp2-vs-cdp1,"9.5.cdp2 vs cdp1 this section summarises the differences between, and the compatibility and interoperability of, the new (cdp2) and legacy (cdp1) cuda dynamic parallelism interfaces. it also shows how to opt-out of the cdp2 interface on devices of compute capability less than 9.0.",0
9.5.1.differences between cdp1 and cdp2,#differences-between-cdp1-and-cdp2,"9.5.1.differences between cdp1 and cdp2 explicit device-side synchronization is no longer possible with cdp2 or on devices of compute capability 9.0 or higher. implicit synchronization (such as tail launches) must be used instead. attempting to query or setcudalimitdevruntimesyncdepth(orcu_limit_dev_runtime_sync_depth) with cdp2 or on devices of compute capability 9.0 or higher results incudaerrorunsupportedlimit. cdp2 no longer has a virtualized pool for pending launches that dont fit in the fixed-sized pool.cudalimitdevruntimependinglaunchcountmust be set to be large enough to avoid running out of launch slots. for cdp2, there is a limit to the total number of events existing at once (note that events are destroyed only after a launch completes), equal to twice the pending launch count.cudalimitdevruntimependinglaunchcountmust be set to be large enough to avoid running out of event slots. streams are tracked per grid with cdp2 or on devices of compute capability 9.0 or higher, not per thread block. this allows work to be launched into a stream created by another thread block. attempting to do so with the cdp1 results incudaerrorinvalidvalue. cdp2 introduces the tail launch (cudastreamtaillaunch) and fire-and-forget (cudastreamfireandforget) named streams. cdp2 is supported only under 64-bit compilation mode.",5
9.5.2.compatibility and interoperability,#compatibility-and-interoperability,"9.5.2.compatibility and interoperability cdp2 is the default. functions can be compiled with-dcuda_force_cdp1_if_supportedto opt-out of using cdp2 on devices of compute capability less than 9.0. functions using cdp1 and cdp2 may be loaded and run simultaneously in the same context. the cdp1 functions are able to use cdp1-specific features (e.g.cudadevicesynchronize) and cdp2 functions are able to use cdp2-specific features (e.g. tail launch and fire-and-forget launch). a function using cdp1 cannot launch a function using cdp2, and vice versa. if a function that would use cdp1 contains in its call graph a function that would use cdp2, or vice versa,cudaerrorcdpversionmismatchwould result during function load.",6
9.6.legacy cuda dynamic parallelism (cdp1),#legacy-cuda-dynamic-parallelism-cdp1,"9.6.legacy cuda dynamic parallelism (cdp1) seecuda dynamic parallelism, above, for cdp2 version of document.",0
9.6.1.execution environment and memory model (cdp1),#execution-environment-and-memory-model-cdp1,"9.6.1.execution environment and memory model (cdp1) seeexecution environment and memory model, above, for cdp2 version of document.",6
9.6.2.programming interface (cdp1),#programming-interface-cdp1,"9.6.2.programming interface (cdp1) seeprogramming interface, above, for cdp2 version of document.",9
9.6.3.programming guidelines (cdp1),#programming-guidelines-cdp1,"9.6.3.programming guidelines (cdp1) seeprogramming guidelines, above, for cdp2 version of document.",9
10.virtual memory management,#virtual-memory-management,10.virtual memory management,5
10.1.introduction,#introduction-virtual-memory-management,"10.1.introduction thevirtual memory management apisprovide a way for the application to directly manage the unified virtual address space that cuda provides to map physical memory to virtual addresses accessible by the gpu. introduced in cuda 10.2, these apis additionally provide a new way to interop with other processes and graphics apis like opengl and vulkan, as well as provide newer memory attributes that a user can tune to fit their applications. historically, memory allocation calls (such ascudamalloc()) in the cuda programming model have returned a memory address that points to the gpu memory. the address thus obtained could be used with any cuda api or inside a device kernel. however, the memory allocated could not be resized depending on the users memory needs. in order to increase an allocations size, the user had to explicitly allocate a larger buffer, copy data from the initial allocation, free it and then continue to keep track of the newer allocations address. this often leads to lower performance and higher peak memory utilization for applications. essentially, users had a malloc-like interface for allocating gpu memory, but did not have a corresponding realloc to complement it. the virtual memory management apis decouple the idea of an address and memory and allow the application to handle them separately. the apis allow applications to map and unmap memory from a virtual address range as they see fit. in the case of enabling peer device access to memory allocations by usingcudaenablepeeraccess, all past and future user allocations are mapped to the target peer device. this lead to users unwittingly paying runtime cost of mapping all cudamalloc allocations to peer devices. however, in most situations applications communicate by sharing only a few allocations with another device and not all allocations are required to be mapped to all the devices. with virtual memory management, applications can specifically choose certain allocations to be accessible from target devices. the cuda virtual memory management apis expose fine grained control to the user for managing the gpu memory in applications. it provides apis that let users: in order to allocate memory, the virtual memory management programming model exposes the following functionality: note that the suite of apis described in this section require a system that supports uva.",5
10.2.query for support,#query-for-support,"10.2.query for support before attempting to use virtual memory management apis, applications must ensure that the devices they want to use support cuda virtual memory management. the following code sample shows querying for virtual memory management support:",5
10.3.allocating physical memory,#allocating-physical-memory,"10.3.allocating physical memory the first step in memory allocation using virtual memory management apis is to create a physical memory chunk that will provide a backing for the allocation. in order to allocate physical memory, applications must use thecumemcreateapi. the allocation created by this function does not have any device or host mappings. the function argumentcumemgenericallocationhandledescribes the properties of the memory to allocate such as the location of the allocation, if the allocation is going to be shared to another process (or other graphics apis), or the physical attributes of the memory to be allocated. users must ensure the requested allocations size must be aligned to appropriate granularity. information regarding an allocations granularity requirements can be queried usingcumemgetallocationgranularity. the following code snippet shows allocating physical memory withcumemcreate: the memory allocated bycumemcreateis referenced by thecumemgenericallocationhandleit returns. this is a departure from the cudamalloc-style of allocation, which returns a pointer to the gpu memory, which was directly accessible by cuda kernel executing on the device. the memory allocated cannot be used for any operations other than querying properties usingcumemgetallocationpropertiesfromhandle. in order to make this memory accessible, applications must map this memory into a va range reserved bycumemaddressreserveand provide suitable access rights to it. applications must free the allocated memory using thecumemreleaseapi.",5
10.3.1.shareable memory allocations,#shareable-memory-allocations,"10.3.1.shareable memory allocations withcumemcreateusers now have the facility to indicate to cuda, at allocation time, that they have earmarked a particular allocation for inter process communication and graphics interop purposes. applications can do this by settingcumemallocationprop::requestedhandletypesto a platform-specific field. on windows, whencumemallocationprop::requestedhandletypesis set tocu_mem_handle_type_win32applications must also specify an lpsecurityattributes attribute incumemallocationprop::win32handlemetadata. this security attribute defines the scope of which exported allocations may be transferred to other processes. the cuda virtual memory management api functions do not support the legacy interprocess communication functions with their memory. instead, they expose a new mechanism for interprocess communication that uses os-specific handles. applications can obtain these os-specific handles corresponding to the allocations by usingcumemexporttoshareablehandle. the handles thus obtained can be transferred by using the usual os native mechanisms for inter process communication. the recipient process should import the allocation by usingcumemimportfromshareablehandle. users must ensure they query for support of the requested handle type before attempting to export memory allocated withcumemcreate. the following code snippet illustrates query for handle type support in a platform-specific way. users should set thecumemallocationprop::requestedhandletypesappropriately as shown below: thememmapipcdrvsample can be used as an example for using ipc with virtual memory management allocations.",5
4.1.1.tiling design approach,#tiling-design-approach,"4.1.1.tiling design approach to be able to share the workload between multiples gpus, the cublasxt api uses a tiling strategy : every matrix is divided in square tiles of user-controllable dimension blockdim x blockdim. the resulting matrix tiling defines the static scheduling policy : each resulting tile is affected to a gpu in a round robin fashion one cpu thread is created per gpu and is responsible to do the proper memory transfers and cublas operations to compute all the tiles that it is responsible for. from a performance point of view, due to this static scheduling strategy, it is better that compute capabilites and pci bandwidth are the same for every gpu. the figure below illustrates the tiles distribution between 3 gpus. to compute the first tile g0 from c, the cpu thread 0 responsible of gpu0, have to load 3 tiles from the first row of a and tiles from the first columun of b in a pipeline fashion in order to overlap memory transfer and computations and sum the results into the first tile g0 of c before to move on to the next tile g0. when the tile dimension is not an exact multiple of the dimensions of c, some tiles are partially filled on the right border or/and the bottom border. the current implementation does not pad the incomplete tiles but simply keep track of those incomplete tiles by doing the right reduced cublas opearations : this way, no extra computation is done. however it still can lead to some load unbalance when all gpus do not have the same number of incomplete tiles to work on. when one or more matrices are located on some gpu devices, the same tiling approach and workload sharing is applied. the memory transfers are in this case done between devices. however, when the computation of a tile and some data are located on the same gpu device, the memory transfer to/from the local data into tiles is bypassed and the gpu operates directly on the local data. this can lead to a significant performance increase, especially when only one gpu is used for the computation. the matrices can be located on any gpu device, and do not have to be located on the same gpu device. furthermore, the matrices can even be located on a gpu device that do not participate to the computation. on the contrary of the cublas api, even if all matrices are located on the same device, the cublasxt api is still a blocking api from the host point of view : the data results wherever located will be valid on the call return and no device synchronization is required.",5
4.1.2.hybrid cpu-gpu computation,#hybrid-cpu-gpu-computation,"4.1.2.hybrid cpu-gpu computation in the case of very large problems, the cublasxt api offers the possibility to offload some of the computation to the host cpu. this feature can be setup with the routinescublasxtsetcpuroutine()andcublasxtsetcpuratio()the workload affected to the cpu is put aside : it is simply a percentage of the resulting matrix taken from the bottom and the right side whichever dimension is bigger. the gpu tiling is done after that on the reduced resulting matrix. if any of the matrices is located on a gpu device, the feature is ignored and all computation will be done only on the gpus this feature should be used with caution because it could interfere with the cpu threads responsible of feeding the gpus. currently, only the routinecublasxt<t>gemmsupports this feature.",5
4.1.3.results reproducibility,#id95,"4.1.3.results reproducibility currently all cublasxt api routines from a given toolkit version, generate the same bit-wise results when the following conditions are respected :",3
10.3.2.memory type,#memory-type,"10.3.2.memory type before cuda 10.2, applications had no user-controlled way of allocating any special type of memory that certain devices may support. withcumemcreate, applications can additionally specify memory type requirements using thecumemallocationprop::allocflagsto opt into any specific memory features. applications must also ensure that the requested memory type is supported on the device of allocation.",5
10.4.reserving a virtual address range,#reserving-a-virtual-address-range,"10.4.reserving a virtual address range since with virtual memory management the notions of address and memory are distinct, applications must carve out an address range that can hold the memory allocations made bycumemcreate. the address range reserved must be at least as large as the sum of the sizes of all the physical memory allocations the user plans to place in them. applications can reserve a virtual address range by passing appropriate parameters tocumemaddressreserve. the address range obtained will not have any device or host physical memory associated with it. the reserved virtual address range can be mapped to memory chunks belonging to any device in the system, thus providing the application a continuous va range backed and mapped by memory belonging to different devices. applications are expected to return the virtual address range back to cuda usingcumemaddressfree. users must ensure that the entire va range is unmapped before callingcumemaddressfree. these functions are conceptually similar to mmap/munmap (on linux) or virtualalloc/virtualfree (on windows) functions. the following code snippet illustrates the usage for the function:",5
4.2.cublasxt api datatypes reference,#cublasxt-api-datatypes-reference,4.2.cublasxt api datatypes reference,3
10.5.virtual aliasing support,#virtual-aliasing-support,"10.5.virtual aliasing support the virtual memory management apis provide a way to create multiple virtual memory mappings or proxies to the same allocation using multiple calls tocumemmapwith different virtual addresses, so-called virtual aliasing. unless otherwise noted in the ptx isa, writes to one proxy of the allocation are considered inconsistent and incoherent with any other proxy of the same memory until the writing device operation (grid launch, memcpy, memset, and so on) completes. grids present on the gpu prior to a writing device operation but reading after the writing device operation completes are also considered to have inconsistent and incoherent proxies. for example, the following snippet is considered undefined, assuming device pointers a and b are virtual aliases of the same memory allocation: the following is defined behavior, assuming these two kernels are ordered monotonically (by streams or events).",5
10.6.mapping memory,#mapping-memory,"10.6.mapping memory the allocated physical memory and the carved out virtual address space from the previous two sections represent the memory and address distinction introduced by the virtual memory management apis. for the allocated memory to be useable, the user must first place the memory in the address space. the address range obtained fromcumemaddressreserveand the physical allocation obtained fromcumemcreateorcumemimportfromshareablehandlemust be associated with each other by usingcumemmap. users can associate allocations from multiple devices to reside in contiguous virtual address ranges as long as they have carved out enough address space. in order to decouple the physical allocation and the address range, users must unmap the address of the mapping by usingcumemunmap. users can map and unmap memory to the same address range as many times as they want, as long as they ensure that they dont attempt to create mappings on va range reservations that are already mapped. the following code snippet illustrates the usage for the function:",6
4.2.1.cublasxthandle_t,#cublasxthandle-t,4.2.1.cublasxthandle_t thecublasxthandle_ttype is a pointer type to an opaque structure holding the cublasxt api context. the cublasxt api context must be initialized usingcublasxtcreate()and the returned handle must be passed to all subsequent cublasxt api function calls. the context should be destroyed at the end usingcublasxtdestroy().,3
4.2.2.cublasxtoptype_t,#cublasxtoptype-t,4.2.2.cublasxtoptype_t thecublasoptype_tenumerates the four possible types supported by blas routines. this enum is used as parameters of the routinescublasxtsetcpuroutineandcublasxtsetcpuratioto setup the hybrid configuration.,3
4.2.3.cublasxtblasop_t,#cublasxtblasop-t,4.2.3.cublasxtblasop_t thecublasxtblasop_ttype enumerates the blas3 or blas-like routine supported by cublasxt api. this enum is used as parameters of the routinescublasxtsetcpuroutineandcublasxtsetcpuratioto setup the hybrid configuration.,3
4.2.4.cublasxtpinningmemmode_t,#cublasxtpinningmemmode-t,4.2.4.cublasxtpinningmemmode_t the type is used to enable or disable the pinning memory mode through the routinecubasmgsetpinningmemmode,3
4.3.cublasxt api helper function reference,#cublasxt-api-helper-function-reference,4.3.cublasxt api helper function reference,3
4.3.1.cublasxtcreate(),#cublasxtcreate,4.3.1.cublasxtcreate() this function initializes the cublasxt api and creates a handle to an opaque structure holding the cublasxt api context. it allocates hardware resources on the host and device and must be called prior to making any other cublasxt api calls.,3
10.7.controlling access rights,#controlling-access-rights,"10.7.controlling access rights the virtual memory management apis enable applications to explicitly protect their va ranges with access control mechanisms. mapping the allocation to a region of the address range usingcumemmapdoes not make the address accessible, and would result in a program crash if accessed by a cuda kernel. users must specifically select access control using thecumemsetaccessfunction, which allows or restricts access for specific devices to a mapped address range. the following code snippet illustrates the usage for the function: the access control mechanism exposed with virtual memory management allows users to be explicit about which allocations they want to share with other peer devices on the system. as specified earlier,cudaenablepeeraccessforces all prior and future cudamallocd allocations to be mapped to the target peer device. this can be convenient in many cases as user doesnt have to worry about tracking the mapping state of every allocation to every device in the system. but for users concerned with performance of their applications this approachhas performance implications. with access control at allocation granularity virtual memory management exposes a mechanism to have peer mappings with minimal overhead. thevectoraddmmapsample can be used as an example for using the virtual memory management apis.",5
11.stream ordered memory allocator,#stream-ordered-memory-allocator,11.stream ordered memory allocator,5
4.3.2.cublasxtdestroy(),#cublasxtdestroy,4.3.2.cublasxtdestroy() this function releases hardware resources used by the cublasxt api context. the release of gpu resources may be deferred until the application exits. this function is usually the last call with a particular handle to the cublasxt api.,3
4.3.3.cublasxtdeviceselect(),#cublasxtdeviceselect,"4.3.3.cublasxtdeviceselect() this function allows the user to provide the number of gpu devices and their respective ids that will participate to the subsequent cublasxt api math function calls. this function will create a cublas context for every gpu provided in that list. currently the device configuration is static and cannot be changed between math function calls. in that regard, this function should be called only once aftercublasxtcreate. to be able to run multiple configurations, multiple cublasxt api contexts should be created.",3
4.3.4.cublasxtsetblockdim(),#cublasxtsetblockdim,4.3.4.cublasxtsetblockdim() this function allows the user to set the block dimension used for the tiling of the matrices for the subsequent math function calls. matrices are split in square tiles of blockdim x blockdim dimension. this function can be called anytime and will take effect for the following math function calls. the block dimension should be chosen in a way to optimize the math operation and to make sure that the pci transfers are well overlapped with the computation.,3
4.3.5.cublasxtgetblockdim(),#cublasxtgetblockdim,4.3.5.cublasxtgetblockdim() this function allows the user to query the block dimension used for the tiling of the matrices.,3
4.3.6.cublasxtsetcpuroutine(),#cublasxtsetcpuroutine,4.3.6.cublasxtsetcpuroutine() this function allows the user to provide a cpu implementation of the corresponding blas routine. this function can be used with the functioncublasxtsetcpuratio()to define an hybrid computation between the cpu and the gpus. currently the hybrid feature is only supported for the xgemm routines.,3
11.1.introduction,#stream-ordered-memory-allocator-intro,"11.1.introduction managing memory allocations usingcudamallocandcudafreecauses gpu to synchronize across all executing cuda streams. the stream order memory allocator enables applications to order memory allocation and deallocation with other work launched into a cuda stream such as kernel launches and asynchronous copies. this improves application memory use by taking advantage of stream-ordering semantics to reuse memory allocations. the allocator also allows applications to control the allocators memory caching behavior. when set up with an appropriate release threshold, the caching behavior allows the allocator to avoid expensive calls into the os when the application indicates it is willing to accept a bigger memory footprint. the allocator also supports the easy and secure sharing of allocations between processes. for many applications, the stream ordered memory allocator reduces the need for custom memory management abstractions, and makes it easier to create high-performance custom memory management for applications that need it. for applications and libraries that already have custom memory allocators, adopting the stream ordered memory allocator enables multiple libraries to share a common pool of memory managed by the driver, thus reducing excess memory consumption. additionally, the driver can perform optimizations based on its awareness of the allocator and other stream management apis. finally, nsight compute and the next-gen cuda debugger is aware of the allocator as part of their cuda 11.3 toolkit support.",5
11.2.query for support,#stream-ordered-querying-memory-support,"11.2.query for support the user can determine whether or not a device supports the stream ordered memory allocator by callingcudadevicegetattribute()with the device attributecudadevattrmemorypoolssupported. starting with cuda 11.3, ipc memory pool support can be queried with thecudadevattrmemorypoolsupportedhandletypesdevice attribute. previous drivers will returncudaerrorinvalidvalueas those drivers are unaware of the attribute enum. performing the driver version check before the query avoids hitting acudaerrorinvalidvalueerror on drivers where the attribute was not yet defined. one can usecudagetlasterrorto clear the error instead of avoiding it.",5
11.3.api fundamentals (cudamallocasync and cudafreeasync),#api-fundamentals-cudamallocasync-and-cudafreeasync,"11.3.api fundamentals (cudamallocasync and cudafreeasync) the apiscudamallocasyncandcudafreeasyncform the core of the allocator.cudamallocasyncreturns an allocation andcudafreeasyncfrees an allocation. both apis accept stream arguments to define when the allocation will become and stop being available for use. the pointer value returned bycudamallocasyncis determined synchronously and is available for constructing future work. it is important to note thatcudamallocasyncignores the current device/context when determining where the allocation will reside. instead,cudamallocasyncdetermines the resident device based on the specified memory pool or the supplied stream. the simplest use pattern is when the memory is allocated, used, and freed back into the same stream. when using an allocation in a stream other than the allocating stream, the user must guarantee that the access will happen after the allocation operation, otherwise the behavior is undefined. the user may make this guarantee either by synchronizing the allocating stream, or by using cuda events to synchronize the producing and consuming streams. cudafreeasync()inserts a free operation into the stream. the user must guarantee that the free operation happens after the allocation operation and any use of the allocation. also, any use of the allocation after the free operation starts results in undefined behavior. events and/or stream synchronizing operations should be used to guarantee any access to the allocation on other streams is complete before the freeing stream begins the free operation. the user can free allocations allocated withcudamalloc()withcudafreeasync(). the user must make the same guarantees about accesses being complete before the free operation begins. the user can free memory allocated withcudamallocasyncwithcudafree(). when freeing such allocations through thecudafree()api, the driver assumes that all accesses to the allocation are complete and performs no further synchronization. the user can usecudastreamquery/cudastreamsynchronize/cudaeventquery/cudaeventsynchronize/cudadevicesynchronizeto guarantee that the appropriate asynchronous work is complete and that the gpu will not try to access the allocation.",5
11.4.memory pools and the cudamempool_t,#memory-pools-and-the-cudamempool-t,"11.4.memory pools and the cudamempool_t memory pools encapsulate virtual address and physical memory resources that are allocated and managed according to the pools attributes and properties. the primary aspect of a memory pool is the kind and location of memory it manages. all calls tocudamallocasyncuse the resources of a memory pool. in the absence of a specified memory pool,cudamallocasyncuses the current memory pool of the supplied streams device. the current memory pool for a device may be set withcudadevicesetmempooland queried withcudadevicegetmempool. by default (in the absence of acudadevicesetmempoolcall), the current memory pool is the default memory pool of a device. the apicudamallocfrompoolasyncandc++ overloads of cudamallocasyncallow a user to specify the pool to be used for an allocation without setting it as the current pool. the apiscudadevicegetdefaultmempoolandcudamempoolcreategive users handles to memory pools.",5
4.3.7.cublasxtsetcpuratio(),#cublasxtsetcpuratio,4.3.7.cublasxtsetcpuratio() this function allows the user to define the percentage of workload that should be done on a cpu in the context of an hybrid computation. this function can be used with the functioncublasxtsetcpuroutine()to define an hybrid computation between the cpu and the gpus. currently the hybrid feature is only supported for the xgemm routines.,5
11.5.default/implicit pools,#default-implicit-pools,"11.5.default/implicit pools the default memory pool of a device may be retrieved with thecudadevicegetdefaultmempoolapi. allocations from the default memory pool of a device are non-migratable device allocation located on that device. these allocations will always be accessible from that device. the accessibility of the default memory pool may be modified withcudamempoolsetaccessand queried bycudamempoolgetaccess. since the default pools do not need to be explicitly created, they are sometimes referred to as implicit pools. the default memory pool of a device does not support ipc.",5
11.6.explicit pools,#explicit-pools,"11.6.explicit pools the apicudamempoolcreatecreates an explicit pool. this allows applications to request properties for their allocation beyond what is provided by the default/implict pools. these include properties such as ipc capability, maximum pool size, allocations resident on a specific cpu numa node on supported platforms etc. the following code snippet illustrates an example of creating an ipc capable memory pool on a valid cpu numa node.",5
11.7.physical page caching behavior,#physical-page-caching-behavior,"11.7.physical page caching behavior by default, the allocator tries to minimize the physical memory owned by a pool. to minimize the os calls to allocate and free physical memory, applications must configure a memory footprint for each pool. applications can do this with the release threshold attribute (cudamempoolattrreleasethreshold). the release threshold is the amount of memory in bytes a pool should hold onto before trying to release memory back to the os. when more than the release threshold bytes of memory are held by the memory pool, the allocator will try to release memory back to the os on the next call to stream, event or device synchronize. setting the release threshold to uint64_max will prevent the driver from attempting to shrink the pool after every synchronization. applications that setcudamempoolattrreleasethresholdhigh enough to effectively disable memory pool shrinking may wish to explicitly shrink a memory pools memory footprint.cudamempooltrimtoallows such applications to do so. when trimming a memory pools footprint, theminbytestokeepparameter allows an application to hold onto an amount of memory it expects to need in a subsequent phase of execution.",5
11.8.resource usage statistics,#resource-usage-statistics,"11.8.resource usage statistics in cuda 11.3, the pool attributescudamempoolattrreservedmemcurrent,cudamempoolattrreservedmemhigh,cudamempoolattrusedmemcurrent, andcudamempoolattrusedmemhighwere added to query the memory usage of a pool. querying thecudamempoolattrreservedmemcurrentattribute of a pool reports the current total physical gpu memory consumed by the pool. querying thecudamempoolattrusedmemcurrentof a pool returns the total size of all of the memory allocated from the pool and not available for reuse. thecudamempoolattr*memhighattributes are watermarks recording the max value achieved by the respectivecudamempoolattr*memcurrentattribute since last reset. they can be reset to the current value by using thecudamempoolsetattributeapi.",5
11.9.memory reuse policies,#memory-reuse-policies,"11.9.memory reuse policies in order to service an allocation request, the driver attempts to reuse memory that was previously freed viacudafreeasync()before attempting to allocate more memory from the os. for example, memory freed in a stream can immediately be reused for a subsequent allocation request in the same stream. similarly, when a stream is synchronized with the cpu, the memory that was previously freed in that stream becomes available for reuse for an allocation in any stream. the stream ordered allocator has a few controllable allocation policies. the pool attributescudamempoolreusefolloweventdependencies,cudamempoolreuseallowopportunistic, andcudamempoolreuseallowinternaldependenciescontrol these policies. upgrading to a newer cuda driver may change, enhance, augment and/or reorder the reuse policies.",5
4.3.8.cublasxtsetpinningmemmode(),#cublasxtsetpinningmemmode,"4.3.8.cublasxtsetpinningmemmode() this function allows the user to enable or disable the pinning memory mode. when enabled, the matrices passed in subsequent cublasxt api calls will be pinned/unpinned using the cudart routinecudahostregister()andcudahostunregister()respectively if the matrices are not already pinned. if a matrix happened to be pinned partially, it will also not be pinned. pinning the memory improve pci transfer performace and allows to overlap pci memory transfer with computation. however pinning/unpinning the memory take some time which might not be amortized. it is advised that the user pins the memory on its own usingcudamallochost()orcudahostregister()and unpin it when the computation sequence is completed. by default, the pinning memory mode is disabled.",3
4.3.9.cublasxtgetpinningmemmode(),#cublasxtgetpinningmemmode,"4.3.9.cublasxtgetpinningmemmode() this function allows the user to query the pinning memory mode. by default, the pinning memory mode is disabled.",3
4.4.cublasxt api math functions reference,#cublasxt-api-math-functions-reference,"4.4.cublasxt api math functions reference in this chapter we describe the actual linear agebra routines that cublasxt api supports. we will use abbreviations <type> for type and <t> for the corresponding short type to make a more concise and clear presentation of the implemented functions. unless otherwise specified <type> and <t> have the following meanings: the abbreviation\(\mathbf{re}(\cdot)\)and\(\mathbf{im}(\cdot)\)will stand for the real and imaginary part of a number, respectively. since imaginary part of a real number does not exist, we will consider it to be zero and can usually simply discard it from the equation where it is being used. also, the\(\bar{\alpha}\)will denote the complex conjugate of\(\alpha\). in general throughout the documentation, the lower case greek symbols\(\alpha\)and\(\beta\)will denote scalars, lower case english letters in bold type\(\mathbf{x}\)and\(\mathbf{y}\)will denote vectors and capital english letters\(a\),\(b\)and\(c\)will denote matrices.",3
4.4.1.cublasxt<t>gemm(),#id96,"4.4.1.cublasxt<t>gemm() this function performs the matrix-matrix multiplication \(c = \alpha\text{op}(a)\text{op}(b) + \beta c\) where\(\alpha\)and\(\beta\)are scalars, and\(a\),\(b\)and\(c\)are matrices stored in column-major format with dimensions\(\text{op}(a)\)\(m \times k\),\(\text{op}(b)\)\(k \times n\)and\(c\)\(m \times n\), respectively. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) and\(\text{op}(b)\)is defined similarly for matrix\(b\). the possible error values returned by this function and their meanings are listed below. for references please refer to: sgemm,dgemm,cgemm,zgemm",3
4.4.2.cublasxt<t>hemm(),#cublasxt-t-hemm,"4.4.2.cublasxt<t>hemm() this function performs the hermitian matrix-matrix multiplication \(c = \left\{ \begin{matrix}
{\alpha ab + \beta c} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_left}$}} \\
{\alpha ba + \beta c} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_right}$}} \\
\end{matrix} \right.\) where\(a\)is a hermitian matrix stored in lower or upper mode,\(b\)and\(c\)are\(m \times n\)matrices, and\(\alpha\)and\(\beta\)are scalars. the possible error values returned by this function and their meanings are listed below. for references please refer to: chemm,zhemm",3
11.9.1.cudamempoolreusefolloweventdependencies,#cudamempoolreusefolloweventdependencies,"11.9.1.cudamempoolreusefolloweventdependencies before allocating more physical gpu memory, the allocator examines dependency information established by cuda events and tries to allocate from memory freed in another stream.",5
11.9.2.cudamempoolreuseallowopportunistic,#cudamempoolreuseallowopportunistic,"11.9.2.cudamempoolreuseallowopportunistic according to thecudamempoolreuseallowopportunisticpolicy, the allocator examines freed allocations to see if the frees stream order semantic has been met (such as the stream has passed the point of execution indicated by the free). when this is disabled, the allocator will still reuse memory made available when a stream is synchronized with the cpu. disabling this policy does not stop thecudamempoolreusefolloweventdependenciesfrom applying.",5
11.9.3.cudamempoolreuseallowinternaldependencies,#cudamempoolreuseallowinternaldependencies,"11.9.3.cudamempoolreuseallowinternaldependencies failing to allocate and map more physical memory from the os, the driver will look for memory whose availability depends on another streams pending progress. if such memory is found, the driver will insert the required dependency into the allocating stream and reuse the memory.",5
4.4.3.cublasxt<t>symm(),#cublasxt-t-symm,"4.4.3.cublasxt<t>symm() this function performs the symmetric matrix-matrix multiplication \(c = \left\{ \begin{matrix}
{\alpha ab + \beta c} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_left}$}} \\
{\alpha ba + \beta c} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_right}$}} \\
\end{matrix} \right.\) where\(a\)is a symmetric matrix stored in lower or upper mode,\(a\)and\(a\)are\(m \times n\)matrices, and\(\alpha\)and\(\beta\)are scalars. the possible error values returned by this function and their meanings are listed below. for references please refer to: ssymm,dsymm,csymm,zsymm",3
11.9.4.disabling reuse policies,#disabling-reuse-policies,"11.9.4.disabling reuse policies while the controllable reuse policies improve memory reuse, users may want to disable them. allowing opportunistic reuse (such ascudamempoolreuseallowopportunistic) introduces run to run variance in allocation patterns based on the interleaving of cpu and gpu execution. internal dependency insertion (such ascudamempoolreuseallowinternaldependencies) can serialize work in unexpected and potentially non-deterministic ways when the user would rather explicitly synchronize an event or stream on allocation failure.",5
11.10.device accessibility for multi-gpu support,#device-accessibility-for-multi-gpu-support,"11.10.device accessibility for multi-gpu support just like allocation accessibility controlled through the virtual memory management apis, memory pool allocation accessibility does not followcudadeviceenablepeeraccessorcuctxenablepeeraccess. instead, the apicudamempoolsetaccessmodifies what devices can access allocations from a pool. by default, allocations are accessible from the device where the allocations are located. this access cannot be revoked. to enable access from other devices, the accessing device must be peer capable with the memory pools device; check withcudadevicecanaccesspeer. if the peer capability is not checked, the set access may fail withcudaerrorinvaliddevice. if no allocations had been made from the pool, thecudamempoolsetaccesscall may succeed even when the devices are not peer capable; in this case, the next allocation from the pool will fail. it is worth noting thatcudamempoolsetaccessaffects all allocations from the memory pool, not just future ones. also the accessibility reported bycudamempoolgetaccessapplies to all allocations from the pool, not just future ones. it is recommended that the accessibility settings of a pool for a given gpu not be changed frequently; once a pool is made accessible from a given gpu, it should remain accessible from that gpu for the lifetime of the pool.",5
11.11.ipc memory pools,#ipc-memory-pools,"11.11.ipc memory pools ipc capable memory pools allow easy, efficient and secure sharing of gpu memory between processes. cudas ipc memory pools provide the same security benefits as cudas virtual memory management apis. there are two phases to sharing memory between processes with memory pools. the processes first need to share access to the pool, then share specific allocations from that pool. the first phase establishes and enforces security. the second phase coordinates what virtual addresses are used in each process and when mappings need to be valid in the importing process.",5
11.11.1.creating and sharing ipc memory pools,#creating-and-sharing-ipc-memory-pools,"11.11.1.creating and sharing ipc memory pools sharing access to a pool involves retrieving an os native handle to the pool (with thecudamempoolexporttoshareablehandle()api), transferring the handle to the importing process using the usual os native ipc mechanisms, and creating an imported memory pool (with thecudamempoolimportfromshareablehandle()api). forcudamempoolexporttoshareablehandleto succeed, the memory pool had to be created with the requested handle type specified in the pool properties structure. please reference samples for the appropriate ipc mechanisms to transfer the os native handle between processes. the rest of the procedure can be found in the following code snippets.",5
11.11.2.set access in the importing process,#set-access-in-the-importing-process,"11.11.2.set access in the importing process imported memory pools are initially only accessible from their resident device. the imported memory pool does not inherit any accessibility set by the exporting process. the importing process needs to enable access (withcudamempoolsetaccess) from any gpu it plans to access the memory from. if the imported memory pool belongs to a non-visible device in the importing process, the user must use thecudamempoolsetaccessapi to enable access from the gpus the allocations will be used on.",5
4.4.4.cublasxt<t>syrk(),#cublasxt-t-syrk,"4.4.4.cublasxt<t>syrk() this function performs the symmetric rank-\(k\)update \(c = \alpha\text{op}(a)\text{op}(a)^{t} + \beta c\) where\(\alpha\)and\(\beta\)are scalars,\(c\)is a symmetric matrix stored in lower or upper mode, and\(a\)is a matrix with dimensions\(\text{op}(a)\)\(n \times k\). also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
\end{matrix} \right.\) the possible error values returned by this function and their meanings are listed below. for references please refer to: ssyrk,dsyrk,csyrk,zsyrk",3
11.11.3.creating and sharing allocations from an exported pool,#creating-and-sharing-allocations-from-an-exported-pool,"11.11.3.creating and sharing allocations from an exported pool once the pool has been shared, allocations made withcudamallocasync()from the pool in the exporting process can be shared with other processes that have imported the pool. since the pools security policy is established and verified at the pool level, the os does not need extra bookkeeping to provide security for specific pool allocations; in other words, the opaquecudamempoolptrexportdatarequired to import a pool allocation may be sent to the importing process using any mechanism. while allocations may be exported and even imported without synchronizing with the allocating stream in any way, the importing process must follow the same rules as the exporting process when accessing the allocation. namely, access to the allocation must happen after the stream ordering of the allocation operation in the allocating stream. the two following code snippets showcudamempoolexportpointer()andcudamempoolimportpointer()sharing the allocation with an ipc event used to guarantee that the allocation isnt accessed in the importing process before the allocation is ready. when freeing the allocation, the allocation needs to be freed in the importing process before it is freed in the exporting process. the following code snippet demonstrates the use of cuda ipc events to provide the required synchronization between thecudafreeasyncoperations in both processes. access to the allocation from the importing process is obviously restricted by the free operation in the importing process side. it is worth noting thatcudafreecan be used to free the allocation in both processes and that other stream synchronization apis may be used instead of cuda ipc events.",5
4.4.5.cublasxt<t>syr2k(),#cublasxt-t-syr2k,"4.4.5.cublasxt<t>syr2k() this function performs the symmetric rank-\(2k\)update \(c = \alpha(\text{op}(a)\text{op}(b)^{t} + \text{op}(b)\text{op}(a)^{t}) + \beta c\) where\(\alpha\)and\(\beta\)are scalars,\(c\)is a symmetric matrix stored in lower or upper mode, and\(a\)and\(b\)are matrices with dimensions\(\text{op}(a)\)\(n \times k\)and\(\text{op}(b)\)\(n \times k\), respectively. also, for matrix\(a\)and\(b\) \(\text{op(}a\text{) and op(}b\text{)} = \left\{ \begin{matrix}
{a\text{ and }b} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_n}$}} \\
{a^{t}\text{ and }b^{t}} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_t}$}} \\
\end{matrix} \right.\) the possible error values returned by this function and their meanings are listed below. for references please refer to: ssyr2k,dsyr2k,csyr2k,zsyr2k",3
4.4.6.cublasxt<t>syrkx(),#cublasxt-t-syrkx,"4.4.6.cublasxt<t>syrkx() this function performs a variation of the symmetric rank-\(k\)update \(c = \alpha(\text{op}(a)\text{op}(b)^{t} + \beta c\) where\(\alpha\)and\(\beta\)are scalars,\(c\)is a symmetric matrix stored in lower or upper mode, and\(a\)and\(b\)are matrices with dimensions\(\text{op}(a)\)\(n \times k\)and\(\text{op}(b)\)\(n \times k\), respectively. also, for matrix\(a\)and\(b\) \(\text{op(}a\text{) and op(}b\text{)} = \left\{ \begin{matrix}
{a\text{ and }b} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_n}$}} \\
{a^{t}\text{ and }b^{t}} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_t}$}} \\
\end{matrix} \right.\) this routine can be used when b is in such way that the result is guaranteed to be symmetric. an usual example is when the matrix b is a scaled form of the matrix a : this is equivalent to b being the product of the matrix a and a diagonal matrix. the possible error values returned by this function and their meanings are listed below. for references please refer to: ssyrk,dsyrk,csyrk,zsyrkand ssyr2k,dsyr2k,csyr2k,zsyr2k",3
11.11.4.ipc export pool limitations,#ipc-export-pool-limitations,"11.11.4.ipc export pool limitations ipc pools currently do not support releasing physical blocks back to the os. as a result thecudamempooltrimtoapi acts as a no-op and thecudamempoolattrreleasethresholdeffectively gets ignored. this behavior is controlled by the driver, not the runtime and may change in a future driver update.",5
11.11.5.ipc import pool limitations,#ipc-import-pool-limitations,"11.11.5.ipc import pool limitations allocating from an import pool is not allowed; specifically, import pools cannot be set current and cannot be used in thecudamallocfrompoolasyncapi. as such, the allocation reuse policy attributes are meaningless for these pools. ipc pools currently do not support releasing physical blocks back to the os. as a result thecudamempooltrimtoapi acts as a no-op and thecudamempoolattrreleasethresholdeffectively gets ignored. the resource usage stat attribute queries only reflect the allocations imported into the process and the associated physical memory.",5
4.4.7.cublasxt<t>herk(),#cublasxt-t-herk,"4.4.7.cublasxt<t>herk() this function performs the hermitian rank-\(k\)update \(c = \alpha\text{op}(a)\text{op}(a)^{h} + \beta c\) where\(\alpha\)and\(\beta\)are scalars,\(c\)is a hermitian matrix stored in lower or upper mode, and\(a\)is a matrix with dimensions\(\text{op}(a)\)\(n \times k\). also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) the possible error values returned by this function and their meanings are listed below. for references please refer to: cherk,zherk",3
4.4.8.cublasxt<t>her2k(),#cublasxt-t-her2k,"4.4.8.cublasxt<t>her2k() this function performs the hermitian rank-\(2k\)update \(c = \alpha\text{op}(a)\text{op}(b)^{h} + \overset{}{\alpha}\text{op}(b)\text{op}(a)^{h} + \beta c\) where\(\alpha\)and\(\beta\)are scalars,\(c\)is a hermitian matrix stored in lower or upper mode, and\(a\)and\(b\)are matrices with dimensions\(\text{op}(a)\)\(n \times k\)and\(\text{op}(b)\)\(n \times k\), respectively. also, for matrix\(a\)and\(b\) \(\text{op(}a\text{) and op(}b\text{)} = \left\{ \begin{matrix}
{a\text{ and }b} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_n}$}} \\
{a^{h}\text{ and }b^{h}} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) the possible error values returned by this function and their meanings are listed below. for references please refer to: cher2k,zher2k",3
4.4.9.cublasxt<t>herkx(),#cublasxt-t-herkx,"4.4.9.cublasxt<t>herkx() this function performs a variation of the hermitian rank-\(k\)update \(c = \alpha\text{op}(a)\text{op}(b)^{h} + \beta c\) where\(\alpha\)and\(\beta\)are scalars,\(c\)is a hermitian matrix stored in lower or upper mode, and\(a\)and\(b\)are matrices with dimensions\(\text{op}(a)\)\(n \times k\)and\(\text{op}(b)\)\(n \times k\), respectively. also, for matrix\(a\)and\(b\) \(\text{op(}a\text{) and op(}b\text{)} = \left\{ \begin{matrix}
{a\text{ and }b} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_n}$}} \\
{a^{h}\text{ and }b^{h}} & {\text{if }\textsf{trans == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) this routine can be used when the matrix b is in such way that the result is guaranteed to be hermitian. an usual example is when the matrix b is a scaled form of the matrix a : this is equivalent to b being the product of the matrix a and a diagonal matrix. for an efficient computation of the product of a regular matrix with a diagonal matrix, refer to the routinecublasxt<t>dgmm. the possible error values returned by this function and their meanings are listed below. for references please refer to: cherk,zherkand cher2k,zher2k",3
4.4.10.cublasxt<t>trsm(),#cublasxt-t-trsm,"4.4.10.cublasxt<t>trsm() this function solves the triangular linear system with multiple right-hand-sides \(\left\{ \begin{matrix}
{\text{op}(a)x = \alpha b} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_left}$}} \\
{x\text{op}(a) = \alpha b} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_right}$}} \\
\end{matrix} \right.\) where\(a\)is a triangular matrix stored in lower or upper mode with or without the main diagonal,\(x\)and\(b\)are\(m \times n\)matrices, and\(\alpha\)is a scalar. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) the solution\(x\)overwrites the right-hand-sides\(b\)on exit. no test for singularity or near-singularity is included in this function. the possible error values returned by this function and their meanings are listed below. for references please refer to: strsm,dtrsm,ctrsm,ztrsm",3
4.4.11.cublasxt<t>trmm(),#cublasxt-t-trmm,"4.4.11.cublasxt<t>trmm() this function performs the triangular matrix-matrix multiplication \(c = \left\{ \begin{matrix}
{\alpha\text{op}(a)b} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_left}$}} \\
{\alpha b\text{op}(a)} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_right}$}} \\
\end{matrix} \right.\) where\(a\)is a triangular matrix stored in lower or upper mode with or without the main diagonal,\(b\)and\(c\)are\(m \times n\)matrix, and\(\alpha\)is a scalar. also, for matrix\(a\) \(\text{op}(a) = \left\{ \begin{matrix}
a & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_n}$}} \\
a^{t} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_t}$}} \\
a^{h} & {\text{if }\textsf{transa == $\mathrm{cublas\_op\_c}$}} \\
\end{matrix} \right.\) notice that in order to achieve better parallelism, similarly to the cublas api, cublasxt api differs from the blas api for this routine. the blas api assumes an in-place implementation (with results written back to b), while the cublasxt api assumes an out-of-place implementation (with results written into c). the application can still obtain the in-place functionality of blas in the cublasxt api by passing the address of the matrix b in place of the matrix c. no other overlapping in the input parameters is supported. the possible error values returned by this function and their meanings are listed below. for references please refer to: strmm,dtrmm,ctrmm,ztrmm",3
4.4.12.cublasxt<t>spmm(),#cublasxt-t-spmm,"4.4.12.cublasxt<t>spmm() this function performs the symmetric packed matrix-matrix multiplication \(c = \left\{ \begin{matrix}
{\alpha ab + \beta c} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_left}$}} \\
{\alpha ba + \beta c} & {\text{if }\textsf{side == $\mathrm{cublas\_side\_right}$}} \\
\end{matrix} \right.\) where\(a\)is a\(n \times n\)symmetric matrix stored in packed format,\(b\)and\(c\)are\(m \times n\)matrices, and\(\alpha\)and\(\beta\)are scalars. ifuplo==cublas_fill_mode_lowerthen the elements in the lower triangular part of the symmetric matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+((2*n-j+1)*j)/2]for\(j = 1,\ldots,n\)and\(i \geq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. ifuplo==cublas_fill_mode_upperthen the elements in the upper triangular part of the symmetric matrix\(a\)are packed together column by column without gaps, so that the element\(a(i,j)\)is stored in the memory locationap[i+(j*(j+1))/2]for\(j = 1,\ldots,n\)and\(i \leq j\). consequently, the packed format requires only\(\frac{n(n + 1)}{2}\)elements for storage. the possible error values returned by this function and their meanings are listed below. for references please refer to: ssymm,dsymm,csymm,zsymm",3
5.using the cublasdx api,#using-the-cublasdx-api,"5.using the cublasdx api the cublasdx library (preview) is a device side api extension for performing blas calculations inside cuda kernels.
by fusing numerical operations you can decrease latency and further improve performance of your applications.",3
6.using the cublas legacy api,#using-the-cublas-legacy-api,"6.using the cublas legacy api this section does not provide a full reference of each legacy api datatype and entry point. instead, it describes how to use the api, especially where this is different from the regular cublas api. note that in this section, all references to the cublas library refer to the legacy cublas api only.",3
6.1.error status,#id99,"6.1.error status thecublasstatustype is used for function status returns. the cublas library helper functions return status directly, while the status of core functions can be retrieved usingcublasgeterror(). notice that reading the error status viacublasgeterror(), resets the internal error state tocublas_status_success. currently, the following values for are defined: this legacy type corresponds to typecublasstatus_tin the cublas library api.",3
6.2.initialization and shutdown,#initialization-and-shutdown,6.2.initialization and shutdown the functionscublasinit()andcublasshutdown()are used to initialize and shutdown the cublas library. it is recommended forcublasinit()to be called before any other function is invoked. it allocates hardware resources on the gpu device that is currently bound to the host thread from which it was invoked. the legacy initialization and shutdown functions are similar to the cublas library api routinescublascreate()andcublasdestroy().,3
6.3.thread safety,#id100,6.3.thread safety the legacy api is not thread safe when used with multiple host threads and devices. it is recommended to be used only when utmost compatibility with fortran is required and when a single host thread is used to setup the library and make all the functions calls.,5
11.12.synchronization api actions,#synchronization-api-actions,"11.12.synchronization api actions one of the optimizations that comes with the allocator being part of the cuda driver is integration with the synchronize apis. when the user requests that the cuda driver synchronize, the driver waits for asynchronous work to complete. before returning, the driver will determine what frees the synchronization guaranteed to be completed. these allocations are made available for allocation regardless of specified stream or disabled allocation policies. the driver also checkscudamempoolattrreleasethresholdhere and releases any excess physical memory that it can.",5
6.4.memory management,#memory-management,"6.4.memory management the memory used by the legacy cublas library api is allocated and released using functionscublasalloc()andcublasfree(), respectively. these functions create and destroy an object in the gpu memory space capable of holding an array ofnelements, where each element requireselemsizebytes of storage. please see the legacy cublas api header file cublas.h for the prototypes of these functions. the functioncublasalloc()is a wrapper around the functioncudamalloc(), therefore device pointers returned bycublasalloc()can be passed to any cuda device kernel functions. however, these device pointers can not be dereferenced in the host code. the functioncublasfree()is a wrapper around the functioncudafree().",5
6.5.scalar parameters,#id101,"6.5.scalar parameters in the legacy cublas api, scalar parameters are passed by value from the host. also, the few functions that do return a scalar result, such as dot() and nrm2(), return the resulting value on the host, and hence these routines will wait for kernel execution on the device to complete before returning, which makes parallelism with streams impractical. however, the majority of functions do not return any value, in order to be more compatible with fortran and the existing blas libraries.",3
6.6.helper functions,#helper-functions,6.6.helper functions in this section we list the helper functions provided by the legacy cublas api and their functionality. for the exact prototypes of these functions please refer to the legacy cublas api header file cublas.h.,3
"6.7.level-1,2,3 functions",#level-1-2-3-functions,"6.7.level-1,2,3 functions the level-1,2,3 cublas functions (also called core functions) have the same name and behavior as the ones listed in the chapters 3, 4 and 5 in this document. please refer to the legacy cublas api header file cublas.h for their exact prototype. also, the next section talks a bit more about the differences between the legacy and the cublas api prototypes, more specifically how to convert the function calls from one api to another.",3
11.13.addendums,#addendums,11.13.addendums,9
6.8.converting legacy to the cublas api,#converting-legacy-to-the-cublas-api,6.8.converting legacy to the cublas api there are a few general rules that can be used to convert from legacy to the cublas api:,3
6.9.examples,#examples,6.9.examples for sample code references that use the legacy cublas api please see the two examples below. they show an application written in c using the legacy cublas library api with two indexing styles (example a.1. application using c and cublas: 1-based indexing and example a.2. application using c and cublas: 0-based indexing). this application is analogous to the one using the cublas library api that is shown in the introduction chapter. example a.1. application using c and cublas: 1-based indexing example a.2. application using c and cublas: 0-based indexing,3
11.13.1.cudamemcpyasync current context/device sensitivity,#cudamemcpyasync-current-context-device-sensitivity,"11.13.1.cudamemcpyasync current context/device sensitivity in the current cuda driver, any asyncmemcpyinvolving memory fromcudamallocasyncshould be done using the specified streams context as the calling threads current context. this is not necessary forcudamemcpypeerasync, as the device primary contexts specified in the api are referenced instead of the current context.",5
7.cublas fortran bindings,#cublas-fortran-bindings,"7.cublas fortran bindings the cublas library is implemented using the c-based cuda toolchain. thus, it provides a c-style api. this makes interfacing to applications written in c and c++ trivial, but the library can also be used by applications written in fortran. in particular, the cublas library uses 1-based indexing and fortran-style column-major storage for multidimensional data to simplify interfacing to fortran applications. unfortunately, fortran-to-c calling conventions are not standardized and differ by platform and toolchain. in particular, differences may exist in the following areas: to provide maximum flexibility in addressing those differences, the cublas fortran interface is provided in the form of wrapper functions and is part of the toolkit delivery. the c source code of those wrapper functions is located in thesrcdirectory and provided in two different forms: the code of one of those two files needs to be compiled into an application for it to call the cublas api functions. providing source code allows users to make any changes necessary for a particular platform and toolchain. the code in those two c files has been used to demonstrate interoperability with the compilers g77 3.2.3 and g95 0.91 on 32-bit linux, g77 3.4.5 and g95 0.91 on 64-bit linux, intel fortran 9.0 and intel fortran 10.0 on 32-bit and 64-bit microsoft windows xp, and g77 3.4.0 and g95 0.92 on mac os x. note that for g77, use of the compiler flag-fno-second-underscoreis required to use these wrappers as provided. also, the use of the default calling conventions with regard to argument and return value passing is expected. using the flag -fno-f2c changes the default calling convention with respect to these two items. the thunking wrappers allow interfacing to existing fortran applications without any changes to the application. during each call, the wrappers allocate gpu memory, copy source data from cpu memory space to gpu memory space, call cublas, and finally copy back the results to cpu memory space and deallocate the gpu memory. as this process causes very significant call overhead, these wrappers are intended for light testing, not for production code. to use the thunking wrappers, the application needs to be compiled with the filefortran_thunking.c. the direct wrappers, intended for production code, substitute device pointers for vector and matrix arguments in all blas functions. to use these interfaces, existing applications need to be modified slightly to allocate and deallocate data structures in gpu memory space (usingcublas_allocandcublas_free) and to copy data between gpu and cpu memory spaces (usingcublas_set_vector,cublas_get_vector,cublas_set_matrix, andcublas_get_matrix). the sample wrappers provided infortran.cmap device pointers to the os-dependent typesize_t, which is 32-bit wide on 32-bit platforms and 64-bit wide on a 64-bit platforms. one approach to deal with index arithmetic on device pointers in fortran code is to use c-style macros, and use the c preprocessor to expand these, as shown in the example below. on linux and mac os x, one way of pre-processing is to use the option-e-xf77-cpp-inputwhen using g77 compiler, or simply the option-cppwhen using g95 or gfortran. on windows platforms with microsoft visual c/c++, using cl -ep achieves similar results. when traditional fixed-form fortran 77 code is ported to use the cublas library, line length often increases when the blas calls are exchanged for cublas calls. longer function names and possible macro expansion are contributing factors. inadvertently exceeding the maximum line length can lead to run-time errors that are difficult to find, so care should be taken not to exceed the 72-column limit if fixed form is retained. the examples in this chapter show a small application implemented in fortran 77 on the host and the same application with the non-thunking wrappers after it has been ported to use the cublas library. the second example should be compiled with arch_64 defined as 1 on 64-bit os system and as 0 on 32-bit os system. for example for g95 or gfortran, this can be done directly on the command line by using the option-cpp-darch_64=1.   code block continues below. space added for formatting purposes.  ",3
11.13.2.cupointergetattribute query,#cupointergetattribute-query,"11.13.2.cupointergetattribute query invokingcupointergetattributeon an allocation after invokingcudafreeasyncon it results in undefined behavior. specifically, it does not matter if an allocation is still accessible from a given stream: the behavior is still undefined.",6
8.interaction with other libraries and tools,#interaction-with-other-libraries-and-tools,8.interaction with other libraries and tools this section describes important requirements and recommendations that ensure correct use of cublas with other libraries and utilities.,3
11.13.3.cugraphaddmemsetnode,#cugraphaddmemsetnode,"11.13.3.cugraphaddmemsetnode cugraphaddmemsetnodedoes not work with memory allocated via the stream ordered allocator. however, memsets of the allocations can be stream captured.",5
8.1.nvprune,#nvprune,"8.1.nvprune nvpruneenables pruning relocatable host objects and static libraries to only contain device code for the specific target architectures. in case of cublas, particular care must be taken if usingnvprunewith compute capabilities, whose minor revision number is different than 0. to reduce binary size, cublas may only store major revision equivalents of cuda binary files for kernels reused between different minor revision versions. therefore, to ensure that a pruned library does not fail for arbitrary problems, the user must keep binaries for a selected architecture and all prior minor architectures in its major architecture. for example, the following call pruneslibcublas_static.ato contain only sm_75 (turing) and sm_70 (volta) cubins: which should be used instead of:",5
11.13.4.pointer attributes,#pointer-attributes,"11.13.4.pointer attributes thecupointergetattributesquery works on stream ordered allocations. since stream ordered allocations are not context associated, queryingcu_pointer_attribute_contextwill succeed but return null in*data. the attributecu_pointer_attribute_device_ordinalcan be used to determine the location of the allocation: this can be useful when selecting a context for making p2h2p copies usingcudamemcpypeerasync. the attributecu_pointer_attribute_mempool_handlewas added in cuda 11.3 and can be useful for debugging and for confirming which pool an allocation comes from before doing ipc.",8
9.acknowledgements,#acknowledgements,9.acknowledgements nvidia would like to thank the following individuals and institutions for their contributions:,4
12.graph memory nodes,#graph-memory-nodes,12.graph memory nodes,5
12.1.introduction,#graph-memory-nodes-intro,"12.1.introduction graph memory nodes allow graphs to create and own memory allocations. graph memory nodes have gpu ordered lifetime semantics, which dictate when memory is allowed to be accessed on the device. these gpu ordered lifetime semantics enable driver-managed memory reuse, and match those of the stream ordered allocation apiscudamallocasyncandcudafreeasync, which may be captured when creating a graph. graph allocations have fixed addresses over the life of a graph including repeated instantiations and launches. this allows the memory to be directly referenced by other operations within the graph without the need of a graph update, even when cuda changes the backing physical memory. within a graph, allocations whose graph ordered lifetimes do not overlap may use the same underlying physical memory. cuda may reuse the same physical memory for allocations across multiple graphs, aliasing virtual address mappings according to the gpu ordered lifetime semantics. for example when different graphs are launched into the same stream, cuda may virtually alias the same physical memory to satisfy the needs of allocations which have single-graph lifetimes.",5
12.2.support and compatibility,#support-and-compatibility,"12.2.support and compatibility graph memory nodes require an 11.4 capable cuda driver and support for the stream ordered allocator on the gpu. the following snippet shows how to check for support on a given device. doing the attribute query inside the driver version check avoids an invalid value return code on 11.0 and 11.1 drivers. be aware that the compute sanitizer emits warnings when it detects cuda returning error codes, and a version check before reading the attribute will avoid this. graph memory nodes are only supported on driver versions 11.4 and newer.",5
12.3.api fundamentals,#api-fundamentals,"12.3.api fundamentals graph memory nodes are graph nodes representing either memory allocation or free actions. as a shorthand, nodes that allocate memory are called allocation nodes. likewise, nodes that free memory are called free nodes. allocations created by allocation nodes are called graph allocations. cuda assigns virtual addresses for the graph allocation at node creation time. while these virtual addresses are fixed for the lifetime of the allocation node, the allocation contents are not persistent past the freeing operation and may be overwritten by accesses referring to a different allocation. graph allocations are considered recreated every time a graph runs. a graph allocations lifetime, which differs from the nodes lifetime, begins when gpu execution reaches the allocating graph node and ends when one of the following occurs: just like othergraph nodes, graph memory nodes are ordered within a graph by dependency edges. a program must guarantee that operations accessing graph memory: graph allocation lifetimes begin and usually end according to gpu execution (as opposed to api invocation). gpu ordering is the order that work runs on the gpu as opposed to the order that the work is enqueued or described. thus, graph allocations are considered gpu ordered.",5
12.3.1.graph node apis,#graph-node-apis,"12.3.1.graph node apis graph memory nodes may be explicitly created with the memory node creation apis,cudagraphaddmemallocnodeandcudagraphaddmemfreenode. the address allocated bycudagraphaddmemallocnodeis returned to the user in thedptrfield of the passedcuda_mem_alloc_node_paramsstructure. all operations using graph allocations inside the allocating graph must be ordered after the allocating node. similarly, any free nodes must be ordered after all uses of the allocation within the graph.cudagraphaddmemfreenodecreates free nodes. in the following figure, there is an example graph with an alloc and a free node. kernel nodesa,b, andcare ordered after the allocation node and before the free node such that the kernels can access the allocation. kernel nodeeis not ordered after the alloc node and therefore cannot safely access the memory. kernel nodedis not ordered before the free node, therefore it cannot safely access the memory. the following code snippet establishes the graph in this figure:",5
12.3.2.stream capture,#stream-capture,"12.3.2.stream capture graph memory nodes can be created by capturing the corresponding stream ordered allocation and free callscudamallocasyncandcudafreeasync. in this case, the virtual addresses returned by the captured allocation api can be used by other operations inside the graph. since the stream ordered dependencies will be captured into the graph, the ordering requirements of the stream ordered allocation apis guarantee that the graph memory nodes will be properly ordered with respect to the captured stream operations (for correctly written stream code). ignoring kernel nodesdande, for clarity, the following code snippet shows how to use stream capture to create the graph from the previous figure:",5
12.3.3.accessing and freeing graph memory outside of the allocating graph,#accessing-and-freeing-graph-memory-outside-of-the-allocating-graph,"12.3.3.accessing and freeing graph memory outside of the allocating graph graph allocations do not have to be freed by the allocating graph. when a graph does not free an allocation, that allocation persists beyond the execution of the graph and can be accessed by subsequent cuda operations. these allocations may be accessed in another graph or directly using a stream operation as long as the accessing operation is ordered after the allocation through cuda events and other stream ordering mechanisms. an allocation may subsequently be freed by regular calls tocudafree,cudafreeasync, or by the launch of another graph with a corresponding free node, or a subsequent launch of the allocating graph (if it was instantiated with thecudagraphinstantiateflagautofreeonlaunchflag). it is illegal to access memory after it has been freed - the free operation must be ordered after all operations accessing the memory using graph dependencies, cuda events, and other stream ordering mechanisms. the following code snippets demonstrate accessing graph allocations outside of the allocating graph with ordering properly established by: using a single stream, using events between streams, and using events baked into the allocating and freeing graph. ordering established by using a single stream: ordering established by recording and waiting on cuda events: ordering established by using graph external event nodes:",5
12.3.4.cudagraphinstantiateflagautofreeonlaunch,#cudagraphinstantiateflagautofreeonlaunch,"12.3.4.cudagraphinstantiateflagautofreeonlaunch under normal circumstances, cuda will prevent a graph from being relaunched if it has unfreed memory allocations because multiple allocations at the same address will leak memory. instantiating a graph with thecudagraphinstantiateflagautofreeonlaunchflag allows the graph to be relaunched while it still has unfreed allocations. in this case, the launch automatically inserts an asynchronous free of the unfreed allocations. auto free on launch is useful for single-producer multiple-consumer algorithms. at each iteration, a producer graph creates several allocations, and, depending on runtime conditions, a varying set of consumers accesses those allocations. this type of variable execution sequence means that consumers cannot free the allocations because a subsequent consumer may require access. auto free on launch means that the launch loop does not need to track the producers allocations - instead, that information remains isolated to the producers creation and destruction logic. in general, auto free on launch simplifies an algorithm which would otherwise need to free all the allocations owned by a graph before each relaunch.",5
12.4.optimized memory reuse,#optimized-memory-reuse,12.4.optimized memory reuse cuda reuses memory in two ways:,5
12.4.1.address reuse within a graph,#address-reuse-within-a-graph,"12.4.1.address reuse within a graph cuda may reuse memory within a graph by assigning the same virtual address ranges to different allocations whose lifetimes do not overlap. since virtual addresses may be reused, pointers to different allocations with disjoint lifetimes are not guaranteed to be unique. the following figure shows adding a new allocation node (2) that can reuse the address freed by a dependent node (1).",5
12.4.2.physical memory management and sharing,#physical-memory-management-and-sharing,"12.4.2.physical memory management and sharing cuda is responsible for mapping physical memory to the virtual address before the allocating node is reached in gpu order. as an optimization for memory footprint and mapping overhead, multiple graphs may use the same physical memory for distinct allocations if they will not run simultaneously; however, physical pages cannot be reused if they are bound to more than one executing graph at the same time, or to a graph allocation which remains unfreed. cuda may update physical memory mappings at any time during graph instantiation, launch, or execution. cuda may also introduce synchronization between future graph launches in order to prevent live graph allocations from referring to the same physical memory. as for any allocate-free-allocate pattern, if a program accesses a pointer outside of an allocations lifetime, the erroneous access may silently read or write live data owned by another allocation (even if the virtual address of the allocation is unique). use of compute sanitizer tools can catch this error. the following figure shows graphs sequentially launched in the same stream. in this example, each graph frees all the memory it allocates. since the graphs in the same stream never run concurrently, cuda can and should use the same physical memory to satisfy all the allocations.",5
12.5.performance considerations,#performance-considerations,"12.5.performance considerations when multiple graphs are launched into the same stream, cuda attempts to allocate the same physical memory to them because the execution of these graphs cannot overlap. physical mappings for a graph are retained between launches as an optimization to avoid the cost of remapping. if, at a later time, one of the graphs is launched such that its execution may overlap with the others (for example if it is launched into a different stream) then cuda must perform some remapping because concurrent graphs require distinct memory to avoid data corruption. in general, remapping of graph memory in cuda is likely caused by these operations: remapping must happen in execution order, but after any previous execution of that graph is complete (otherwise memory that is still in use could be unmapped). due to this ordering dependency, as well as because mapping operations are os calls, mapping operations can be relatively expensive. applications can avoid this cost by launching graphs containing allocation memory nodes consistently into the same stream.",5
12.5.1.first launch / cudagraphupload,#first-launch-cudagraphupload,"12.5.1.first launch / cudagraphupload physical memory cannot be allocated or mapped during graph instantiation because the stream in which the graph will execute is unknown. mapping is done instead during graph launch. callingcudagraphuploadcan separate out the cost of allocation from the launch by performing all mappings for that graph immediately and associating the graph with the upload stream. if the graph is then launched into the same stream, it will launch without any additional remapping. using different streams for graph upload and graph launch behaves similarly to switching streams, likely resulting in remap operations. in addition, unrelated memory pool management is permitted to pull memory from an idle stream, which could negate the impact of the uploads.",5
12.6.physical memory footprint,#physical-memory-footprint,"12.6.physical memory footprint the pool-management behavior of asynchronous allocation means that destroying a graph which contains memory nodes (even if their allocations are free) will not immediately return physical memory to the os for use by other processes. to explicitly release memory back to the os, an application should use thecudadevicegraphmemtrimapi. cudadevicegraphmemtrimwill unmap and release any physical memory reserved by graph memory nodes that is not actively in use. allocations that have not been freed and graphs that are scheduled or running are considered to be actively using the physical memory and will not be impacted. use of the trim api will make physical memory available to other allocation apis and other applications or processes, but will cause cuda to reallocate and remap memory when the trimmed graphs are next launched. note thatcudadevicegraphmemtrimoperates on a different pool fromcudamempooltrimto(). the graph memory pool is not exposed to the steam ordered memory allocator. cuda allows applications to query their graph memory footprint through thecudadevicegetgraphmemattributeapi. querying the attributecudagraphmemattrreservedmemcurrentreturns the amount of physical memory reserved by the driver for graph allocations in the current process. queryingcudagraphmemattrusedmemcurrentreturns the amount of physical memory currently mapped by at least one graph. either of these attributes can be used to track when new physical memory is acquired by cuda for the sake of an allocating graph. both of these attributes are useful for examining how much memory is saved by the sharing mechanism.",5
12.7.peer access,#peer-access,"12.7.peer access graph allocations can be configured for access from multiple gpus, in which case cuda will map the allocations onto the peer gpus as required. cuda allows graph allocations requiring different mappings to reuse the same virtual address. when this occurs, the address range is mapped onto all gpus required by the different allocations. this means an allocation may sometimes allow more peer access than was requested during its creation; however, relying on these extra mappings is still an error.",5
12.7.1.peer access with graph node apis,#peer-access-with-graph-node-apis,"12.7.1.peer access with graph node apis thecudagraphaddmemallocnodeapi accepts mapping requests in theaccessdescsarray field of the node parameters structures. thepoolprops.locationembedded structure specifies the resident device for the allocation. access from the allocating gpu is assumed to be needed, thus the application does not need to specify an entry for the resident device in theaccessdescsarray.",5
12.7.2.peer access with stream capture,#peer-access-with-stream-capture,"12.7.2.peer access with stream capture for stream capture, the allocation node records the peer accessibility of the allocating pool at the time of the capture. altering the peer accessibility of the allocating pool after acudamallocfrompoolasynccall is captured does not affect the mappings that the graph will make for the allocation.",5
13.mathematical functions,#mathematical-functions-appendix,"13.mathematical functions the reference manual lists, along with their description, all the functions of the c/c++ standard library mathematical functions that are supported in device code, as well as all intrinsic functions (that are only supported in device code). this section provides accuracy information for some of these functions when applicable. it uses ulp for quantification. for further information on the definition of the unit in the last place (ulp), please see jean-michel mullers paperon the definition of ulp(x), rr-5504, lip rr-2005-09, inria, lip. 2005, pp.16 athttps://hal.inria.fr/inria-00070503/document. mathematical functions supported in device code do not set the globalerrnovariable, nor report any floating-point exceptions to indicate errors; thus, if error diagnostic mechanisms are required, the user should implement additional screening for inputs and outputs of the functions. the user is responsible for the validity of pointer arguments. the user must not pass uninitialized parameters to the mathematical functions as this may result in undefined behavior: functions are inlined in the user program and thus are subject to compiler optimizations.",6
13.1.standard functions,#standard-functions,"13.1.standard functions the functions from this section can be used in both host and device code. this section specifies the error bounds of each function when executed on the device and also when executed on the host in the case where the host does not supply the function. the error bounds are generated from extensive but not exhaustive tests, so they are not guaranteed bounds. single-precision floating-point functions addition and multiplication are ieee-compliant, so have a maximum error of 0.5 ulp. the recommended way to round a single-precision floating-point operand to an integer, with the result being a single-precision floating-point number isrintf(), notroundf(). the reason is thatroundf()maps to a 4-instruction sequence on the device, whereasrintf()maps to a single instruction.truncf(),ceilf(), andfloorf()each map to a single instruction as well. double-precision floating-point functions the recommended way to round a double-precision floating-point operand to an integer, with the result being a double-precision floating-point number isrint(), notround(). the reason is thatround()maps to a 5-instruction sequence on the device, whereasrint()maps to a single instruction.trunc(),ceil(), andfloor()each map to a single instruction as well.",6
13.2.intrinsic functions,#intrinsic-functions,"13.2.intrinsic functions the functions from this section can only be used in device code. among these functions are the less accurate, but faster versions of some of the functions ofstandard functions.
they have the same name prefixed with__(such as__sinf(x)).
they are faster as they map to fewer native instructions.
the compiler has an option (-use_fast_math) that forces each function intable 15to compile to its intrinsic counterpart. in addition to reducing the accuracy of the affected functions,
it may also cause some differences in special case handling. a more robust approach is to selectively replace
mathematical function calls by calls to intrinsic functions only where it is merited by the performance gains
and where changed properties such as reduced accuracy and different special case handling can be tolerated. single-precision floating-point functions __fadd_[rn,rz,ru,rd]()and__fmul_[rn,rz,ru,rd]()map to addition and multiplication operations that the compiler never merges into fmads. by contrast, additions and multiplications generated from the * and + operators will frequently be combined into fmads. functions suffixed with_rnoperate using the round to nearest even rounding mode. functions suffixed with_rzoperate using the round towards zero rounding mode. functions suffixed with_ruoperate using the round up (to positive infinity) rounding mode. functions suffixed with_rdoperate using the round down (to negative infinity) rounding mode. the accuracy of floating-point division varies depending on whether the code is compiled with-prec-div=falseor-prec-div=true. when the code is compiled with-prec-div=false, both the regular division/operator and__fdividef(x,y)have the same accuracy, but for 2126<|y|< 2128,__fdividef(x,y)delivers a result of zero, whereas the/operator delivers the correct result to
within the accuracy stated intable 16.
also, for 2126<|y|< 2128, ifxis infinity,__fdividef(x,y)delivers
anan(as a result of multiplying infinity by zero), while the/operator returns infinity.
on the other hand, the/operator is ieee-compliant when the code is compiled with-prec-div=trueor without any-prec-divoption at all since its default value is true. double-precision floating-point functions __dadd_rn()and__dmul_rn()map to addition and multiplication operations that the compiler never merges into fmads. by contrast, additions and multiplications generated from the * and + operators will frequently be combined into fmads.",6
14.c++ language support,#c-language-support,"14.c++ language support as described incompilation with nvcc, cuda source files compiled withnvcccan include a mix of host code and device code. the cuda front-end compiler aims to emulate the host compiler behavior with respect to c++ input code. the input source code is processed according to the c++ iso/iec 14882:2003, c++ iso/iec 14882:2011, c++ iso/iec 14882:2014 or c++ iso/iec 14882:2017 specifications, and the cuda front-end compiler aims to emulate any host compiler divergences from the iso specification. in addition, the supported language is extended with cuda-specific constructs described in this document13, and is subject to the restrictions described below. c++11 language features,c++14 language featuresandc++17 language featuresprovide support matrices for the c++11, c++14, c++17 and c++20 features, respectively.restrictionslists the language restrictions.polymorphic function wrappersandextended lambdasdescribe additional features.code samplesgives code samples.",0
14.1.c++11 language features,#c-11-language-features,"14.1.c++11 language features the following table lists new language features that have been accepted into the c++11 standard. the proposal column provides a link to the iso c++ committee proposal that describes the feature, while the available in nvcc (device code) column indicates the first version of nvcc that contains an implementation of this feature (if it has been implemented) for device code.",6
14.2.c++14 language features,#c-14-language-features,14.2.c++14 language features the following table lists new language features that have been accepted into the c++14 standard.,6
14.3.c++17 language features,#c-17-language-features,"14.3.c++17 language features all c++17 language features are supported in nvcc version 11.0 and later, subject to restrictions describedhere.",6
14.4.c++20 language features,#c-20-language-features,"14.4.c++20 language features all c++20 language features are supported in nvcc version 12.0 and later, subject to restrictions describedhere.",6
14.5.restrictions,#language-restrictions,14.5.restrictions,9
14.5.1.host compiler extensions,#host-compiler-extensions,14.5.1.host compiler extensions host compiler specific language extensions are not supported in device code. __complextypes are only supported in host code. __int128type is supported in device code when compiled in conjunction with a host compiler that supports it. __float128type is only supported in host code on 64-bit x86 linux platforms. a constant expression of__float128type may be processed by the compiler in a floating point representation with lower precision.,6
14.5.2.preprocessor symbols,#preprocessor-symbols,14.5.2.preprocessor symbols,6
14.5.3.qualifiers,#qualifiers,14.5.3.qualifiers,9
14.5.4.pointers,#pointers,"14.5.4.pointers dereferencing a pointer either to global or shared memory in code that is executed on the host, or to host memory in code that is executed on the device results in an undefined behavior, most often in a segmentation fault and application termination. the address obtained by taking the address of a__device__,__shared__or__constant__variable can only be used in device code. the address of a__device__or__constant__variable obtained throughcudagetsymboladdress()as described indevice memorycan only be used in host code.",6
14.5.5.operators,#operators,14.5.5.operators,6
14.5.6.run time type information (rtti),#run-time-type-information-rtti,"14.5.6.run time type information (rtti) the following rtti-related features are supported in host code, but not in device code.",6
14.5.7.exception handling,#exception-handling,"14.5.7.exception handling exception handling is only supported in host code, but not in device code. exception specification is not supported for__global__functions.",6
14.5.8.standard library,#standard-library,"14.5.8.standard library standard libraries are only supported in host code, but not in device code, unless specified otherwise.",6
14.5.9.namespace reservations,#namespace-reservations,"14.5.9.namespace reservations unless an exception is otherwise noted, it is undefined behavior to add any declarations or definitions tocuda::,nv::,cooperative_groups::or any namespace nested within. examples:",6
14.5.10.functions,#functions,14.5.10.functions,9
14.5.11.classes,#classes,14.5.11.classes,9
14.5.12.templates,#templates,"14.5.12.templates a type or template cannot be used in the type, non-type or template template argument of a__global__function template instantiation or a__device__/__constant__variable instantiation if either: example:",6
14.5.13.trigraphs and digraphs,#trigraphs-and-digraphs,14.5.13.trigraphs and digraphs trigraphs are not supported on any platform. digraphs are not supported on windows.,9
14.5.14.const-qualified variables,#const-qualified-variables,"14.5.14.const-qualified variables let v denote a namespace scope variable or a class static member variable that has const qualified type and does not have execution space annotations (for example,__device__,__constant__,__shared__). v is considered to be a host code variable. the value of v may be directly used in device code, if device source code cannot contain a reference to v or take the address of v. example:",6
14.5.15.long double,#long-double,14.5.15.long double the use oflongdoubletype is not supported in device code.,6
14.5.16.deprecation annotation,#deprecation-annotation,"14.5.16.deprecation annotation nvcc supports the use ofdeprecatedattribute when usinggcc,clang,xlc,iccorpgcchost compilers, and the use ofdeprecateddeclspec when using thecl.exehost compiler. it also supports the[[deprecated]]standard attribute when the c++14 dialect has been enabled. the cuda frontend compiler will generate a deprecation diagnostic for a reference to a deprecated entity from within the body of a__device__,__global__or__host____device__function when__cuda_arch__is defined (i.e., during device compilation phase). other references to deprecated entities will be handled by the host compiler, e.g., a reference from within a__host__function. the cuda frontend compiler does not support thepragmagccdiagnosticorpragmawarningmechanisms supported by various host compilers. therefore, deprecation diagnostics generated by the cuda frontend compiler are not affected by these pragmas, but diagnostics generated by the host compiler will be affected. to suppress the warning for device-code, user can use nvidia specific pragmapragma nv_diag_suppress. thenvccflag-wno-deprecated-declarationscan be used to suppress all deprecation warnings, and the flag-werror=deprecated-declarationscan be used to turn deprecation warnings into errors.",6
14.5.17.noreturn annotation,#noreturn-annotation,"14.5.17.noreturn annotation nvcc supports the use ofnoreturnattribute when usinggcc,clang,xlc,iccorpgcchost compilers, and the use ofnoreturndeclspec when using thecl.exehost compiler. it also supports the[[noreturn]]standard attribute when the c++11 dialect has been enabled. the attribute/declspec can be used in both host and device code.",6
14.5.18.[[likely]] / [[unlikely]] standard attributes,#likely-unlikely-standard-attributes,"14.5.18.[[likely]] / [[unlikely]] standard attributes these attributes are accepted in all configurations that support the c++ standard attribute syntax. the attributes can be used to hint to the device compiler optimizer whether a statement is more or less likely to be executed compared to any alternative path that does not include the statement. example: if these attributes are used in host code when__cuda_arch__is undefined, then they will be present in the code parsed by the host compiler, which may generate a warning if the attributes are not supported. for example,clang11 host compiler will generate an unknown attribute warning.",6
14.5.19.const and pure gnu attributes,#const-and-pure-gnu-attributes,"14.5.19.const and pure gnu attributes these attributes are supported for both host and device functions, when using a language dialect and host compiler that also supports these attributes e.g. with g++ host compiler. for a device function annotated with thepureattribute, the device code optimizer assumes that the function does not change any mutable state visible to caller functions (e.g. memory). for a device function annotated with theconstattribute, the device code optimizer assumes that the function does not access or change any mutable state visible to caller functions (e.g. memory). example:",6
14.5.20.__nv_pure__ attribute,#nv-pure-attribute,"14.5.20.__nv_pure__ attribute the__nv_pure__attributed is supported for both host and device functions. for host functions, when using a language dialect that supports thepuregnu attribute, the__nv_pure__attribute is translated to thepuregnu attribute. similarly when using msvc as the host compiler, the attribute is translated to the msvcnoaliasattribute. when a device function is annotated with the__nv_pure__attribute, the device code optimizer assumes that the function does not change any mutable state visible to caller functions (e.g. memory).",6
14.5.21.intel host compiler specific,#intel-host-compiler-specific,"14.5.21.intel host compiler specific the cuda frontend compiler parser does not recognize some of the intrinsic functions supported by the intel compiler (e.g.icc). when using the intel compiler as a host compiler,nvccwill therefore enable the macro__intel_compiler_use_intrinsic_prototypesduring preprocessing. this macro enables explicit declarations of the intel compiler intrinsic functions in the associated header files, allowingnvccto support use of such functions in host code19.",0
14.5.22.c++11 features,#c-11-features,"14.5.22.c++11 features c++11 features that are enabled by default by the host compiler are also supported by nvcc, subject to the restrictions described in this document. in addition, invoking nvcc with-std=c++11flag turns on all c++11 features and also invokes the host preprocessor, compiler and linker with the corresponding c++11 dialect option20.",6
14.5.23.c++14 features,#c-14-features,"14.5.23.c++14 features c++14 features enabled by default by the host compiler are also supported by nvcc. passing nvcc-std=c++14flag turns on all c++14 features and also invokes the host preprocessor, compiler and linker with the corresponding c++14 dialect option26. this section describes the restrictions on the supported c++14 features.",6
14.5.24.c++17 features,#c-17-features,"14.5.24.c++17 features c++17 features enabled by default by the host compiler are also supported by nvcc. passing nvcc-std=c++17flag turns on all c++17 features and also invokes the host preprocessor, compiler and linker with the corresponding c++17 dialect option27. this section describes the restrictions on the supported c++17 features.",6
14.5.25.c++20 features,#c-20-features,"14.5.25.c++20 features c++20 features enabled by default by the host compiler are also supported by nvcc. passing nvcc-std=c++20flag turns on all c++20 features and also invokes the host preprocessor, compiler and linker with the corresponding c++20 dialect option28. this section describes the restrictions on the supported c++20 features.",6
14.6.polymorphic function wrappers,#polymorphic-function-wrappers,"14.6.polymorphic function wrappers a polymorphic function wrapper class templatenvstd::functionis provided in thenvfunctionalheader. instances of this class template can be used to store, copy and invoke any callable target, e.g., lambda expressions.nvstd::functioncan be used in both host and device code. example: instances ofnvstd::functionin host code cannot be initialized with the address of a__device__function or with a functor whoseoperator()is a__device__function. instances ofnvstd::functionin device code cannot be initialized with the address of a__host__function or with a functor whoseoperator()is a__host__function. nvstd::functioninstances cannot be passed from host code to device code (and vice versa) at run time.nvstd::functioncannot be used in the parameter type of a__global__function, if the__global__function is launched from host code. example: nvstd::functionis defined in thenvfunctionalheader as follows:",6
14.7.extended lambdas,#extended-lambdas,"14.7.extended lambdas the nvcc flag'--extended-lambda'allows explicit execution space annotations in a lambda expression29. the execution space annotations should be present after the lambda-introducer and before the optional lambda-declarator. nvcc will define the macro__cudacc_extended_lambda__when the'--extended-lambda'flag has been specified. an extended__device__lambda is a lambda expression that is annotated explicitly with __device__, and is defined within the immediate or nested block scope of a__host__or__host____device__function. an extended__host____device__lambda is a lambda expression that is annotated explicitly with both __host__ and __device__, and is defined within the immediate or nested block scope of a__host__or__host____device__function. an extended lambda denotes either an extended__device__lambda or an extended__host____device__lambda. extended lambdas can be used in the type arguments of__global__ function template instantiation. if the execution space annotations are not explicitly specified, they are computed based on the scopes enclosing the closure class associated with the lambda, as described in the section on c++11 support. the execution space annotations are applied to all methods of the closure class associated with the lambda. example:",6
14.7.1.extended lambda type traits,#extended-lambda-type-traits,"14.7.1.extended lambda type traits the compiler provides type traits to detect closure types for extended lambdas at compile time: __nv_is_extended_device_lambda_closure_type(type): if type is the closure class created for an extended__device__lambda, then the trait is true, otherwise it is false. __nv_is_extended_device_lambda_with_preserved_return_type(type): if type is the closure class created for an extended__device__lambda and the lambda is defined with trailing return type (with restriction), then the trait is true, otherwise it is false. if the trailing return type definition refers to any lambda parameter name, the return type is not preserved. __nv_is_extended_host_device_lambda_closure_type(type): if type is the closure class created for an extended__host____device__lambda, then the trait is true, otherwise it is false. these traits can be used in all compilation modes, irrespective of whether lambdas or extended lambdas are enabled30. example:",6
14.7.2.extended lambda restrictions,#extended-lambda-restrictions,"14.7.2.extended lambda restrictions the cuda compiler will replace an extended lambda expression with an instance of a placeholder type defined in namespace scope, before invoking the host compiler. the template argument of the placeholder type requires taking the address of a function enclosing the original extended lambda expression. this is required for the correct execution of any__global__function template whose template argument involves the closure type of an extended lambda. theenclosing functionis computed as follows. by definition, the extended lambda is present within the immediate or nested block scope of a__host__or__host____device__function. if this function is not theoperator()of a lambda expression, then it is considered the enclosing function for the extended lambda. otherwise, the extended lambda is defined within the immediate or nested block scope of theoperator()of one or more enclosing lambda expressions. if the outermost such lambda expression is defined in the immediate or nested block scope of a functionf, thenfis the computed enclosing function, else the enclosing function does not exist. example: here are the restrictions on extended lambdas: the cuda compiler will generate compiler diagnostics for a subset of cases described in 1-12; no diagnostic will be generated for cases 13-17, but the host compiler may fail to compile the generated code.",6
14.7.3.notes on __host__ __device__ lambdas,#notes-on-host-device-lambdas,"14.7.3.notes on __host__ __device__ lambdas unlike__device__lambdas,__host____device__lambdas can be called from host code. as described earlier, the cuda compiler replaces an extended lambda expression defined in host code with an instance of a named placeholder type. the placeholder type for an extended__host____device__lambda invokes the original lambdasoperator()with an indirect function call30. the presence of the indirect function call may cause an extended__host____device__lambda to be less optimized by the host compiler than lambdas that are implicitly or explicitly__host__only. in the latter case, the host compiler can easily inline the body of the lambda into the calling context. but in case of an extended__host____device__lambda, the host compiler encounters the indirect function call and may not be able to easily inline the original__host____device__lambda body.",6
14.7.4.*this capture by value,#this-capture-by-value,"14.7.4.*this capture by value when a lambda is defined within a non-static class member function, and the body of the lambda refers to a class member variable, c++11/c++14 rules require that thethispointer of the class is captured by value, instead of the referenced member variable. if the lambda is an extended__device__or__host____device__lambda defined in a host function, and the lambda is executed on the gpu, accessing the referenced member variable on the gpu will cause a run time error if thethispointer points to host memory. example: c++17 solves this problem by adding a new *this capture mode. in this mode, the compiler makes a copy of the object denoted by *this instead of capturing the pointerthisby value. the *this capture mode is described in more detail here:http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0018r3.html. the cuda compiler supports the *this capture mode for lambdas defined within__device__and__global__functions and for extended__device__lambdas defined in host code, when the--extended-lambdanvcc flag is used. heres the above example modified to use *this capture mode: *this capture mode is not allowed for unannotated lambdas defined in host code, or for extended__host____device__lambdas. examples of supported and unsupported usage:",5
14.7.5.additional notes,#additional-notes,14.7.5.additional notes,9
14.8.code samples,#code-samples,14.8.code samples,9
14.8.1.data aggregation class,#data-aggregation-class,14.8.1.data aggregation class,9
14.8.2.derived class,#derived-class,14.8.2.derived class,9
14.8.3.class template,#class-template,14.8.3.class template,9
14.8.4.function template,#function-template,14.8.4.function template,6
14.8.5.functor class,#functor-class,14.8.5.functor class,9
15.texture fetching,#texture-fetching,"15.texture fetching this section gives the formula used to compute the value returned by the texture functions oftexture functionsdepending on the various attributes of the texture object (seetexture and surface memory). the texture bound to the texture object is represented as an arraytof it is fetched using non-normalized texture coordinatesx,y, andz, or the normalized texture coordinatesx/n,y/m, andz/las described intexture memory. in this section, the coordinates are assumed to be in the valid range.texture memoryexplained how out-of-range coordinates are remapped to the valid range based on the addressing mode.",6
15.1.nearest-point sampling,#nearest-point-sampling,"15.1.nearest-point sampling in this filtering mode, the value returned by the texture fetch is wherei=floor(x),j=floor(y), andk=floor(z). figure 32illustrates nearest-point sampling for a one-dimensional texture withn=4. for integer textures, the value returned by the texture fetch can be optionally remapped to [0.0, 1.0] (seetexture memory).",6
15.2.linear filtering,#linear-filtering,"15.2.linear filtering in this filtering mode, which is only available for floating-point textures, the value returned by the texture fetch is where: \(\alpha\),\(\beta\), and\(\gamma\)are stored in 9-bit fixed point format with 8 bits of fractional value (so 1.0 is exactly represented). figure 33illustrates linear filtering of a one-dimensional texture withn=4.",6
15.3.table lookup,#table-lookup,"15.3.table lookup a table lookuptl(x)wherexspans the interval[0,r]can be implemented astl(x)=tex((n-1)/r)x+0.5)in order to ensure thattl(0)=t[0]andtl(r)=t[n-1]. figure 34illustrates the use of texture filtering to implement a table lookup withr=4orr=1from a one-dimensional texture withn=4.",6
16.compute capabilities,#compute-capabilities,"16.compute capabilities the general specifications and features of a compute device depend on its compute capability (seecompute capability). table 20andtable 21show the features and technical specifications associated with each compute capability that is currently supported. floating-point standardreviews the compliance with the ieee floating-point standard. sectionscompute capability 5.x,compute capability 6.x,compute capability 7.x,compute capability 8.xandcompute capability 9.0give more details on the architecture of devices of compute capabilities 5.x, 6.x, 7.x, 8.x and 9.0 respectively.",6
16.1.feature availability,#feature-availability,"16.1.feature availability a compute feature is introduced with a compute architecture with the intention that the feature will be available on all subsequent architectures.  this is shown in table 20 by the yes for availability of a feature on compute capabilities subsequent to its introduction. highly specialized compute features that are introduced with an architecture may not be guaranteed to be available on all subsequent compute capabilities. these features target acceleration of specialized operations which are not intended for all classes of compute capabilities (denoted by the compute capabilitys minor number) or are likely to significantly change on future generations (denoted by the compute capabilitys major number). there are potentially two sets of compute features for a given compute capability: compute capability .: the predominant set of compute features that are introduced with the intent to be available for subsequent compute architectures.  these features and their availability are summarized in table 20. compute capability .a: a small and highly specialized set of features that are introduced to accelerate specialized operations, which are not guaranteed to be available or might change significantly on subsequent compute architecture.  these features are summarized in the respective compute capability . subsection. compilation of device code targets a particular compute capability.  a feature which appears in device code must be available for the targeted compute capability.  for example:",5
16.2.features and technical specifications,#features-and-technical-specifications,"16.2.features and technical specifications note that the kb and k units used in the following table correspond to 1024 bytes (i.e., a kib) and 1024 respectively.",6
16.3.floating-point standard,#floating-point-standard,"16.3.floating-point standard all compute devices follow the ieee 754-2008 standard for binary floating-point arithmetic with the following deviations: code must be compiled with-ftz=false,-prec-div=true, and-prec-sqrt=trueto ensure ieee compliance (this is the default setting; see thenvccuser manual for description of these compilation flags). regardless of the setting of the compiler flag-ftz, in accordance to the ieee-754r standard, if one of the input parameters tofminf(),fmin(),fmaxf(), orfmax()is nan, but not the other, the result is the non-nan parameter. the conversion of a floating-point value to an integer value in the case where the floating-point value falls outside the range of the integer format is left undefined by ieee-754. for compute devices, the behavior is to clamp to the end of the supported range. this is unlike the x86 architecture behavior. the behavior of integer division by zero and integer overflow is left undefined by ieee-754. for compute devices, there is no mechanism for detecting that such integer operation exceptions have occurred. integer division by zero yields an unspecified, machine-specific value. https://developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpusincludes more information on the floating point accuracy and compliance of nvidia gpus.",6
16.4.compute capability 5.x,#compute-capability-5-x,16.4.compute capability 5.x,6
16.4.1.architecture,#architecture,"16.4.1.architecture an sm consists of: when an sm is given warps to execute, it first distributes them among the four schedulers. then, at every instruction issue time, each scheduler issues one instruction for one of its assigned warps that is ready to execute, if any. an sm has: the unified l1/texture cache is also used by the texture unit that implements the various addressing modes and data filtering mentioned intexture and surface memory. there is also an l2 cache shared by all sms that is used to cache accesses to local or global memory, including temporary register spills. applications may query the l2 cache size by checking thel2cachesizedevice property (seedevice enumeration). the cache behavior (e.g., whether reads are cached in both the unified l1/texture cache and l2 or in l2 only) can be partially configured on a per-access basis using modifiers to the load instruction.",5
16.4.2.global memory,#global-memory-5-x,"16.4.2.global memory global memory accesses are always cached in l2. data that is read-only for the entire lifetime of the kernel can also be cached in the unified l1/texture cache described in the previous section by reading it using the__ldg()function (seeread-only data cache load function). when the compiler detects that the read-only condition is satisfied for some data, it will use__ldg()to read it. the compiler might not always be able to detect that the read-only condition is satisfied for some data. marking pointers used for loading such data with both theconstand__restrict__qualifiers increases the likelihood that the compiler will detect the read-only condition. data that is not read-only for the entire lifetime of the kernel cannot be cached in the unified l1/texture cache for devices of compute capability 5.0. for devices of compute capability 5.2, it is, by default, not cached in the unified l1/texture cache, but caching may be enabled using the following mechanisms: when caching is enabled using one of the three mechanisms listed above, devices of compute capability 5.2 will cache global memory reads in the unified l1/texture cache for all kernel launches except for the kernel launches for which thread blocks consume too much of the sms register file. these exceptions are reported by the profiler.",5
16.4.3.shared memory,#shared-memory-5-x,"16.4.3.shared memory shared memory has 32 banks that are organized such that successive 32-bit words map to successive banks. each bank has a bandwidth of 32 bits per clock cycle. a shared memory request for a warp does not generate a bank conflict between two threads that access any address within the same 32-bit word (even though the two addresses fall in the same bank). in that case, for read accesses, the word is broadcast to the requesting threads and for write accesses, each address is written by only one of the threads (which thread performs the write is undefined). figure 22shows some examples of strided access. figure 23shows some examples of memory read accesses that involve the broadcast mechanism.",5
16.5.compute capability 6.x,#compute-capability-6-x,16.5.compute capability 6.x,6
16.5.1.architecture,#architecture-6-x,"16.5.1.architecture an sm consists of: when an sm is given warps to execute, it first distributes them among its schedulers. then, at every instruction issue time, each scheduler issues one instruction for one of its assigned warps that is ready to execute, if any. an sm has: the unified l1/texture cache is also used by the texture unit that implements the various addressing modes and data filtering mentioned intexture and surface memory. there is also an l2 cache shared by all sms that is used to cache accesses to local or global memory, including temporary register spills. applications may query the l2 cache size by checking thel2cachesizedevice property (seedevice enumeration). the cache behavior (e.g., whether reads are cached in both the unified l1/texture cache and l2 or in l2 only) can be partially configured on a per-access basis using modifiers to the load instruction.",5
16.5.2.global memory,#global-memory-6-x,16.5.2.global memory global memory behaves the same way as in devices of compute capability 5.x (seeglobal memory).,5
16.5.3.shared memory,#shared-memory-6-x,16.5.3.shared memory shared memory behaves the same way as in devices of compute capability 5.x (seeshared memory).,5
16.6.compute capability 7.x,#compute-capability-7-x,16.6.compute capability 7.x,6
16.6.1.architecture,#architecture-7-x,"16.6.1.architecture an sm consists of: an sm statically distributes its warps among its schedulers. then, at every instruction issue time, each scheduler issues one instruction for one of its assigned warps that is ready to execute, if any. an sm has: shared memory is partitioned out of unified data cache, and can be configured to various sizes (seeshared memory.) the remaining data cache serves as an l1 cache and is also used by the texture unit that implements the various addressing and data filtering modes mentioned intexture and surface memory.",5
16.6.2.independent thread scheduling,#independent-thread-scheduling,"16.6.2.independent thread scheduling thevoltaarchitecture introducesindependent thread schedulingamong threads in a warp, enabling intra-warp synchronization patterns previously unavailable and simplifying code changes when porting cpu code. however, this can lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity of previous hardware architectures. below are code patterns of concern and suggested corrective actions for volta-safe code. since the intrinsics are available with cuda 9.0+, (if necessary) code can be executed conditionally with the following preprocessor macro: these intrinsics are available on all architectures, not justvoltaorturing, and in most cases a single code-base will suffice for all architectures. note, however, that forpascaland earlier architectures, all threads in mask must execute the same warp intrinsic instruction in convergence, and the union of all values in mask must be equal to the warps active mask. the following code pattern is valid onvolta, but not onpascalor earlier architectures. the replacement for__ballot(1)is__activemask(). note that threads within a warp can diverge even within a single code path. as a result,__activemask()and__ballot(1)may return only a subset of the threads on the current code path. the following invalid code example sets bitiofoutputto 1 whendata[i]is greater thanthreshold.__activemask()is used in an attempt to enable cases wheredatalenis not a multiple of 32. this code is invalid because cuda does not guarantee that the warp will diverge only at the loop condition. when divergence happens for other reasons, conflicting results will be computed for the same 32-bit output element by different subsets of threads in the warp. a correct code might use a non-divergent loop condition together with__ballot_sync()to safely enumerate the set of threads in the warp participating in the threshold calculation as follows. discovery patterndemonstrates a valid use case for__activemask(). theracecheckandsyncchecktools provided bycompute-sanitercan help with locating violations. to aid migration while implementing the above-mentioned corrective actions, developers can opt-in to the pascal scheduling model that does not support independent thread scheduling. seeapplication compatibilityfor details.",5
16.6.3.global memory,#global-memory-7-x,16.6.3.global memory global memory behaves the same way as in devices of compute capability 5.x (seeglobal memory).,5
16.6.4.shared memory,#shared-memory-7-x,"16.6.4.shared memory the amount of the unified data cache reserved for shared memory is configurable on a per kernel basis. for thevoltaarchitecture (compute capability 7.0), the unified data cache has a size of 128 kb, and the shared memory capacity can be set to 0, 8, 16, 32, 64 or 96 kb. for theturingarchitecture (compute capability 7.5), the unified data cache has a size of 96 kb, and the shared memory capacity can be set to either 32 kb or 64 kb. unlike kepler, the driver automatically configures the shared memory capacity for each kernel to avoid shared memory occupancy bottlenecks while also allowing concurrent execution with already launched kernels where possible. in most cases, the drivers default behavior should provide optimal performance. because the driver is not always aware of the full workload, it is sometimes useful for applications to provide additional hints regarding the desired shared memory configuration. for example, a kernel with little or no shared memory use may request a larger carveout in order to encourage concurrent execution with later kernels that require more shared memory. the newcudafuncsetattribute()api allows applications to set a preferred shared memory capacity, orcarveout, as a percentage of the maximum supported shared memory capacity (96 kb forvolta, and 64 kb forturing). cudafuncsetattribute()relaxes enforcement of the preferred shared capacity compared to the legacycudafuncsetcacheconfig()api introduced with kepler. the legacy api treated shared memory capacities as hard requirements for kernel launch. as a result, interleaving kernels with different shared memory configurations would needlessly serialize launches behind shared memory reconfigurations. with the new api, the carveout is treated as a hint. the driver may choose a different configuration if required to execute the function or to avoid thrashing. in addition to an integer percentage, several convenience enums are provided as listed in the code comments above. where a chosen integer percentage does not map exactly to a supported capacity (sm 7.0 devices support shared capacities of 0, 8, 16, 32, 64, or 96 kb), the next larger capacity is used. for instance, in the example above, 50% of the 96 kb maximum is 48 kb, which is not a supported shared memory capacity. thus, the preference is rounded up to 64 kb. compute capability 7.x devices allow a single thread block to address the full capacity of shared memory: 96 kb onvolta, 64 kb onturing. kernels relying on shared memory allocations over 48 kb per block are architecture-specific, as such they must use dynamic shared memory (rather than statically sized arrays) and require an explicit opt-in usingcudafuncsetattribute()as follows. otherwise, shared memory behaves the same way as for devices of compute capability 5.x (seeshared memory).",5
16.7.compute capability 8.x,#compute-capability-8-x,16.7.compute capability 8.x,6
16.7.1.architecture,#architecture-8-x,"16.7.1.architecture a streaming multiprocessor (sm) consists of: an sm statically distributes its warps among its schedulers. then, at every instruction issue time, each scheduler issues one instruction for one of its assigned warps that is ready to execute, if any. an sm has: shared memory is partitioned out of the unified data cache, and can be configured to various sizes (seeshared memorysection). the remaining data cache serves as an l1 cache and is also used by the texture unit that implements the various addressing and data filtering modes mentioned intexture and surface memory.",5
16.7.2.global memory,#global-memory-8-x,16.7.2.global memory global memory behaves the same way as for devices of compute capability 5.x (seeglobal memory).,5
16.7.3.shared memory,#shared-memory-8-x,"16.7.3.shared memory similar to thevolta architecture, the amount of the unified data cache reserved for shared memory is configurable on a per kernel basis. for thenvidia ampere gpu architecture, the unified data cache has a size of 192 kb for devices of compute capability 8.0 and 8.7 and 128 kb for devices of compute capabilities 8.6 and 8.9. the shared memory capacity can be set to 0, 8, 16, 32, 64, 100, 132 or 164 kb for devices of compute capability 8.0 and 8.7, and to 0, 8, 16, 32, 64 or 100 kb for devices of compute capabilities 8.6 and 8.9. an application can set thecarveout, i.e., the preferred shared memory capacity, with thecudafuncsetattribute(). the api can specify the carveout either as an integer percentage of the maximum supported shared memory capacity of 164 kb for devices of compute capability 8.0 and 8.7 and 100 kb for devices of compute capabilities 8.6 and 8.9 respectively, or as one of the following values:{cudasharedmemcarveoutdefault,cudasharedmemcarveoutmaxl1, orcudasharedmemcarveoutmaxshared. when using a percentage, the carveout is rounded up to the nearest supported shared memory capacity. for example, for devices of compute capability 8.0, 50% will map to a 100 kb carveout instead of an 82 kb one. setting thecudafuncattributepreferredsharedmemorycarveoutis considered a hint by the driver; the driver may choose a different configuration, if needed. devices of compute capability 8.0 and 8.7 allow a single thread block to address up to 163 kb of shared memory, while devices of compute capabilities 8.6 and 8.9 allow up to 99 kb of shared memory. kernels relying on shared memory allocations over 48 kb per block are architecture-specific, and must use dynamic shared memory rather than statically sized shared memory arrays. these kernels require an explicit opt-in by usingcudafuncsetattribute()to set thecudafuncattributemaxdynamicsharedmemorysize; seeshared memoryfor the volta architecture. note that the maximum amount of shared memory per thread block is smaller than the maximum shared memory partition available per sm. the 1 kb of shared memory not made available to a thread block is reserved for system use.",5
16.8.compute capability 9.0,#compute-capability-9-0,16.8.compute capability 9.0,6
16.8.1.architecture,#architecture-9-0,"16.8.1.architecture a streaming multiprocessor (sm) consists of: an sm statically distributes its warps among its schedulers. then, at every instruction issue time, each scheduler issues one instruction for one of its assigned warps that is ready to execute, if any. an sm has: shared memory is partitioned out of the unified data cache, and can be configured to various sizes (seeshared memorysection). the remaining data cache serves as an l1 cache and is also used by the texture unit that implements the various addressing and data filtering modes mentioned intexture and surface memory.",5
16.8.2.global memory,#global-memory-9-0,16.8.2.global memory global memory behaves the same way as for devices of compute capability 5.x (seeglobal memory).,5
16.8.3.shared memory,#shared-memory-9-0,"16.8.3.shared memory similar to thenvidia ampere gpu architecture, the amount of the unified data cache reserved for shared memory is configurable on a per kernel basis. for thenvidia h100 tensor core gpu architecture, the unified data cache has a size of 256 kb for devices of compute capability 9.0. the shared memory capacity can be set to 0, 8, 16, 32, 64, 100, 132, 164, 196 or 228 kb. as with thenvidia ampere gpu architecture, an application can configure its preferred shared memory capacity, i.e., thecarveout. devices of compute capability 9.0 allow a single thread block to address up to 227 kb of shared memory. kernels relying on shared memory allocations over 48 kb per block are architecture-specific, and must use dynamic shared memory rather than statically sized shared memory arrays. these kernels require an explicit opt-in by usingcudafuncsetattribute()to set thecudafuncattributemaxdynamicsharedmemorysize; seeshared memoryfor the volta architecture. note that the maximum amount of shared memory per thread block is smaller than the maximum shared memory partition available per sm. the 1 kb of shared memory not made available to a thread block is reserved for system use.",5
16.8.4.features accelerating specialized computations,#features-accelerating-specialized-computations,"16.8.4.features accelerating specialized computations the nvidia hopper gpu architecture includes features to accelerate matrix multiply-accumulate (mma) computations with: this feature set is only available within the cuda compilation toolchain through inline ptx. it is strongly recommended that applications utilize this complex feature set through cuda-x libraries such as cublas, cudnn, or cufft. it is strongly recommended that device kernels utilize this complex feature set throughcutlass, a collection of cuda c++ template abstractions for implementing high-performance matrix-multiplication (gemm) and related computations at all levels and scales within cuda.",5
17.driver api,#driver-api,"17.driver api this section assumes knowledge of the concepts described incuda runtime. the driver api is implemented in thecudadynamic library (cuda.dllorcuda.so) which is copied on the system during the installation of the device driver. all its entry points are prefixed with cu. it is a handle-based, imperative api: most objects are referenced by opaque handles that may be specified to functions to manipulate the objects. the objects available in the driver api are summarized intable 22. the driver api must be initialized withcuinit()before any function from the driver api is called. a cuda context must then be created that is attached to a specific device and made current to the calling host thread as detailed incontext. within a cuda context, kernels are explicitly loaded as ptx or binary objects by the host code as described inmodule. kernels written in c++ must therefore be compiled separately intoptxor binary objects. kernels are launched using api entry points as described inkernel execution. any application that wants to run on future device architectures must loadptx, not binary code. this is because binary code is architecture-specific and therefore incompatible with future architectures, whereasptxcode is compiled to binary code at load time by the device driver. here is the host code of the sample fromkernelswritten using the driver api: full code can be found in thevectoradddrvcuda sample.",0
17.1.context,#context,"17.1.context a cuda context is analogous to a cpu process. all resources and actions performed within the driver api are encapsulated inside a cuda context, and the system automatically cleans up these resources when the context is destroyed. besides objects such as modules and texture or surface references, each context has its own distinct address space. as a result,cudeviceptrvalues from different contexts reference different memory locations. a host thread may have only one device context current at a time. when a context is created withcuctxcreate(), it is made current to the calling host thread. cuda functions that operate in a context (most functions that do not involve device enumeration or context management) will returncuda_error_invalid_contextif a valid context is not current to the thread. each host thread has a stack of current contexts.cuctxcreate()pushes the new context onto the top of the stack.cuctxpopcurrent()may be called to detach the context from the host thread. the context is then floating and may be pushed as the current context for any host thread.cuctxpopcurrent()also restores the previous current context, if any. a usage count is also maintained for each context.cuctxcreate()creates a context with a usage count of 1.cuctxattach()increments the usage count andcuctxdetach()decrements it. a context is destroyed when the usage count goes to 0 when callingcuctxdetach()orcuctxdestroy(). the driver api is interoperable with the runtime and it is possible to access theprimary context(seeinitialization) managed by the runtime from the driver api viacudeviceprimaryctxretain(). usage count facilitates interoperability between third party authored code operating in the same context. for example, if three libraries are loaded to use the same context, each library would callcuctxattach()to increment the usage count andcuctxdetach()to decrement the usage count when the library is done using the context. for most libraries, it is expected that the application will have created a context before loading or initializing the library; that way, the application can create the context using its own heuristics, and the library simply operates on the context handed to it. libraries that wish to create their own contexts - unbeknownst to their api clients who may or may not have created contexts of their own - would usecuctxpushcurrent()andcuctxpopcurrent()as illustrated in the following figure.",5
17.2.module,#module,"17.2.module modules are dynamically loadable packages of device code and data, akin to dlls in windows, that are output by nvcc (seecompilation with nvcc). the names for all symbols, including functions, global variables, and texture or surface references, are maintained at module scope so that modules written by independent third parties may interoperate in the same cuda context. this code sample loads a module and retrieves a handle to some kernel: this code sample compiles and loads a new module from ptx code and parses compilation errors: this code sample compiles, links, and loads a new module from multiple ptx codes and parses link and compilation errors: full code can be found in theptxjitcuda sample.",0
17.3.kernel execution,#kernel-execution,"17.3.kernel execution culaunchkernel()launches a kernel with a given execution configuration. parameters are passed either as an array of pointers (next to last parameter ofculaunchkernel()) where the nth pointer corresponds to the nth parameter and points to a region of memory from which the parameter is copied, or as one of the extra options (last parameter ofculaunchkernel()). when parameters are passed as an extra option (thecu_launch_param_buffer_pointeroption), they are passed as a pointer to a single buffer where parameters are assumed to be properly offset with respect to each other by matching the alignment requirement for each parameter type in device code. alignment requirements in device code for the built-in vector types are listed intable 5. for all other basic types, the alignment requirement in device code matches the alignment requirement in host code and can therefore be obtained using__alignof(). the only exception is when the host compiler alignsdoubleandlonglong(andlongon a 64-bit system) on a one-word boundary instead of a two-word boundary (for example, usinggccs compilation flag-mno-align-double) since in device code these types are always aligned on a two-word boundary. cudeviceptris an integer, but represents a pointer, so its alignment requirement is__alignof(void*). the following code sample uses a macro (align_up()) to adjust the offset of each parameter to meet its alignment requirement and another macro (add_to_param_buffer()) to add each parameter to the parameter buffer passed to thecu_launch_param_buffer_pointeroption. the alignment requirement of a structure is equal to the maximum of the alignment requirements of its fields. the alignment requirement of a structure that contains built-in vector types,cudeviceptr, or non-aligneddoubleandlonglong, might therefore differ between device code and host code. such a structure might also be padded differently. the following structure, for example, is not padded at all in host code, but it is padded in device code with 12 bytes after fieldfsince the alignment requirement for fieldf4is 16.",6
17.4.interoperability between runtime and driver apis,#interoperability-between-runtime-and-driver-apis,"17.4.interoperability between runtime and driver apis an application can mix runtime api code with driver api code. if a context is created and made current via the driver api, subsequent runtime calls will pick up this context instead of creating a new one. if the runtime is initialized (implicitly as mentioned incuda runtime),cuctxgetcurrent()can be used to retrieve the context created during initialization. this context can be used by subsequent driver api calls. the implicitly created context from the runtime is called theprimary context(seeinitialization). it can be managed from the driver api with theprimary context managementfunctions. device memory can be allocated and freed using either api.cudeviceptrcan be cast to regular pointers and vice-versa: in particular, this means that applications written using the driver api can invoke libraries written using the runtime api (such as cufft, cublas, ). all functions from the device and version management sections of the reference manual can be used interchangeably.",5
17.5.driver entry point access,#driver-entry-point-access,17.5.driver entry point access,9
17.5.1.introduction,#introduction-driver-entry-point-access,"17.5.1.introduction thedriverentrypointaccessapisprovide a way to retrieve the address of a cuda driver function. starting from cuda 11.3, users can call into available cuda driver apis using function pointers obtained from these apis. these apis provide functionality similar to their counterparts, dlsym on posix platforms and getprocaddress on windows. the provided apis will let users:",0
17.5.2.driver function typedefs,#driver-function-typedefs,"17.5.2.driver function typedefs to help retrieve the cuda driver api entry points, the cuda toolkit provides access to headers containing the function pointer definitions for all cuda driver apis. these headers are installed with the cuda toolkit and are made available in the toolkitsinclude/directory. the table below summarizes the header files containing thetypedefsfor each cuda api header file. the above headers do not define actual function pointers themselves; they define the typedefs for function pointers. for example,cudatypedefs.hhas the below typedefs for the driver apicumemalloc: cuda driver symbols have a version based naming scheme with a_v*extension in its name except for the first version. when the signature or the semantics of a specific cuda driver api changes, we increment the version number of the corresponding driver symbol. in the case of thecumemallocdriver api, the first driver symbol name iscumemallocand the next symbol name iscumemalloc_v2. the typedef for the first version which was introduced in cuda 2.0 (2000) ispfn_cumemalloc_v2000. the typedef for the next version which was introduced in cuda 3.2 (3020) ispfn_cumemalloc_v3020. thetypedefscan be used to more easily define a function pointer of the appropriate type in code: the above method is preferable if users are interested in a specific version of the api. additionally, the headers have predefined macros for the latest version of all driver symbols that were available when the installed cuda toolkit was released; these typedefs do not have a_v*suffix. for cuda 11.3 toolkit,cumemalloc_v2was the latest version and so we can also define its function pointer as below:",0
17.5.3.driver function retrieval,#driver-function-retrieval,"17.5.3.driver function retrieval using the driver entry point access apis and the appropriate typedef, we can get the function pointer to any cuda driver api.",0
17.5.4.potential implications with cugetprocaddress,#potential-implications-with-cugetprocaddress,17.5.4.potential implications with cugetprocaddress below is a set of concrete and theoretical examples of potential issues withcugetprocaddressandcudagetdriverentrypoint.,8
17.5.5.determining cugetprocaddress failure reasons,#determining-cugetprocaddress-failure-reasons,"17.5.5.determining cugetprocaddress failure reasons there are two types of errors with cugetprocaddress. those are (1) api/usage errors and (2) inability to find the driver api requested. the first error type will return error codes from the api via the curesult return value. things like passing null as thepfnvariable or passing invalidflags. the second error type encodes in thecudriverprocaddressqueryresult*symbolstatusand can be used to help distinguish potential issues with the driver not being able to find the symbol requested. take the following example: the first case with the return codecu_get_proc_address_version_not_sufficientindicates that thesymbolwas found when searching in the cuda driver but it was added later than thecudaversionsupplied. in the example, specifyingcudaversionas anything 11030 or less and when running against a cuda driver >= cuda 11.4 would give this result ofcu_get_proc_address_version_not_sufficient. this is becausecudevicegetexecaffinitysupportwas added in cuda 11.4 (11040). the second case with the return codecu_get_proc_address_symbol_not_foundindicates that thesymbolwas not found when searching in the cuda driver. this can be due to a few reasons such as unsupported cuda function due to older driver as well as just having a typo. in the latter, similar to the last example if the user had putsymbolas cudevicegetexecaffinitysupport - notice the capital cu to start the string -cugetprocaddresswould not be able to find the api because the string doesnt match. in the former case an example might be the user developing an application against a cuda driver supporting the new api, and deploying the application against an older cuda driver. using the last example, if the developer developed against cuda 11.4 or later but was deployed against a cuda 11.3 driver, during their development they may have had a succesfulcugetprocaddress, but when deploying an application running against a cuda 11.3 driver the call would no longer work with thecu_get_proc_address_symbol_not_foundreturned indriverstatus.",8
18.cuda environment variables,#cuda-environment-variables,18.cuda environment variables the following table lists the cuda environment variables. environment variables related to the multi-process service are documented in the multi-process service section of the gpu deployment and management guide.,0
19.unified memory programming,#unified-memory-programming,19.unified memory programming this documentation on unified memory is divided into 3 parts:,5
19.1.unified memory introduction,#unified-memory-introduction,"19.1.unified memory introduction cuda unified memory provides all processors with: unified memory improves gpu programming in several ways: with cuda unified memory, data movement still takes place, and hints may improve performance.
these hints are not required for correctness or functionality, that is, programmers may focus on parallelizing
their applications across gpus and cpus first, and worry about data-movement later in the development cycle as a performance optimzation.
note that the physical location of data is invisible to a program and may be changed at any time,
but accesses to the datas virtual address will remain valid and coherent from any processor regardless of locality. there are two main ways to obtain cuda unified memory:",5
19.1.1.system requirements for unified memory,#system-requirements-for-unified-memory,"19.1.1.system requirements for unified memory the following table shows the different levels of support for cuda unified memory,
the device properties required to detect these levels of support
and links to the documentation specific to each level of support: the behavior of an application that attempts to use unified memory on a system that does not support it is undefined.
the following properties enable cuda applications to check the level of system support for unified memory, and
to be portable between systems with different levels of support: a program may query the level of gpu support for cuda unified memory, by querying the attributes in
tableoverview of levels of unified memory supportabove usingcudagetdeviceproperties().",0
19.1.2.programming model,#um-opt-in,"19.1.2.programming model with cuda unified memory, separate allocations between host and device, and explicit memory transfers between them, are no longer required.
programs may allocate unified memory in the following ways: most examples in this chapter provide at least two versions, one using cuda managed memory and one using system-allocated memory.
tabs allow you to choose between them. the following samples illustrate how unified memory simplifies cuda programs: these examples combine two numbers together on the gpu with a per-thread id returning the values in an array:",5
19.2.unified memory on devices with full cuda unified memory support,#unified-memory-on-devices-with-full-cuda-unified-memory-support,19.2.unified memory on devices with full cuda unified memory support,5
19.2.1.system-allocated memory: in-depth examples,#system-allocated-memory-in-depth-examples,"19.2.1.system-allocated memory: in-depth examples systems with full cuda unified memory supportallow the device to access any memory owned by the host process interacting with the device.
this section shows a few advanced use-cases, using a kernel that simply prints
the first 8 characters of an input character array to the standard output stream: the following tabs show various ways of how this kernel may be called: the first three tabs above show the example as already detailed in theprogramming model section.
the next three tabs show various ways a file-scope or global-scope variable can
be accessed from the device. note that for the extern variable, it could be declared and its memory
owned and managed by a third-party library, which does not interact with cuda at all. also note that stack variables as well as file-scope and global-scope variables can
only be accessed through a pointer by the gpu. in this specific example, this is
convenient because the character array is already declared as a pointer:constchar*.
however, consider the following example with a global-scope integer: in the example above, we need to ensure to pass apointerto the global variable
to the kernel instead of directly accessing the global variable in the kernel.
this is because global variables without the__managed__specifier are declared
as__host__-only by default, thus most compilers wont allow using these
variables directly in device code as of now.",5
19.2.2.performance tuning,#performance-tuning,"19.2.2.performance tuning in order to achieve good performance with unified memory, it is important to: as general advice,unified memory performance hintsmight provide improved performance, but using them incorrectly might degrade performance
compared to the default behavior.
also note that any hint has a performance cost associated with it on the host,
thus useful hints must at the very least improve performance enough to overcome this cost.",5
19.3.unified memory on devices without full cuda unified memory support,#unified-memory-on-devices-without-full-cuda-unified-memory-support,19.3.unified memory on devices without full cuda unified memory support,5
19.3.1.unified memory on devices with only cuda managed memory support,#unified-memory-on-devices-with-only-cuda-managed-memory-support,"19.3.1.unified memory on devices with only cuda managed memory support for devices with compute capability 6.x or higher but withoutpageable memory access,
cuda managed memory is fully supported and coherent.
the programming model and performance tuning of unified memory is largely similar
to the model as described inunified memory on devices with full cuda unified memory support,
with the notable exception that system allocators cannot be used to allocate memory.
thus, the following list of sub-sections do not apply:",5
19.3.2.unified memory on windows or devices with compute capability 5.x,#unified-memory-on-windows-or-devices-with-compute-capability-5-x,19.3.2.unified memory on windows or devices with compute capability 5.x devices with compute capability lower than 6.0 or windows platforms support cuda managed memory v1.0 with limited support for data migration and coherency as well as memory oversubscription. the following sub-sections describe in more detail how to use and optimize managed memory on these platforms.,5
20.lazy loading,#lazy-loading,20.lazy loading,9
20.1.what is lazy loading?,#what-is-lazy-loading,"20.1.what is lazy loading? lazy loading delays loading of cuda modules and kernels from program initalization closer to kernels execution.
if a program does not use every single kernel it has included, then some kernels will be loaded unneccesarily.
this is very common, especially if you include any libraries.
most of the time, programs only use a small amount of kernels from libraries they include. thanks to lazy loading, programs are able to only load kernels they are actually going to use, saving time on initialization.
this reduces memory overhead, both on gpu memory and host memory. lazy loading is enabled by setting thecuda_module_loadingenvironment variable tolazy. firstly, cuda runtime will no longer load all modules during program initialization, with the exception of modules containing managed variables.
each module will be loaded on first usage of a variable or a kernel from that module.
this optimization is only relevant to cuda runtime users, cuda driver users who usecumoduleloadare unaffected. this optimization shipped in cuda 11.8.
the behavior for cuda driver users who useculibraryloadto load module data into memory can be changed by
setting thecuda_module_data_loadingenvironment variable. secondly, loading a module (cumoduleload*()family of functions) will not be loading kernels immediately,
instead it will delay loading of a kernel untilcumodulegetfunction()is called.
there are certain exceptions here, some kernels have to be loaded duringcumoduleload*(),
such as kernels of which pointers are stored in global variables.
this optimization is relevant to both cuda runtime and cuda driver users.
cuda runtime will only callcumodulegetfunction()when a kernel is used/referenced for the first time.
this optimization shipped in cuda 11.7. both of these optimizations are designed to be invisible to the user, assuming cuda programming model is followed.",5
20.2.lazy loading version support,#lazy-loading-version-support,20.2.lazy loading version support lazy loading is a cuda runtime and cuda driver feature. upgrades to both might be necessary to utilize the feature.,0
20.2.1.driver,#driver,"20.2.1.driver lazy loading requires r515+ user-mode library, but it supports forward compatibility, meaning it can run on top of older kernel mode drivers. without r515+ user-mode library, lazy loading is not available in any shape or form, even if toolkit version is 11.7+.",9
20.2.2.toolkit,#toolkit,"20.2.2.toolkit lazy loading was introduced in cuda 11.7, and received a significant upgrade in cuda 11.8. if your application uses cuda runtime, then in order to see benefits from lazy loading your application must use 11.7+ cuda runtime. as cuda runtime is usually linked statically into programs and libraries,
this means that you have to recompile your program with cuda 11.7+ toolkit and use cuda 11.7+ libraries. otherwise you will not see the benefits of lazy loading, even if your driver version supports it. if only some of your libraries are 11.7+, you will only see benefits of lazy loading in those libraries.
other libraries will still load everything eagerly.",0
20.2.3.compiler,#compiler,"20.2.3.compiler lazy loading does not require any compiler support. both sass and ptx compiled with pre-11.7 compilers can be loaded with lazy loading enabled,
and will see full benefits of the feature. however, 11.7+ cuda runtime is still required, as described above.",0
20.3.triggering loading of kernels in lazy mode,#triggering-loading-of-kernels-in-lazy-mode,"20.3.triggering loading of kernels in lazy mode loading kernels and variables happens automatically, without any need for explicit loading.
simply launching a kernel or referencing a variable or a kernel will automatically load relevant modules and kernels. however, if for any reason you wish to load a kernel without executing it or modifying it in any way, we recommend the following.",6
20.3.1.cuda driver api,#cuda-driver-api,"20.3.1.cuda driver api loading of kernels happens duringcumodulegetfunction()call.
this call is necessary even without lazy loading, as it is the only way to obtain a kernel handle. however, you can also use this api to control with finer granularity when kernels are loaded.",0
20.3.2.cuda runtime api,#cuda-runtime-api,"20.3.2.cuda runtime api cuda runtime api manages module management automatically,
so we recommend simply usingcudafuncgetattributes()to reference the kernel. this will ensure that the kernel is loaded without changing the state.",0
20.4.querying whether lazy loading is turned on,#querying-whether-lazy-loading-is-turned-on,"20.4.querying whether lazy loading is turned on in order to check whether user enabled lazy loading,curesultcumodulegetloadingmode(cumoduleloadingmode*mode)can be used. its important to note that cuda must be initialized before running this function. sample usage can be seen in the snippet below.",0
20.5.possible issues when adopting lazy loading,#possible-issues-when-adopting-lazy-loading,"20.5.possible issues when adopting lazy loading lazy loading is designed so that it should not require any modifications to applications to use it.
that said, there are some caveats, especially when applications are not fully compliant with cuda programming model.",0
20.5.1.concurrent execution,#concurrent-execution,"20.5.1.concurrent execution loading kernels might require context synchronization.
some programs incorrectly treat the possibility of concurrent execution of kernels as a guarantee.
in such cases, if program assumes that two kernels will be able to execute concurrently,
and one of the kernels will not return without the other kernel executing, there is a possibility of a deadlock. if kernel a will be spinning in an infinite loop until kernel b is executing.
in such case launching kernel b will trigger lazy loading of kernel b. if this loading will require context synchronization,
then we have a deadlock: kernel a is waiting for kernel b, but loading kernel b is stuck waiting for kernel a to finish to synchronize the context. such program is an anti-pattern, but if for any reason you want to keep it you can do the following:",5
20.5.2.allocators,#allocators,"20.5.2.allocators lazy loading delays loading code from initialization phase of the program closer to execution phase.
loading code onto the gpu requires memory allocation. if your application tries to allocate the entire vram on startup, e.g. to use it for its own allocator,
then it might turn out that there will be no more memory left to load the kernels.
this is despite the fact that overall lazy loading frees up more memory for the user.
cuda will need to allocate some memory to load each kernel, which usually happens at first launch time of each kernel.
if your application allocator greedily allocated everything, cuda will fail to allocate memory. possible solutions:",5
20.5.3.autotuning,#autotuning,"20.5.3.autotuning some applications launch several kernels implementing the same functionality to determine which one is the fastest.
while it is overall advisable to run at least one warmup iteration, it becomes especially important with lazy loading.
after all, including time taken to load the kernel will skew your results. possible solutions:",5
21.extended gpu memory,#extended-gpu-memory,"21.extended gpu memory the extended gpu memory (egm) feature, utilizing the high-bandwidth
nvlink-c2c, facilitates efficient access to all system memory by gpus,
in a single-node system.
egm applies to integrated cpu-gpu nvidia systems by allowing physical memory
allocation that can be accessed from any gpu
thread within the setup. egm ensures that all gpus can access
its resources at the speed of either gpu-gpu nvlink or nvlink-c2c. in this setup, memory accesses occur via the local high-bandwidth
nvlink-c2c. for remote memory accesses,
gpu nvlink and, in some cases, nvlink-c2c are used. with egm, gpu
threads gain the capability to access all available memory resources,
including cpu attached memory and hbm3, over the nvswitch fabric.",5
21.1.preliminaries,#preliminaries,"21.1.preliminaries before diving into api changes for egm functionalities, we are going to
cover currently supported topologies, identifier assignment,
prerequisites for virtual memory management, and cuda types for egm.",5
21.1.1.egm platforms: system topology,#egm-platforms-system-topology,"21.1.1.egm platforms: system topology currently, egm can be enabled in three platforms:(1) single-node, single-gpu:
consists of an arm-based cpu, cpu attached memory, and a gpu. between the cpu
and the gpu there is a high bandwidth c2c (chip-to-chip) interconnect.(2) single-node, multi-gpu: consists of fully connected four
single-node, single-gpu platforms.(3) multi-node, single-gpu:
two or more single-node multi-socket systems.",5
21.1.2.socket identifiers: what are they? how to access them?,#socket-identifiers-what-are-they-how-to-access-them,"21.1.2.socket identifiers: what are they? how to access them? numa (non-uniform memory access) is a memory architecture used in
multi-processor computer systems such that the memory is divided into
multiple nodes. each node has its own processors and memory. in such a
system, numa divides the system into nodes and assigns a unique
identifier (numaid) to every node. egm uses the numa node identifier which is assigned by the operating
system. note that, this identifier is different from the ordinal of a
device and it is associated with the closest host node. in addition to
the existing methods, the user can obtain the identifier of the host
node (numaid) by callingcudevicegetattributewithcu_device_attribute_host_numa_idattribute type as follows:",6
21.1.3.allocators and egm support,#allocators-and-egm-support,"21.1.3.allocators and egm support mapping system memory as egm does not cause any performance issues. in
fact, accessing a remote sockets system memory mapped as egm is going
to be faster. because, with egm traffic is guaranteed to be routed over
nvlinks. currently,cumemcreateandcudamempoolcreateallocators are
supported with appropriate location type and numa identifiers.",5
21.1.4.memory management extensions to current apis,#memory-management-extensions-to-current-apis,"21.1.4.memory management extensions to current apis currently, egm memory can be mapped with virtual memory (cumemcreate) or
stream ordered memory (cudamempoolcreate) allocators. the user is
responsible for allocating physical memory and mapping it to a virtual
memory address space on all sockets. new cuda property types have been added to apis for allowing those
approaches to understand allocation locations using numa-like node
identifiers:",5
21.2.using the egm interface,#using-the-egm-interface,21.2.using the egm interface,9
"21.2.1.single-node, single-gpu",#single-node-single-gpu,"21.2.1.single-node, single-gpu any of the existing cuda host allocators as well as system allocated
memory can be used to benefit from high-bandwidth c2c. to the user,
local access is what a host allocation is today.",5
"21.2.2.single-node, multi-gpu",#single-node-multi-gpu,"21.2.2.single-node, multi-gpu in a multi-gpu system, the user has to provide host information for
the placement. as we mentioned, a natural way to express that
information would be by using numa node ids and egm follows this
approach. therefore, using thecudevicegetattributefunction the
user should be able to learn the closest numa node id. (seesocket identifiers: what are they? how to access them?).
then the user can allocate and manage egm memory using vmm (virtual
memory management) api or cuda memory pool.",5
"21.2.3.multi-node, single-gpu",#multi-node-single-gpu,"21.2.3.multi-node, single-gpu beyond memory allocation, remote peer access does not have egm-specific
modification and it follows cuda inter process (ipc) protocol. seecuda programming guidefor more details in ipc. the user should allocate memory usingcumemcreateand again the
user has to explicitly providecu_mem_location_type_host_numaas
the location type andnumaidas the location identifier. in
additioncu_mem_handle_type_fabricshould be defined as the
requested handle type. the following code snippet shows allocating
physical memory onnode a: after creating allocation handle usingcumemcreatethe user can
export that handle to the other node, node b, callingcumemexporttoshareablehandle: on node b, the handle can be imported usingcumemimportfromshareablehandleand treated as any other fabric
handle when handle is imported at node b, then the user can reserve an address
space and map it locally in a regular fashion: as the final step, the user should give appropriate accesses to each of
the local gpus at node b. an example code snippet that gives read and
write access to eight local gpus:",5
22.notices,#notices,22.notices,9
22.1.notice,#notice,"22.1.notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation (nvidia) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material (defined below), code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer (terms of sale). nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion and/or use of nvidia products in such equipment or applications and therefore such inclusion and/or use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions and/or requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the nvidia product in any manner that is contrary to this document or (ii) customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding third-party products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents (together and separately, materials) are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product.",4
22.2.opencl,#opencl,22.2.opencl opencl is a trademark of apple inc. used under license to the khronos group inc.,4
22.3.trademarks,#trademarks,22.3.trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.,4
